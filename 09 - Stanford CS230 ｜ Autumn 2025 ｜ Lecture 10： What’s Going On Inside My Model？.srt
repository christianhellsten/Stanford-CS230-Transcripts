1
00:00:05,490 --> 00:00:12,890
Welcome to Lecture 9 already. I hope everybody had a good, uh, fall break.

2
00:00:12,890 --> 00:00:19,050
Um, today we're going to talk about neural networks,

3
00:00:19,050 --> 00:00:22,890
both convolutional neural networks and transformers,

4
00:00:22,890 --> 00:00:26,890
um, and we're gonna unpack it to see what's going on inside.

5
00:00:26,890 --> 00:00:29,449
Um, this lecture used to be called,

6
00:00:29,449 --> 00:00:31,890
um, Neural Network Interpretability,

7
00:00:31,890 --> 00:00:34,250
but I've broadened the scope because there is,

8
00:00:34,250 --> 00:00:39,009
uh, section now where we talk more about frontier models,

9
00:00:39,009 --> 00:00:44,409
um, and the interpretability or visualization methods have not

10
00:00:44,409 --> 00:00:48,850
quite been figured out for most models that you play with out there.

11
00:00:48,850 --> 00:00:52,090
So think about this one as research areas,

12
00:00:52,090 --> 00:00:56,329
what we know from convolutions and what we're trying to figure out for,

13
00:00:56,329 --> 00:00:57,969
uh, frontier models.

14
00:00:57,969 --> 00:01:01,929
Uh, we're gonna start with a very packed agenda with

15
00:01:01,929 --> 00:01:06,209
the case study, uh, where I'm going to ask you a question, um,

16
00:01:06,209 --> 00:01:08,849
and let you brainstorm a little bit, uh,

17
00:01:08,849 --> 00:01:13,129
all together, um, on how you would, you know,

18
00:01:13,129 --> 00:01:19,450
try to understand what's happening inside of a frontier model.

19
00:01:19,450 --> 00:01:21,329
Um, in the second section,

20
00:01:21,329 --> 00:01:24,329
we're gonna look at the example of convolutions

21
00:01:24,329 --> 00:01:29,730
specifically and try to interpret everything possible about a convolution.

22
00:01:29,730 --> 00:01:33,409
Meaning, we're going to look at input-output relationship.

23
00:01:33,409 --> 00:01:37,370
We're going to look at a specific neuron inside and try to interpret it.

24
00:01:37,370 --> 00:01:39,730
We're going to look also at, uh,

25
00:01:39,730 --> 00:01:43,170
specific feature maps and try to understand what they do.

26
00:01:43,170 --> 00:01:45,450
I will present many methods to do that.

27
00:01:45,450 --> 00:01:49,409
Those methods are real and they've been used for convolutions.

28
00:01:49,409 --> 00:01:54,409
Um, but again, they're not the methods that you might see frontier labs

29
00:01:54,409 --> 00:02:00,170
use for today's language or, uh, vision, uh, uh, large, large models.

30
00:02:00,170 --> 00:02:03,530
However, they are going to bring you the skills, uh,

31
00:02:03,530 --> 00:02:06,489
that will allow you to understand the methods for frontier models

32
00:02:06,489 --> 00:02:09,090
as researchers are trying to figure them out.

33
00:02:09,090 --> 00:02:13,449
Um, the second half of the lecture is going to focus more on

34
00:02:13,449 --> 00:02:16,969
the modern representation analysis, uh,

35
00:02:16,969 --> 00:02:19,169
we're gonna talk about scaling laws,

36
00:02:19,169 --> 00:02:22,490
capability benchmarking, data diagnostics,

37
00:02:22,490 --> 00:02:26,530
and then I- I end on- on a few closing remarks.

38
00:02:26,530 --> 00:02:29,610
Okay? Are we ready for this one?

39
00:02:29,610 --> 00:02:31,930
Lots of visualizations in this lecture.

40
00:02:31,930 --> 00:02:36,250
So first, um, um, question for you all.

41
00:02:36,250 --> 00:02:41,729
Um, let's say the case study is you are a model trainer,

42
00:02:41,729 --> 00:02:43,530
um, and you're, you know,

43
00:02:43,530 --> 00:02:48,330
working on a 200 billion parameters model, um,

44
00:02:48,330 --> 00:02:51,650
at a frontier lab and overnight, you know,

45
00:02:51,650 --> 00:02:54,889
a new checkpoint passes a training sanity check,

46
00:02:54,889 --> 00:02:57,490
but a few issues arise.

47
00:02:57,490 --> 00:02:59,810
Things like, you know, uh,

48
00:02:59,810 --> 00:03:03,849
model is getting worse on reasoning benchmarks, um,

49
00:03:03,849 --> 00:03:07,770
some safety evals are failing, um,

50
00:03:07,770 --> 00:03:09,930
and there is a weird spike in, let's say,

51
00:03:09,930 --> 00:03:14,729
latency for tool use when you actually use this model for an agentic workflow.

52
00:03:14,729 --> 00:03:18,789
Your VP is wondering what's happening and they ask,

53
00:03:18,789 --> 00:03:20,389
what is going on, um,

54
00:03:20,389 --> 00:03:22,189
and what are you going to look at first?

55
00:03:22,189 --> 00:03:26,110
So what I want you to discuss, um,

56
00:03:26,110 --> 00:03:29,949
for a minute or so, think about it first and I'll open up, um,

57
00:03:29,949 --> 00:03:33,870
is what are the type of evidences that you would look,

58
00:03:33,870 --> 00:03:37,229
want to inspect before even, you know,

59
00:03:37,229 --> 00:03:40,110
touching the code or retraining the- the model?

60
00:03:40,110 --> 00:03:42,189
What are the things that you wanna look at?

61
00:03:42,189 --> 00:03:48,150
Jumping. There's no single answer.

62
00:03:48,150 --> 00:03:51,310
So I wanna know everything you're gonna look at.

63
00:03:51,310 --> 00:03:54,030
Okay. So, um, error analysis.

64
00:03:54,030 --> 00:03:55,590
So look- look at- you said,

65
00:03:55,590 --> 00:04:01,349
I will look at the reasoning benchmarks and find the examples where the model is failing.

66
00:04:01,349 --> 00:04:05,789
Specifically, try to find patterns in order to pinpoint what the issue might be.

67
00:04:05,789 --> 00:04:08,990
And then same thing on the safety, um, evals,

68
00:04:08,990 --> 00:04:11,949
where you wanna see what type of safety issues are arising.

69
00:04:11,949 --> 00:04:14,509
Is it everywhere? Is it specific to something?

70
00:04:14,509 --> 00:04:18,350
Yeah, I agree. Error analysis in general. What else?

71
00:04:18,350 --> 00:04:35,639
Remember, you're- you're the model trainer.

72
00:04:35,639 --> 00:04:37,040
So you're training this model.

73
00:04:37,040 --> 00:04:40,839
You're- you're supposed to be watching certain things when you're training.

74
00:04:40,839 --> 00:04:47,870
What can be interesting?

75
00:04:47,870 --> 00:04:58,490
Yeah. Yeah. Let's say not necessarily passing,

76
00:04:58,490 --> 00:04:59,730
but those are great examples.

77
00:04:59,730 --> 00:05:01,129
So you're- you're mentioning- yeah.

78
00:05:01,129 --> 00:05:02,490
As you're the model trainer,

79
00:05:02,490 --> 00:05:04,490
you would be watching the training loss.

80
00:05:04,490 --> 00:05:09,290
And you wanna see what- what- what are you going to look for in that training loss?

81
00:05:09,290 --> 00:05:10,129
Convergence.

82
00:05:10,129 --> 00:05:11,810
Okay. Convergence.

83
00:05:11,810 --> 00:05:14,970
You probably want to make sure that it's smooth.

84
00:05:14,970 --> 00:05:17,529
You don't want big spikes.

85
00:05:17,529 --> 00:05:19,689
Um, how about the validation loss?

86
00:05:19,689 --> 00:05:22,850
What- what is your expectation on the validation loss?

87
00:05:22,850 --> 00:05:30,560
Yeah, should probably follow the same curve as the training loss,

88
00:05:30,560 --> 00:05:33,560
but is likely slightly higher because you're probably

89
00:05:33,560 --> 00:05:38,000
performing slightly less well on the validation set than on the training set.

90
00:05:38,000 --> 00:05:39,600
If you're seeing spikes,

91
00:05:39,600 --> 00:05:43,600
it might- it might lead to cert- it might mean there are some issues.

92
00:05:43,600 --> 00:05:47,360
Um, what else are you looking at?

93
00:05:47,360 --> 00:05:59,230
Yeah. So this batch you mean?

94
00:05:59,230 --> 00:06:02,470
Yeah, yeah, yeah. So you're looking at this round of training data.

95
00:06:02,470 --> 00:06:07,870
Maybe the last round of data that we trained on,

96
00:06:07,870 --> 00:06:09,709
there were some issue in that data.

97
00:06:09,709 --> 00:06:12,829
Maybe that data was, uh, you know, uh,

98
00:06:12,829 --> 00:06:16,189
probably, you know, poisoned or biased toward

99
00:06:16,189 --> 00:06:18,790
a certain category of data that we're failing on.

100
00:06:18,790 --> 00:06:20,870
You're- you're totally right. Yeah.

101
00:06:20,870 --> 00:06:25,430
Maybe that specific checkpoint is doing poorly compared to the previous checkpoint.

102
00:06:25,430 --> 00:06:29,069
And so you have pinpoint- you pinpoint where the issue arises.

103
00:06:29,069 --> 00:06:31,189
You know, during the- the training.

104
00:06:31,189 --> 00:06:32,910
What else are you looking at?

105
00:06:32,910 --> 00:06:50,610
Yeah. Okay.

106
00:06:50,610 --> 00:06:53,810
Because it's overnight and it seemed everything was good up to yesterday,

107
00:06:53,810 --> 00:06:54,970
and now there's an issue,

108
00:06:54,970 --> 00:06:57,250
maybe you're saying there is a hardware issue.

109
00:06:57,250 --> 00:06:58,970
Yeah, we could check actually.

110
00:06:58,970 --> 00:07:03,490
Is- yeah, latency has been, uh,

111
00:07:03,490 --> 00:07:05,850
pointed out so maybe the- the hardware has failed.

112
00:07:05,850 --> 00:07:18,879
Yeah, you're right. What else?

113
00:07:18,879 --> 00:07:22,399
So we- we- a lot of the answers are global answers.

114
00:07:22,399 --> 00:07:24,000
You're looking at the model in general.

115
00:07:24,000 --> 00:07:26,439
You're not looking at specific portions of the model.

116
00:07:26,439 --> 00:07:28,759
What would you look at if you were to inspect,

117
00:07:28,759 --> 00:07:37,620
um, the model more precisely from the inside?

118
00:07:37,620 --> 00:07:39,259
And this one's a language model.

119
00:07:39,259 --> 00:07:46,910
So you can- you can think about the fact that it's a language model.

120
00:07:46,910 --> 00:08:08,029
Yeah. Yeah, you're right.

121
00:08:08,029 --> 00:08:13,029
Like, you wanna look at different checkpoints and see where did we fail

122
00:08:13,029 --> 00:08:17,629
and might be able to trace back to that moment and figure out what the issue was.

123
00:08:17,629 --> 00:08:21,430
So for example, maybe your initialization, um,

124
00:08:21,430 --> 00:08:25,829
was actually pretty good and the first checkpoints were doing well,

125
00:08:25,829 --> 00:08:28,550
but suddenly at some point, uh,

126
00:08:28,550 --> 00:08:30,589
the model saturated in a certain way.

127
00:08:30,589 --> 00:08:32,750
Maybe you're seeing exploding gradients or

128
00:08:32,750 --> 00:08:37,070
vanishing gradients in certain moments and you wanna pinpoint that.

129
00:08:37,070 --> 00:08:40,190
Yeah. What- what else?

130
00:08:40,190 --> 00:08:42,350
We're- we're adding so many methods right now,

131
00:08:42,350 --> 00:08:47,519
but I- I wanna hear what else you have for language models.

132
00:08:47,519 --> 00:08:50,159
What other things can you visualize for language models

133
00:08:50,159 --> 00:09:00,000
that might- might mean something's going wrong?

134
00:09:00,000 --> 00:09:07,879
Yeah. The attention maps.

135
00:09:07,879 --> 00:09:09,519
Yeah, yeah, fair enough.

136
00:09:09,519 --> 00:09:11,639
You- you've learned about transformers, uh,

137
00:09:11,639 --> 00:09:13,840
in the online videos, um,

138
00:09:13,840 --> 00:09:16,960
the attention maps which are representative of,

139
00:09:16,960 --> 00:09:19,799
you know, the relationship between different tokens.

140
00:09:19,799 --> 00:09:21,440
They might not make sense to you.

141
00:09:21,440 --> 00:09:24,320
You might actually be plotting certain attention maps and be like,

142
00:09:24,320 --> 00:09:27,120
this token has nothing to do with that one,

143
00:09:27,120 --> 00:09:28,960
but the model seems to think it has.

144
00:09:28,960 --> 00:09:31,320
And you might be able to, um,

145
00:09:31,320 --> 00:09:34,840
identify certain issues with the attention maps.

146
00:09:34,840 --> 00:09:42,559
What else beyond the attention maps?

147
00:09:42,559 --> 00:09:57,639
What- what- yeah. So you mean,

148
00:09:57,639 --> 00:09:59,600
tell me more about the sensitivity analysis.

149
00:09:59,600 --> 00:10:01,279
What would you fix, for example,

150
00:10:01,279 --> 00:10:03,039
and what would you change?

151
00:10:03,039 --> 00:10:30,289
Okay. Yeah, but I like the idea of sensitivity analysis.

152
00:10:30,289 --> 00:10:34,850
You might fix- you might try to figure out which hyperparameter went wrong.

153
00:10:34,850 --> 00:10:37,330
Is there something wrong with our optimizer?

154
00:10:37,330 --> 00:10:40,690
Is our learning rate schedule poorly tuned?

155
00:10:40,850 --> 00:10:43,769
Um, uh, maybe scaling laws.

156
00:10:43,769 --> 00:10:46,129
You know, we know that we can play with compute,

157
00:10:46,129 --> 00:10:47,129
we can play with data,

158
00:10:47,129 --> 00:10:48,850
we can play with model size,

159
00:10:48,850 --> 00:10:51,090
and one of those might be going wrong.

160
00:10:51,090 --> 00:10:55,850
Maybe an analysis would allow us to identify the model is fine,

161
00:10:55,850 --> 00:10:57,289
it just needs to be trained longer,

162
00:10:57,289 --> 00:11:01,009
or the model is actually too small for the amount of data we're giving it.

163
00:11:01,009 --> 00:11:03,529
You know, that type of stuff would come with, um,

164
00:11:03,529 --> 00:11:06,690
either doing a sensitivity analysis or, uh,

165
00:11:06,690 --> 00:11:10,370
comparing what we're doing to the scaling laws that we know from other models.

166
00:11:10,370 --> 00:11:13,009
Uh, we're gonna look into that.

167
00:11:13,009 --> 00:11:25,649
Okay. Any other ideas?

168
00:11:25,649 --> 00:11:29,610
Might be. So you're- you're saying you're- you- I gave you 200 billion parameters,

169
00:11:29,610 --> 00:11:32,090
which is a very large model even as of today.

170
00:11:32,090 --> 00:11:34,129
Uh, it might be over-parameterized.

171
00:11:34,129 --> 00:11:37,330
That's a good question because it depends on what it's been trained on,

172
00:11:37,330 --> 00:11:38,570
how much data we're feeding it,

173
00:11:38,570 --> 00:11:40,769
how much compute. It's all relative to each other.

174
00:11:40,769 --> 00:11:42,049
But yeah, it's a large model,

175
00:11:42,049 --> 00:11:45,450
so I would expect a lot of compute and a lot of data along with it.

176
00:11:45,450 --> 00:11:46,809
Um, you know, in fact,

177
00:11:46,809 --> 00:11:49,610
a lot of these models might be built as a mixture of experts.

178
00:11:49,610 --> 00:11:52,850
You- you- you- you've heard about mixture of experts.

179
00:11:52,850 --> 00:11:56,809
One thing that could happen is that some of the experts are failing.

180
00:11:56,809 --> 00:12:02,889
And you might be inspecting if experts are in fact failing or the routing module

181
00:12:02,889 --> 00:12:06,610
is, um, always selecting the same experts because it's just,

182
00:12:06,610 --> 00:12:09,730
you know, found an expert that is really good and generalized,

183
00:12:09,730 --> 00:12:12,169
and the other experts are not being used.

184
00:12:12,169 --> 00:12:14,570
That might be another issue as well, uh,

185
00:12:14,570 --> 00:12:17,169
that might, you know, be related to the model capacity.

186
00:12:17,169 --> 00:12:19,529
Because if the model is not using

187
00:12:19,529 --> 00:12:24,809
all its experts it's probably not actually operating as a 200 billion parameter model.

188
00:12:24,809 --> 00:12:26,970
It's operating as a smaller model.

189
00:12:26,970 --> 00:12:29,250
Yeah. Okay. So, you know,

190
00:12:29,250 --> 00:12:31,250
generally this is to motivate the lecture.

191
00:12:31,250 --> 00:12:34,450
We're gonna look into all of these together today, right?

192
00:12:34,450 --> 00:12:36,570
And we start with convolutions, uh,

193
00:12:36,570 --> 00:12:38,169
because they're very visual.

194
00:12:38,169 --> 00:12:39,690
For the convolutional part,

195
00:12:39,690 --> 00:12:41,370
we're gonna go super deep.

196
00:12:41,370 --> 00:12:44,009
Um, but then for the frontier models,

197
00:12:44,009 --> 00:12:47,129
I'm just gonna get broader and- and give you the areas of research.

198
00:12:47,129 --> 00:12:51,330
So the answer to the question I asked typically would fall under four buckets,

199
00:12:51,330 --> 00:12:53,289
every solution that we looked into together.

200
00:12:53,289 --> 00:12:54,730
One is training and scaling.

201
00:12:54,730 --> 00:12:57,169
So people are looking at loss curves,

202
00:12:57,169 --> 00:13:00,490
at, you know, um, things like gradients,

203
00:13:00,490 --> 00:13:03,610
uh, learning rates, mixture of experts, routing,

204
00:13:03,610 --> 00:13:06,169
scaling laws, we're- we're gonna talk about all these.

205
00:13:06,169 --> 00:13:11,129
Um, the second category is representation and internal aspect of the model.

206
00:13:11,129 --> 00:13:14,330
You mentioned attention, uh, heads and maps,

207
00:13:14,330 --> 00:13:16,289
embeddings, nobody mentioned embeddings,

208
00:13:16,289 --> 00:13:18,330
but you could actually visualize embeddings and see,

209
00:13:18,330 --> 00:13:19,529
does it make sense to you?

210
00:13:19,529 --> 00:13:20,690
Are these, um, you know,

211
00:13:20,690 --> 00:13:23,649
tokens close to each other as you would expect,

212
00:13:23,649 --> 00:13:28,970
meaning the- the model's mental understanding of language is correct.

213
00:13:28,970 --> 00:13:32,129
Um, and then your own level behaviors,

214
00:13:32,129 --> 00:13:34,889
although that's really hard with a large model,

215
00:13:34,889 --> 00:13:37,610
um, and nobody has quite figured it out yet.

216
00:13:37,610 --> 00:13:41,730
Um, and then the other category might be data and distribution,

217
00:13:41,730 --> 00:13:43,250
maybe, you know,

218
00:13:43,250 --> 00:13:46,610
the actual, um, uh,

219
00:13:46,610 --> 00:13:49,730
benchmark that we're looking at has been contaminated,

220
00:13:49,730 --> 00:13:54,330
meaning, you know, the model is just not- either it's doing too well on

221
00:13:54,330 --> 00:13:58,250
that benchmark or it doesn't mean anything or it's doing poorly for a certain reason,

222
00:13:58,250 --> 00:14:01,610
because the data distribution used in

223
00:14:01,610 --> 00:14:05,210
the test set is completely different from the training or validation set.

224
00:14:05,210 --> 00:14:06,769
Um, and then, you know,

225
00:14:06,769 --> 00:14:08,730
it might be failing at different levels.

226
00:14:08,730 --> 00:14:11,129
You can run benchmarks on the language model,

227
00:14:11,129 --> 00:14:16,730
you can run benchmarks on the agentic workflow that is using that language model.

228
00:14:16,730 --> 00:14:19,769
And because you want the language model to be used in agentic workflow,

229
00:14:19,769 --> 00:14:22,110
those are two levels that you need to inspect.

230
00:14:22,110 --> 00:14:23,929
So for example, when, uh,

231
00:14:23,929 --> 00:14:25,970
a frontier lab says, um,

232
00:14:25,970 --> 00:14:28,809
our model is doing really well for tool use,

233
00:14:28,809 --> 00:14:33,289
what they mean is the language model has been tested on upstream tasks in

234
00:14:33,289 --> 00:14:37,570
a workflow and it's actually good at tool use against their benchmarks.

235
00:14:37,570 --> 00:14:42,009
So those are different levels of capability analysis.

236
00:14:42,009 --> 00:14:45,490
Okay. So let's talk about, uh, convolutions.

237
00:14:45,490 --> 00:14:48,210
We're gonna dive deep inside convolutions and then we'll

238
00:14:48,210 --> 00:14:51,850
go back up and look at frontier models, okay?

239
00:14:51,850 --> 00:14:55,090
So, um, first case study, uh,

240
00:14:55,090 --> 00:15:02,769
for convolutions, let's say that you have built an animal classifier for a zoo,

241
00:15:02,769 --> 00:15:04,970
and they are very reluctant to use

242
00:15:04,970 --> 00:15:08,370
your model, um, without any human supervising because they

243
00:15:08,370 --> 00:15:12,330
don't understand the decision-making process of the model.

244
00:15:12,330 --> 00:15:14,370
How can you alleviate their concerns?

245
00:15:14,370 --> 00:15:19,570
How can you give them intuition about the decision-making process of the model so that they feel like,

246
00:15:19,570 --> 00:15:25,389
ah, the model's doing things that feel natural and- and human.

247
00:15:30,159 --> 00:15:31,820
So let's say, just to simplify,

248
00:15:31,820 --> 00:15:35,340
let's say you have a- a convolution neural network,

249
00:15:35,340 --> 00:15:36,899
and there's a softmax layer,

250
00:15:36,899 --> 00:15:38,860
and it's supposed to identify animals.

251
00:15:38,860 --> 00:15:41,259
So the number of classes are many animals.

252
00:15:41,259 --> 00:15:59,960
Yeah. If you were to write a quick Python code to give them some intuition,

253
00:15:59,960 --> 00:16:01,799
how would you do it? Yeah.

254
00:16:01,799 --> 00:16:35,289
I'm trying like first with a softmax.

255
00:16:35,289 --> 00:16:38,370
Good, good, good. So just to recap, um,

256
00:16:38,370 --> 00:16:40,090
the zoo is not AI native,

257
00:16:40,090 --> 00:16:41,529
so you have to explain certain things.

258
00:16:41,529 --> 00:16:43,049
You're gonna tell them what softmax is,

259
00:16:43,049 --> 00:16:45,970
so we're gonna have a probability for each animal classes.

260
00:16:45,970 --> 00:16:47,289
That's how it works.

261
00:16:47,289 --> 00:16:48,450
Um, and on top of that,

262
00:16:48,450 --> 00:16:50,970
you also mentioned you might talk about convolutions and say,

263
00:16:50,970 --> 00:16:53,610
here are how features are identified,

264
00:16:53,610 --> 00:16:56,049
here is how a filter scans through the image,

265
00:16:56,049 --> 00:16:58,049
and, you know, we're expecting this to learn.

266
00:16:58,049 --> 00:17:00,529
So you're gonna educate them first. That's totally right.

267
00:17:00,529 --> 00:17:04,569
The second thing you mentioned is maybe you run a- a dataset search.

268
00:17:04,569 --> 00:17:07,569
So you can try to build their intuition by showing them

269
00:17:07,569 --> 00:17:10,369
animal pictures and showing that the model's doing well.

270
00:17:10,369 --> 00:17:11,690
And yeah, I agree.

271
00:17:11,690 --> 00:17:13,009
Those are good approaches.

272
00:17:13,009 --> 00:17:16,849
We're gonna see how to do a proper dataset search large-scale.

273
00:17:16,849 --> 00:17:19,329
But what else can you do,

274
00:17:19,329 --> 00:17:22,970
that's gonna give a little bit more confidence because this is explanation,

275
00:17:22,970 --> 00:17:27,009
but it's not proof that the model is looking at the right place systematically.

276
00:17:27,009 --> 00:17:43,019
Yeah. So you say ideally,

277
00:17:43,019 --> 00:17:45,980
you would give them intuition at a fi- at a filter level.

278
00:17:45,980 --> 00:17:47,299
Like this filter,

279
00:17:47,299 --> 00:17:50,819
we know it's responsible for finding the legs of an animal.

280
00:17:50,819 --> 00:18:02,910
That's what you're saying. So how would you do that?

281
00:18:02,910 --> 00:18:04,630
You know, good, you know, intuition.

282
00:18:04,630 --> 00:18:08,750
You're asking is it as simple as just printing out the weights of- that are

283
00:18:08,750 --> 00:18:13,309
identified or the feature map that results of that filter?

284
00:18:13,309 --> 00:18:15,029
Unfortunately, not usually,

285
00:18:15,029 --> 00:18:18,029
because that might be true for the first layer.

286
00:18:18,029 --> 00:18:19,630
But as you get deeper,

287
00:18:19,630 --> 00:18:22,589
things mix up so much that, you know,

288
00:18:22,589 --> 00:18:25,390
if you were just to print the filter,

289
00:18:25,390 --> 00:18:28,869
it wouldn't make any sense pretty much.

290
00:18:28,869 --> 00:18:31,109
Um, but there are other methods that we're gonna see.

291
00:18:31,109 --> 00:18:32,349
So your intuition is right.

292
00:18:32,349 --> 00:18:33,869
We're- we're gonna try to give them that.

293
00:18:33,869 --> 00:18:36,950
Something simpler. Input-output relationship.

294
00:18:36,950 --> 00:18:39,670
How would you show that, um,

295
00:18:39,670 --> 00:18:45,470
that the output is actually related to the right portions of the inputs?

296
00:18:45,470 --> 00:18:48,390
For this dog, for example.

297
00:18:48,390 --> 00:18:58,380
Yeah. Yeah. Confusion metrics across a lot of data.

298
00:18:58,380 --> 00:19:01,619
You would find true positives, false- et cetera.

299
00:19:01,619 --> 00:19:03,259
Yeah. Correct. Something else?

300
00:19:03,259 --> 00:19:27,220
So similar to what he said with, um,

301
00:19:27,220 --> 00:19:30,299
you wanna give them intuition from the inner workings of the network,

302
00:19:30,299 --> 00:19:33,579
and you're saying how about we mask the latter parts of the network,

303
00:19:33,579 --> 00:19:37,779
and we treat every intermediary layer as an output,

304
00:19:37,779 --> 00:19:39,819
and analyze if the output makes sense.

305
00:19:39,819 --> 00:19:41,859
Yeah. We're gonna look at that actually.

306
00:19:41,859 --> 00:19:43,140
Um, yeah. Those are more advanced.

307
00:19:43,140 --> 00:19:46,259
What I was looking for is even more- more basic, much more basic.

308
00:19:46,259 --> 00:19:48,859
It's, uh, if you wanna show the, uh,

309
00:19:48,859 --> 00:19:52,619
relationship between an input and an output of a CNN,

310
00:19:52,619 --> 00:19:54,859
uh, or, you know, any vision model,

311
00:19:54,859 --> 00:19:58,059
um, you might take the score of the dog,

312
00:19:58,059 --> 00:20:00,180
uh, in the output layer, okay?

313
00:20:00,180 --> 00:20:03,420
And what is exactly this quantity?

314
00:20:03,420 --> 00:20:05,140
What- what- what is the intuition,

315
00:20:05,140 --> 00:20:07,380
your intuition for what this quantity means?

316
00:20:07,380 --> 00:20:18,740
If you take the derivative of the score of a nanomole class with respect to x,

317
00:20:18,740 --> 00:20:25,349
with x being the input image.

318
00:20:25,349 --> 00:20:36,859
Yeah. Yeah. How does the score of dog change when you move pixels around?

319
00:20:36,859 --> 00:20:38,619
Which is what you want, right?

320
00:20:38,619 --> 00:20:40,660
You wanna be able to- if you can do that,

321
00:20:40,660 --> 00:20:44,059
you would indicate that which are the pixels of the image,

322
00:20:44,059 --> 00:20:45,339
that if we change them,

323
00:20:45,339 --> 00:20:47,220
it changes the score of dog.

324
00:20:47,220 --> 00:20:48,819
If you can print that,

325
00:20:48,819 --> 00:20:51,180
then you would be able to show this is where the model is

326
00:20:51,180 --> 00:20:55,339
looking at when it's predicting a dog, right?

327
00:20:55,339 --> 00:20:59,859
So, yeah, if you actually calculate this derivative, um,

328
00:20:59,859 --> 00:21:03,980
you would get something like that where some of the pixels are gonna be brighter,

329
00:21:03,980 --> 00:21:05,940
meaning their gradient is higher,

330
00:21:05,940 --> 00:21:07,859
and some of the pixels are gonna be darker,

331
00:21:07,859 --> 00:21:12,220
meaning we move that pixel and it didn't modify the score of dog at all.

332
00:21:12,420 --> 00:21:19,779
That's a very quick way to look at which pixels in the input were relevant for the score of dog.

333
00:21:19,779 --> 00:21:27,660
Uh, now, why should we select the score of dog pre-softmax versus post-softmax?

334
00:21:27,660 --> 00:21:33,960
It's usually a very common mis- misconception.

335
00:21:33,960 --> 00:21:50,720
Yeah. So, what's the issue with the scaled version class of dog?

336
00:21:50,720 --> 00:21:57,480
Yeah. So, uh, what- what you said is the- the post-softmax score is not only depending on dog,

337
00:21:57,480 --> 00:21:59,319
it's also dependent on all the other scores.

338
00:21:59,319 --> 00:22:01,000
So, you could actually, um,

339
00:22:01,000 --> 00:22:02,759
take a pixel, move it,

340
00:22:02,759 --> 00:22:06,279
and it happens to modify the score of a panda,

341
00:22:06,279 --> 00:22:08,519
because there's a panda in the background or something,

342
00:22:08,519 --> 00:22:11,519
and it would influence what you're trying to show.

343
00:22:11,519 --> 00:22:13,039
Uh, but you're only looking at dog.

344
00:22:13,039 --> 00:22:15,440
You just want the score of dog to be influenced.

345
00:22:15,440 --> 00:22:19,119
So, that's why in this method called saliency maps,

346
00:22:19,119 --> 00:22:25,799
we use the pre-softmax scores that is only representative of the class at hand that you're analyzing.

347
00:22:25,799 --> 00:22:27,880
Okay? So, you could do that.

348
00:22:27,880 --> 00:22:30,359
And actually, if you were to- in the past,

349
00:22:30,359 --> 00:22:33,400
not anymore, but you could use that for a quick segmentation,

350
00:22:33,400 --> 00:22:36,279
chance-sanity check because the pixels that are brighter,

351
00:22:36,279 --> 00:22:40,759
the gradients that are brighter are representatives of the pixels that should be overlaid on the dog.

352
00:22:40,759 --> 00:22:44,640
And in fact, if you do the saliency maps and you realize that the pixels that are

353
00:22:44,640 --> 00:22:48,079
bright when you compute that gradient are all over the place,

354
00:22:48,079 --> 00:22:50,519
it's probably that the model is not even looking at the right place.

355
00:22:50,519 --> 00:22:53,400
It's just getting lucky. Okay?

356
00:22:53,400 --> 00:22:56,400
So, this first method, saliency maps.

357
00:22:56,400 --> 00:22:58,640
Now, I have that in your toolkit.

358
00:22:58,640 --> 00:23:00,240
Very easy to implement, right?

359
00:23:00,240 --> 00:23:01,599
You just write a Python script,

360
00:23:01,599 --> 00:23:04,079
you- you perform the gradients calculation,

361
00:23:04,079 --> 00:23:09,279
you print it, it's a matrix of pixels that are brighter or darker, and you're done.

362
00:23:09,279 --> 00:23:11,640
Um, one of the main, uh,

363
00:23:11,640 --> 00:23:16,119
issues with saliency maps is that it's looking at just a pixel level,

364
00:23:16,200 --> 00:23:19,759
um, which doesn't make too much sense if you want to

365
00:23:19,759 --> 00:23:22,880
interpret semantically where the model is looking at.

366
00:23:22,880 --> 00:23:25,599
You know, the model will never see a cat or a dog with

367
00:23:25,599 --> 00:23:29,000
one pixel being different than the rest.

368
00:23:29,000 --> 00:23:30,519
It would be too discontinuous.

369
00:23:30,519 --> 00:23:32,200
So, instead, there is another method.

370
00:23:32,200 --> 00:23:33,319
I'm not gonna go into the detail,

371
00:23:33,319 --> 00:23:38,519
but I link the paper which is way more common called integrated gradients.

372
00:23:38,519 --> 00:23:40,920
Um, and integrated gradients, uh,

373
00:23:40,920 --> 00:23:44,119
the idea is that instead of doing that directly by taking

374
00:23:44,119 --> 00:23:46,799
the ds of dog over dx,

375
00:23:46,799 --> 00:23:50,480
we're gonna take an image of the animal and we're gonna, uh,

376
00:23:50,480 --> 00:23:55,119
generate, you know, sort of many pictures that are, uh,

377
00:23:55,119 --> 00:23:59,119
taking, uh, dark, completely black zeros, um,

378
00:23:59,119 --> 00:24:02,039
a pixel all the way to the animal, um,

379
00:24:02,039 --> 00:24:09,000
the final image and then we're going to look at the path of gradients across all of this update,

380
00:24:09,000 --> 00:24:11,440
and it's going to be way more interpretive.

381
00:24:11,440 --> 00:24:12,759
I'm not gonna go into the details,

382
00:24:12,759 --> 00:24:15,920
but integrated gradients is just an extension of saliency maps

383
00:24:15,920 --> 00:24:20,920
that happens to use a different formula with an integration and is way more common.

384
00:24:20,920 --> 00:24:22,680
Um, if you look at it practically,

385
00:24:22,680 --> 00:24:24,559
this is an example from the medical field.

386
00:24:24,559 --> 00:24:27,119
Here is a- an image of a retina,

387
00:24:27,119 --> 00:24:30,440
and, um, if you perform the integrated gradients,

388
00:24:30,440 --> 00:24:32,200
you would see that, uh,

389
00:24:32,200 --> 00:24:35,559
the original, um, image, uh,

390
00:24:35,559 --> 00:24:41,759
the- the- the- the annotation for the les- lesions are exactly where the model is looking at.

391
00:24:41,759 --> 00:24:46,470
When it's giving you a probability that there is a lesion.

392
00:24:46,470 --> 00:24:50,190
Okay? So second method called integrated gradients.

393
00:24:50,190 --> 00:24:52,670
Let's push it a little further.

394
00:24:52,670 --> 00:24:56,829
Um, the next case study is, uh, that, you know,

395
00:24:56,829 --> 00:25:01,230
you- you- you- you- you want to now, uh,

396
00:25:01,230 --> 00:25:05,269
tell them a little more about the decision process, um,

397
00:25:05,269 --> 00:25:10,069
of the model, uh, with, uh,

398
00:25:10,069 --> 00:25:12,670
I guess- I guess let me- let me rephrase.

399
00:25:12,670 --> 00:25:15,950
The saliency maps looked at the pixel level.

400
00:25:15,950 --> 00:25:20,069
Uh, what- what you can do in order to give a better intuition, uh,

401
00:25:20,069 --> 00:25:24,069
which was mentioned earlier is another approach, uh,

402
00:25:24,069 --> 00:25:25,910
called occlusion sensitivity,

403
00:25:25,910 --> 00:25:30,349
which is, uh, actually way more, uh, intuitive and simple,

404
00:25:30,349 --> 00:25:32,309
where you could actually, uh, you know,

405
00:25:32,309 --> 00:25:35,950
take the dog image and paste it into the CNN,

406
00:25:35,950 --> 00:25:37,990
and you would get a score of a dog.

407
00:25:37,990 --> 00:25:41,230
You could also overlay a dark square.

408
00:25:41,230 --> 00:25:47,109
So zero out or mask partially the input image and give it to the same CNN,

409
00:25:47,109 --> 00:25:53,349
and track the modifications on the score of dog that you're tracking.

410
00:25:53,349 --> 00:25:55,349
If you actually do that,

411
00:25:55,349 --> 00:26:00,329
you can plot a probability map of how is the score of dog changing,

412
00:26:00,329 --> 00:26:04,509
as I move the dark square through the image.

413
00:26:04,509 --> 00:26:06,549
So let's do it together.

414
00:26:06,549 --> 00:26:08,509
I'm gonna say that, you know,

415
00:26:08,509 --> 00:26:12,109
this one when you- when you actually put the dark square on the top left of the image,

416
00:26:12,109 --> 00:26:13,630
the score of dog is unchanged.

417
00:26:13,630 --> 00:26:14,710
It's still very high.

418
00:26:14,710 --> 00:26:16,990
Now, you, uh, move, uh,

419
00:26:16,990 --> 00:26:19,029
the square a little bit to the right,

420
00:26:19,029 --> 00:26:22,170
and you see that it's still very high, the score of dog.

421
00:26:22,170 --> 00:26:24,230
You do it again, still very high.

422
00:26:24,230 --> 00:26:29,309
Now, uh, the square is partially occluding the face of the dog,

423
00:26:29,309 --> 00:26:32,309
and you should see the score of dog drop,

424
00:26:32,309 --> 00:26:35,470
if the model is in fact looking at the dog.

425
00:26:35,470 --> 00:26:37,150
And you perform that many times,

426
00:26:37,150 --> 00:26:40,269
so you scan, uh, through the image with your dark square,

427
00:26:40,269 --> 00:26:43,190
and you plot what we call, um, you know,

428
00:26:43,190 --> 00:26:49,509
the, uh, the probability map of the true class for different positions of the gray square.

429
00:26:49,509 --> 00:26:52,910
Does that make sense? So pretty simple,

430
00:26:52,910 --> 00:26:55,150
computationally expensive though.

431
00:26:55,150 --> 00:26:59,430
Just have to rerun the image so many times through the model.

432
00:26:59,430 --> 00:27:02,470
Here, practical examples, um,

433
00:27:02,470 --> 00:27:05,390
to look at the first one, uh,

434
00:27:05,390 --> 00:27:09,349
the true label is a Pomeranian cute dog,

435
00:27:09,349 --> 00:27:11,990
and you see that the model is, uh,

436
00:27:11,990 --> 00:27:19,230
failing to recognize the true class when the square is overlapping with sort of the center of the face.

437
00:27:19,230 --> 00:27:22,630
Which makes sense because here the true class that we're tracking is not dog,

438
00:27:22,630 --> 00:27:27,029
it's Pomeranian, and I could see how if you occlude the face,

439
00:27:27,029 --> 00:27:29,750
it's hard to get the breed of the dog.

440
00:27:29,750 --> 00:27:33,230
The second example, the true label is a,

441
00:27:33,269 --> 00:27:35,349
a car wheel, sorry, I hadn't shown you that.

442
00:27:35,349 --> 00:27:37,029
Uh, the true label is a car wheel,

443
00:27:37,029 --> 00:27:41,029
and you can see that when the square is on the wheel, um, it is,

444
00:27:41,029 --> 00:27:46,509
um, it is in fact dropping in terms of the true class probability.

445
00:27:46,509 --> 00:27:48,750
And then finally, the Afghan hound.

446
00:27:48,750 --> 00:27:54,470
What's interesting about that third example is the probability is dropping when the square is on the dog,

447
00:27:54,470 --> 00:27:57,069
but it's also increasing, uh,

448
00:27:57,069 --> 00:28:00,829
when the square is on the face of the human on the left.

449
00:28:00,869 --> 00:28:02,910
Which means that if you actually occlude the,

450
00:28:02,910 --> 00:28:03,869
the face of the human,

451
00:28:03,869 --> 00:28:08,670
the model thinks even more that the true class is in fact an Afghan hound.

452
00:28:08,670 --> 00:28:15,009
You're just removing additional unnecessary information for it to discover the true class.

453
00:28:15,009 --> 00:28:17,250
So this model seems to be doing well, right?

454
00:28:17,250 --> 00:28:19,250
It seems to be looking in the right place.

455
00:28:19,250 --> 00:28:23,299
We call that occlusion sensitivity,

456
00:28:23,299 --> 00:28:28,579
pretty simple, another tool with saliency map and integrated gradients in your toolkit.

457
00:28:28,579 --> 00:28:34,289
Let's push it slightly further.

458
00:28:34,809 --> 00:28:41,660
Um, here, uh, here we're,

459
00:28:41,660 --> 00:28:43,339
we're given, um, you know,

460
00:28:43,339 --> 00:28:45,619
along with the classification outputs,

461
00:28:45,619 --> 00:28:50,980
we- the zoo wants a real-time visualization of the model's decision process,

462
00:28:50,980 --> 00:28:53,619
and you have one day to show that.

463
00:28:53,619 --> 00:28:55,420
And we're talking about convolutions again.

464
00:28:55,420 --> 00:28:57,940
What do you do? So the important part is to know,

465
00:28:57,940 --> 00:28:59,420
uh, since the,

466
00:28:59,420 --> 00:29:02,700
the methods that we've seen so far are sort of

467
00:29:02,700 --> 00:29:07,299
post, uh, methods where you analyze the output or you show something.

468
00:29:07,299 --> 00:29:10,579
Here we're looking at a sort of ideally a,

469
00:29:10,579 --> 00:29:13,059
a module that we could plug in our network that would

470
00:29:13,059 --> 00:29:16,180
constantly give us the decision-making process of the network,

471
00:29:16,180 --> 00:29:18,460
or at least where it's looking at.

472
00:29:18,460 --> 00:29:26,769
How would you do this?

473
00:29:26,769 --> 00:29:28,250
This is our network by the way.

474
00:29:28,250 --> 00:29:29,529
We're taking an input,

475
00:29:29,529 --> 00:29:30,950
we're adding zero padding,

476
00:29:30,950 --> 00:29:34,490
and we have a series of conv relu max pool blocks.

477
00:29:34,490 --> 00:29:36,450
And then at the end we flatten,

478
00:29:36,450 --> 00:29:39,569
we have a triple fully connected layer,

479
00:29:39,569 --> 00:29:43,289
a softmax, and we get our probability output for classification.

480
00:29:43,289 --> 00:29:47,650
So just first question,

481
00:29:47,650 --> 00:29:52,089
where do you think is the weakness of this network when it

482
00:29:52,089 --> 00:29:56,009
comes to interpreting where the model is looking at on a picture?

483
00:29:56,009 --> 00:30:08,950
A part of this architecture is very- makes interpreting,

484
00:30:08,950 --> 00:30:10,789
uh, the network way harder. Yeah.

485
00:30:10,789 --> 00:30:12,470
The fully connected layer is there?

486
00:30:12,470 --> 00:30:13,150
Why?

487
00:30:13,150 --> 00:30:29,859
Because.

488
00:30:29,859 --> 00:30:31,099
Yeah. Totally right.

489
00:30:31,099 --> 00:30:32,940
The fully connected layers,

490
00:30:32,940 --> 00:30:34,819
you're looking at all the pieces at the same time,

491
00:30:34,819 --> 00:30:36,900
you're mixing everything and you're doing it three times.

492
00:30:36,900 --> 00:30:39,140
So by the end of those three layers,

493
00:30:39,140 --> 00:30:42,220
the information has been mixed together, essentially.

494
00:30:42,220 --> 00:30:45,180
You do not find the localized information that you had

495
00:30:45,180 --> 00:30:48,200
pre that with the max pools and the conv layers.

496
00:30:48,200 --> 00:30:52,740
So how could you change that layer in order to avoid that?

497
00:30:52,740 --> 00:30:55,180
How would you modify your network if you wanted to retain

498
00:30:55,180 --> 00:30:56,460
maybe the, the,

499
00:30:56,460 --> 00:31:10,160
the performance of the model but not lose that localized information?

500
00:31:10,160 --> 00:31:12,359
Yeah. Good idea. How, how could we,

501
00:31:12,359 --> 00:31:14,279
instead of doing three, can we do one?

502
00:31:14,279 --> 00:31:18,000
Can we, you know, still have a layer that makes the,

503
00:31:18,000 --> 00:31:19,839
uh, you know, makes the interpretation easy?

504
00:31:19,839 --> 00:31:22,359
Actually, there's another trick which we're gonna see,

505
00:31:22,359 --> 00:31:24,759
but it's similar to what you described.

506
00:31:24,759 --> 00:31:28,359
Let's say we convert this network into, uh,

507
00:31:28,359 --> 00:31:29,640
something where the, you know,

508
00:31:29,640 --> 00:31:33,279
the flattening of the pixels and the fully connected layers are

509
00:31:33,279 --> 00:31:40,119
converted into a single global average pooling layer and a fully connected layer.

510
00:31:40,119 --> 00:31:44,119
So here we reduce from three to one the fully connected layers.

511
00:31:44,119 --> 00:31:46,880
We still need our fully connected layers in our softmax because it's

512
00:31:46,880 --> 00:31:50,720
a classification task and we want a good decision engine at the end.

513
00:31:50,720 --> 00:31:52,039
Uh, but we converted,

514
00:31:52,039 --> 00:31:53,759
we, we added a global average pooling.

515
00:31:53,759 --> 00:31:55,160
So let me explain why,

516
00:31:55,160 --> 00:31:56,880
why this is, uh, better.

517
00:31:56,880 --> 00:32:03,599
So the last conv block essentially is giving us a volume.

518
00:32:03,599 --> 00:32:05,400
For the sake of simplicity,

519
00:32:05,400 --> 00:32:11,359
let's say that volume is a four by four with six channels, okay?

520
00:32:11,359 --> 00:32:14,079
And I color-coded them for simplicity.

521
00:32:14,079 --> 00:32:19,880
So each of these channels is a feature map that is resulting from

522
00:32:19,880 --> 00:32:24,359
a filter being scanned through the previous input, right?

523
00:32:24,359 --> 00:32:26,480
Everybody's clear on that?

524
00:32:26,480 --> 00:32:33,279
So global average pooling is gonna take each of these channels,

525
00:32:33,279 --> 00:32:38,240
feature maps, and is going to average them in a single number.

526
00:32:38,240 --> 00:32:42,960
So if you take the orange matrix and you average it,

527
00:32:42,960 --> 00:32:44,880
it gives you one of 4.7.

528
00:32:44,880 --> 00:32:46,960
You do the same thing with the green one,

529
00:32:46,960 --> 00:32:49,480
the blue one, all six of them,

530
00:32:49,480 --> 00:32:55,960
and you get a volume or call it a vector of size six, one, one, six.

531
00:32:55,960 --> 00:32:58,279
So why is that interesting?

532
00:32:58,279 --> 00:33:01,960
Because we actually did not lose the localized information.

533
00:33:01,960 --> 00:33:03,720
Um, we did not mix things up.

534
00:33:03,720 --> 00:33:09,640
We just assigned a single number to a feature map that we retained.

535
00:33:09,640 --> 00:33:14,680
So the localized information is still there on the previous volume.

536
00:33:14,680 --> 00:33:17,880
And now we can treat that as a vector that goes through

537
00:33:17,880 --> 00:33:20,359
a decision engine or a fully connected layer

538
00:33:20,359 --> 00:33:24,079
that ultimately goes through our softmax and give us the probabilities.

539
00:33:24,079 --> 00:33:30,559
So this architecture is easier to trace back to localized information in

540
00:33:30,559 --> 00:33:33,319
the input space because you can actually look at,

541
00:33:33,319 --> 00:33:39,559
let's say, one of the score of dog and you can look at the weights of each of

542
00:33:39,559 --> 00:33:43,480
these edges that tell you how much has

543
00:33:43,480 --> 00:33:49,210
the feature map from the volume before contributed to that score.

544
00:33:49,210 --> 00:33:51,690
So in other words, if I had to summarize,

545
00:33:51,690 --> 00:33:53,250
let's say the feature map looks like this.

546
00:33:53,250 --> 00:33:55,049
So this feature map is very high,

547
00:33:55,049 --> 00:34:00,450
has somehow activated heavily in some portion of the input image.

548
00:34:00,450 --> 00:34:04,690
The others similarly have activated to other things.

549
00:34:04,690 --> 00:34:07,809
You're taking the weights from your fully connected layer.

550
00:34:07,809 --> 00:34:09,409
By the way, you have to retrain that layer.

551
00:34:09,409 --> 00:34:12,250
You have to just train that layer, that last one.

552
00:34:12,250 --> 00:34:14,949
And then you sum all of them and it gives you what we

553
00:34:14,949 --> 00:34:21,630
call a class activation map for the class that you're visualizing.

554
00:34:21,630 --> 00:34:24,909
So you're overlaying those last six feature maps

555
00:34:24,909 --> 00:34:28,349
and you're weighing them with the weights of the last fully connected layer.

556
00:34:28,349 --> 00:34:29,590
You're not losing information.

557
00:34:29,590 --> 00:34:31,789
You're not mixing three fully connected layers

558
00:34:31,789 --> 00:34:39,070
that are impossible to trace back to the input space.

559
00:34:39,070 --> 00:34:43,869
Okay. So if you now give it an input image and you overlay

560
00:34:43,869 --> 00:34:45,909
the class activation map for the score of dog,

561
00:34:45,909 --> 00:34:47,389
which you can do for other classes.

562
00:34:47,389 --> 00:34:49,349
You can do the same thing for the class of cats.

563
00:34:49,349 --> 00:34:51,710
Look at the different weights, the feature maps.

564
00:34:51,710 --> 00:34:53,070
And maybe for the class of cat,

565
00:34:53,070 --> 00:34:55,510
the weights will certainly be different.

566
00:34:55,510 --> 00:34:58,550
So the contribution of each feature map will be different.

567
00:34:58,550 --> 00:35:00,789
Yeah. And this is what you get.

568
00:35:00,789 --> 00:35:03,949
This is called class activation map.

569
00:35:04,150 --> 00:35:07,909
And it's from folks over at Berkeley.

570
00:35:07,909 --> 00:35:10,349
And so here's a video that describes it.

571
00:35:10,349 --> 00:35:11,869
It runs really quickly.

572
00:35:11,869 --> 00:35:16,070
You can think of it as a slight modification to a vision network

573
00:35:16,070 --> 00:35:18,150
that can allow you to unpack what's

574
00:35:18,150 --> 00:35:21,429
happening inside and what's the decision process.

575
00:35:21,429 --> 00:35:25,329
There is also sort of an improvement to

576
00:35:25,329 --> 00:35:29,329
the CAM or class activation map algorithm called GradCAM,

577
00:35:29,329 --> 00:35:31,550
which enhances that method.

578
00:35:31,550 --> 00:35:41,980
Okay. Any questions on class activation maps?

579
00:35:41,980 --> 00:35:48,119
So we're getting to know convolutions a little better.

580
00:35:48,119 --> 00:35:58,610
Yeah. Yeah. So you were saying in the video,

581
00:35:58,610 --> 00:36:02,690
it seems like the model sometimes is looking at meaningless things.

582
00:36:02,690 --> 00:36:04,530
Yeah. I mean, it's not surprising, frankly.

583
00:36:04,530 --> 00:36:06,969
This is the previous generation of models.

584
00:36:06,969 --> 00:36:10,210
And on top of that, you're looking on a video.

585
00:36:10,210 --> 00:36:12,670
The model is a classification network.

586
00:36:12,670 --> 00:36:17,250
So it might look sometimes at things that are not even labeled.

587
00:36:17,250 --> 00:36:19,409
And so it has to find the closest one.

588
00:36:19,409 --> 00:36:20,889
It might not make sense at all.

589
00:36:20,889 --> 00:36:24,449
That's why you build that type of module to visualize and

590
00:36:24,449 --> 00:36:28,369
understand like the network's actually not working that well.

591
00:36:28,369 --> 00:36:32,409
But maybe on the main objects

592
00:36:32,409 --> 00:36:33,889
that you actually want for your task,

593
00:36:33,889 --> 00:36:36,809
let's say the zoo wants to do very well with cats and dogs.

594
00:36:36,809 --> 00:36:38,849
You can verify that when a cat is moving even at

595
00:36:38,849 --> 00:36:41,570
fast speed the model is quickly looking at it.

596
00:36:41,570 --> 00:36:47,079
Super. Let's do a couple more methods

597
00:36:47,079 --> 00:36:51,440
because it's going to build our intuition for frontier models.

598
00:36:51,440 --> 00:36:55,159
Um, so now the zoo trusts you.

599
00:36:55,159 --> 00:36:58,380
It trusts that the model correctly locates animals.

600
00:36:58,380 --> 00:37:05,519
But they get sort of scared and they wonder if the model understands what a dog is.

601
00:37:05,519 --> 00:37:08,480
Like, does it understand actually what a dog is or is it

602
00:37:08,480 --> 00:37:11,719
just like pattern matching random things?

603
00:37:11,719 --> 00:37:14,840
Um, you know, how could you,

604
00:37:14,840 --> 00:37:20,599
um, take this ConvNet and sort of query

605
00:37:20,599 --> 00:37:23,559
what the dog- what the model thinks a dog is?

606
00:37:23,559 --> 00:37:24,960
How would you do that?

607
00:37:24,960 --> 00:37:31,690
If you- how could you ask the model what's your best representation of a dog?

608
00:37:31,690 --> 00:37:57,130
Okay. Yeah. You did say two things.

609
00:37:57,130 --> 00:37:59,449
So get an image,

610
00:37:59,449 --> 00:38:03,489
so a forged image that maximizes the probability of dog.

611
00:38:03,489 --> 00:38:06,769
Yeah. Let's do that actually and then on your second point,

612
00:38:06,769 --> 00:38:10,849
um, um, about, uh, reverse engineering,

613
00:38:10,849 --> 00:38:12,730
we're going to look at the method there as well.

614
00:38:12,730 --> 00:38:13,789
But yeah, I agree.

615
00:38:13,789 --> 00:38:16,309
You could, um, so how would you concretely do that?

616
00:38:16,309 --> 00:38:20,940
Like, what would you maximize?

617
00:38:20,940 --> 00:38:23,780
Okay. Actually, what we said earlier,

618
00:38:23,780 --> 00:38:28,840
but I- I think you came right after that the- is we would not take the softmax output.

619
00:38:28,840 --> 00:38:32,059
Because the softmax output is dependent on other classes.

620
00:38:32,059 --> 00:38:35,739
You divide by the sum of the exponentials of other classes.

621
00:38:35,739 --> 00:38:40,519
And so you could actually maximize the softmax output by not maximizing the class you want,

622
00:38:40,519 --> 00:38:42,940
but by, uh, minimizing the other classes,

623
00:38:42,940 --> 00:38:44,860
which is different than what you want.

624
00:38:44,860 --> 00:38:47,219
So, um, here's what we'll do.

625
00:38:47,219 --> 00:38:51,300
We'll define the loss function where we take the pre-softmax score of dog,

626
00:38:51,300 --> 00:38:53,139
so the thing right before the softmax,

627
00:38:53,139 --> 00:38:55,460
which is only dependent on that specific class,

628
00:38:55,460 --> 00:38:58,980
and we might also regularize it to make sure it looks natural.

629
00:38:58,980 --> 00:39:06,559
The reason we want the regularization term is because pixels need to be between 0 and 255, roughly.

630
00:39:06,559 --> 00:39:09,860
And so you don't want to run an optimization, uh,

631
00:39:09,860 --> 00:39:14,780
a problem where pixels can have values that go all over the place.

632
00:39:14,780 --> 00:39:17,500
It's just not gonna look good to the human eye.

633
00:39:17,500 --> 00:39:20,619
Um, and so we're gonna do that.

634
00:39:20,619 --> 00:39:22,900
We're gonna run a gradient ascent algorithm.

635
00:39:22,900 --> 00:39:25,980
So similar to what we've seen in some of the previous classes,

636
00:39:25,980 --> 00:39:29,780
um, where we are gonna update the pixels of an input image,

637
00:39:29,780 --> 00:39:32,460
a completely random input image,

638
00:39:32,460 --> 00:39:35,659
until we can maximize the loss function we defined.

639
00:39:35,659 --> 00:39:38,219
Um, so we forward-propagated the random image,

640
00:39:38,219 --> 00:39:39,579
we compute the objective,

641
00:39:39,579 --> 00:39:41,320
we back-propagate, um,

642
00:39:41,320 --> 00:39:43,159
all the way back to the pixels,

643
00:39:43,159 --> 00:39:46,000
and then we update the pixels to maximize that objective.

644
00:39:46,000 --> 00:39:49,199
And we do that many times until, uh,

645
00:39:49,199 --> 00:39:52,280
we end up with something that might look like this.

646
00:39:52,280 --> 00:39:59,079
So let's say we take the score of a Dalmatian, um, you know,

647
00:39:59,079 --> 00:40:02,719
here, um, researchers, um,

648
00:40:02,719 --> 00:40:04,159
and- and, uh, showed,

649
00:40:04,159 --> 00:40:07,119
and this is work for Jason- from Jason Yosinski,

650
00:40:07,119 --> 00:40:11,440
um, shows that you can start seeing the model.

651
00:40:11,440 --> 00:40:13,340
If you ask the model, what is a Dalmatian,

652
00:40:13,340 --> 00:40:19,820
it will tell you it's something with black dots on a white background, roughly.

653
00:40:19,820 --> 00:40:23,260
So actually might not understand fully what the dog is,

654
00:40:23,260 --> 00:40:25,599
but it- that's what it thinks it is.

655
00:40:25,599 --> 00:40:29,380
So we just unpacked it a little bit and queried, uh, queried that.

656
00:40:29,380 --> 00:40:32,260
Another interesting one is if you look at,

657
00:40:32,260 --> 00:40:34,019
um, the goose.

658
00:40:34,019 --> 00:40:37,219
So here, the- the- the top left, um,

659
00:40:37,219 --> 00:40:43,599
labeled, uh, goose for the model is many of them.

660
00:40:43,599 --> 00:40:46,360
What does that mean? It means probably the model has seen

661
00:40:46,360 --> 00:40:51,900
a bunch of geese all the time together and has rarely seen a single one.

662
00:40:51,900 --> 00:40:57,320
And maybe the labeled data was labeling that as goose when it was geese.

663
00:40:57,320 --> 00:41:00,059
And so the model actually doesn't understand that it's a single one.

664
00:41:00,059 --> 00:41:03,480
It thinks that all of them are the label.

665
00:41:03,480 --> 00:41:07,909
Does that make sense? Okay.

666
00:41:07,909 --> 00:41:10,849
Super. So that's called class model visualization.

667
00:41:10,849 --> 00:41:12,150
You can actually- oh, sorry,

668
00:41:12,150 --> 00:41:13,949
I wasn't showing what I was talking about.

669
00:41:13,949 --> 00:41:14,869
Or no, I was showing.

670
00:41:14,869 --> 00:41:19,349
The, uh, the- the- the way to

671
00:41:19,349 --> 00:41:22,949
improve those visualization is just to change some of the regularization method.

672
00:41:22,949 --> 00:41:26,550
So the researchers have shown that you can actually, um,

673
00:41:26,550 --> 00:41:30,269
add more color by regularizing better so it looks better to the human eye.

674
00:41:30,269 --> 00:41:34,030
And then it becomes easier to query the model for a variety of classes,

675
00:41:34,030 --> 00:41:36,989
just to make sure that it understood those classes.

676
00:41:36,989 --> 00:41:40,750
And so same with flamingos, um,

677
00:41:40,750 --> 00:41:45,909
actually the label flamingo to the model feels like many flamingos.

678
00:41:45,909 --> 00:41:51,710
Just something you can observe.

679
00:41:51,710 --> 00:41:53,510
Any questions on, uh,

680
00:41:53,510 --> 00:41:55,230
class model visualization?

681
00:41:55,230 --> 00:41:58,309
Nothing super new, just another, um,

682
00:41:58,309 --> 00:42:01,130
you know, tool in your kit.

683
00:42:01,130 --> 00:42:04,469
It turns out you can apply the same,

684
00:42:04,469 --> 00:42:07,949
uh, type of method as class model visualization.

685
00:42:07,949 --> 00:42:10,869
But instead of doing it at the class level,

686
00:42:10,869 --> 00:42:13,630
uh, you do it in, uh,

687
00:42:13,630 --> 00:42:15,949
an intermediary activation.

688
00:42:15,949 --> 00:42:17,949
So you could actually do the same exercise,

689
00:42:17,949 --> 00:42:20,190
sort of what you were saying earlier with

690
00:42:20,190 --> 00:42:23,650
the masking of the later layer and just looking inside the network.

691
00:42:23,650 --> 00:42:26,269
You could pick an activation in the network,

692
00:42:26,269 --> 00:42:29,190
create an objective function with the regularization,

693
00:42:29,190 --> 00:42:30,869
and say, hey,

694
00:42:30,869 --> 00:42:34,869
show me the input picture that maximizes this activation.

695
00:42:34,869 --> 00:42:44,820
And that should tell you what is the input that maximizes activation the most.

696
00:42:44,820 --> 00:42:47,820
Right? Okay.

697
00:42:47,820 --> 00:42:50,699
So that's class model visualization,

698
00:42:50,699 --> 00:42:55,800
which can also be applied with gradient ascent anywhere inside the network at any neuron.

699
00:42:55,800 --> 00:42:58,539
And that already gives you some sort of a,

700
00:42:58,539 --> 00:43:01,500
a method to look at the neuron level and say, hey,

701
00:43:01,500 --> 00:43:04,539
what's the input that maximizes the fake input that

702
00:43:04,539 --> 00:43:08,340
theoretically you could generate that maximizes that activation.

703
00:43:09,340 --> 00:43:14,059
The next method is actually the most commonly used,

704
00:43:14,059 --> 00:43:18,260
um, today because it's so simple and intuitive.

705
00:43:18,260 --> 00:43:19,940
It's a dataset search.

706
00:43:19,940 --> 00:43:24,139
So what you could actually do is to pick a filter.

707
00:43:24,139 --> 00:43:26,940
You, you, you just pick one filter,

708
00:43:26,940 --> 00:43:30,460
you pick its feature map among, you know,

709
00:43:30,460 --> 00:43:33,980
I guess you pick one feature map at some point in the network,

710
00:43:33,980 --> 00:43:37,900
such as at, at, at after this max pooling layer.

711
00:43:37,900 --> 00:43:43,460
And, um, so let's say you have 256 filters in that convolution layer.

712
00:43:43,460 --> 00:43:48,940
So you have 256 feature maps of size five by five.

713
00:43:48,940 --> 00:43:51,940
Um, and you, um,

714
00:43:51,940 --> 00:43:54,579
find across all your data,

715
00:43:54,579 --> 00:43:56,019
your validation sets,

716
00:43:56,019 --> 00:43:59,420
the top five image that, uh,

717
00:43:59,420 --> 00:44:03,030
maximize this feature map.

718
00:44:03,030 --> 00:44:06,469
Yeah. So you just track the activation in that feature map.

719
00:44:06,469 --> 00:44:10,829
You find the highest activation across all your data,

720
00:44:10,829 --> 00:44:13,230
and you find the top im- the top five images,

721
00:44:13,230 --> 00:44:14,909
and you can do that, you know, again.

722
00:44:14,909 --> 00:44:17,469
You would say this seems that the filter,

723
00:44:17,469 --> 00:44:22,309
um, that produced that feature map has learned to detect shirts.

724
00:44:22,309 --> 00:44:26,269
Because if you find the top five images that activated that feature map the most,

725
00:44:26,269 --> 00:44:28,150
that filter the most, uh,

726
00:44:28,150 --> 00:44:29,949
it's all images of shirts.

727
00:44:29,949 --> 00:44:34,250
If you were to find something like this,

728
00:44:34,250 --> 00:44:37,969
you would say it seems that the filter has learned to detect edges.

729
00:44:37,969 --> 00:44:47,769
And you could do that across every feature map to interpret your filters.

730
00:44:47,769 --> 00:44:53,369
Okay. Simple dataset search that can allow you to

731
00:44:53,369 --> 00:44:59,489
interpret if a, if a filter is reacting to something meaningful.

732
00:44:59,489 --> 00:45:04,920
So if you look at these pictures that I printed at the bottom of the slide,

733
00:45:04,920 --> 00:45:06,800
they're all cropped.

734
00:45:06,800 --> 00:45:09,119
So why are they cropped?

735
00:45:09,119 --> 00:45:15,239
They don't look like images from the dataset.

736
00:45:15,239 --> 00:45:26,639
The image is probably bigger than that, right?

737
00:45:26,639 --> 00:45:30,199
Hmm? The what?

738
00:45:30,199 --> 00:45:41,269
So- so we took- we- so we took the input image,

739
00:45:41,269 --> 00:45:45,150
we send it through the conv relu blocks,

740
00:45:45,150 --> 00:45:50,590
and then we pick a feature map in the fifth block, let's say.

741
00:45:50,590 --> 00:45:52,909
What is this feature map looking at?

742
00:45:52,909 --> 00:45:55,389
Is it looking at the whole image or no?

743
00:45:55,389 --> 00:45:58,750
Sorry, what is the- so we pick a feature map,

744
00:45:58,750 --> 00:46:02,030
and in that feature map we find the activation that's the highest.

745
00:46:02,030 --> 00:46:07,510
So let's say the activation is the row number five, column number three.

746
00:46:07,510 --> 00:46:10,389
If you pick that activation,

747
00:46:10,389 --> 00:46:14,309
does it have access to the entire input image or not?

748
00:46:14,309 --> 00:46:38,519
So you don't necessarily have access to the entire image.

749
00:46:38,519 --> 00:46:41,039
The best way to visualize it is at the first layer.

750
00:46:41,039 --> 00:46:42,440
Let's say on the first layer,

751
00:46:42,440 --> 00:46:43,719
you take the input image,

752
00:46:43,719 --> 00:46:45,280
you take a filter,

753
00:46:45,280 --> 00:46:46,840
and you run it through.

754
00:46:46,840 --> 00:46:51,159
Well, that activation in the feature map of the first layer is

755
00:46:51,159 --> 00:46:53,760
only going to see what the filter sees, right?

756
00:46:53,760 --> 00:46:56,000
When you go deeper in the network,

757
00:46:56,000 --> 00:47:00,719
do you see more or less on average of the image?

758
00:47:00,719 --> 00:47:05,719
A single activation, does it have access to more or less parts of the image?

759
00:47:05,719 --> 00:47:17,530
I'm saying more? Yeah, it's more.

760
00:47:17,530 --> 00:47:22,489
Let's look at it. So here's a picture 64 by 64 by three, let's say.

761
00:47:22,489 --> 00:47:24,929
We have a conv network,

762
00:47:25,889 --> 00:47:28,250
and after five layers,

763
00:47:28,250 --> 00:47:33,650
the last conv has 13 filters and leads to a 13 by 13.

764
00:47:33,650 --> 00:47:40,699
Sorry, it has 256 filters that leads to a 13 by 13 feature maps.

765
00:47:40,699 --> 00:47:44,019
If I look at this feature map,

766
00:47:44,019 --> 00:47:46,699
let's say that's the most activated.

767
00:47:46,699 --> 00:47:50,219
I trace back to the input space,

768
00:47:50,219 --> 00:47:54,619
it will have access to this part of the image, let's say.

769
00:47:54,619 --> 00:47:58,539
Now, if there was another conv relu block,

770
00:47:58,539 --> 00:48:00,539
and I was looking at a feature map,

771
00:48:00,539 --> 00:48:04,579
it would have even more abstraction of multiple portions of these squares.

772
00:48:04,579 --> 00:48:07,699
So it would actually see slightly more from the input image.

773
00:48:07,699 --> 00:48:09,300
Does that make sense?

774
00:48:09,300 --> 00:48:14,059
So that's how you would think about it.

775
00:48:14,059 --> 00:48:16,179
It makes sense because at the end of it,

776
00:48:16,179 --> 00:48:19,539
the last output has access to the entire image, right?

777
00:48:19,539 --> 00:48:27,639
Because all these things are adding up to the prediction.

778
00:48:27,639 --> 00:48:30,079
Okay. So the deeper the activation,

779
00:48:30,079 --> 00:48:31,360
the more it sees from the image,

780
00:48:31,360 --> 00:48:33,039
and that's why the images were cropped.

781
00:48:33,039 --> 00:48:35,679
They were just cropped based on tracing back

782
00:48:35,679 --> 00:48:38,239
what that activation was looking at in the input image,

783
00:48:38,239 --> 00:48:44,309
which you can do very simply computationally.

784
00:48:44,309 --> 00:48:47,469
Okay. So now we're going to look at our last method for

785
00:48:47,469 --> 00:48:51,670
convs which has to do with reverse engineering, a conv,

786
00:48:51,670 --> 00:48:54,630
and then we'll move to the frontier models.

787
00:48:54,630 --> 00:49:03,829
So remember this slide from when we introduced generative adversarial networks, GANs.

788
00:49:03,829 --> 00:49:08,550
I didn't talk too much about the generator architecture.

789
00:49:08,550 --> 00:49:10,349
I just said it was a neural network,

790
00:49:10,349 --> 00:49:14,389
but I did mention that something's weird about that network,

791
00:49:14,389 --> 00:49:17,869
which is that the input is way smaller than its output.

792
00:49:17,869 --> 00:49:19,690
The input is a vector z,

793
00:49:19,690 --> 00:49:23,929
the output is an image of more dimensions.

794
00:49:23,929 --> 00:49:31,090
It is very common that such a network needs to upsample and thus would use deconvolution.

795
00:49:31,090 --> 00:49:34,769
Sometimes in the literature you're going to see dimensions of

796
00:49:34,769 --> 00:49:42,409
deconvolutions as an upsampling network with the inputs is smaller than the output.

797
00:49:42,409 --> 00:49:45,809
Sometimes those are called transposed convolutions,

798
00:49:45,809 --> 00:49:49,179
but we're going to talk about why.

799
00:49:49,179 --> 00:49:53,739
Another example where you might run into upsampling is when you have

800
00:49:53,739 --> 00:49:57,500
an encoder-decoder type networks such as for segmentation.

801
00:49:57,500 --> 00:50:03,780
So let's say you're given an image of a cellular set of cells like this,

802
00:50:03,780 --> 00:50:06,099
and then you want to label segments,

803
00:50:06,099 --> 00:50:09,739
the pixels that belong to a cell just to find the different cells.

804
00:50:09,739 --> 00:50:15,940
Typically, you would use a set of convolutions that reduces the volume in height and width,

805
00:50:15,940 --> 00:50:19,980
and then you'll get information encoded in

806
00:50:19,980 --> 00:50:22,659
a dense format that you will then

807
00:50:22,659 --> 00:50:27,179
upsample because your output should be of the size of the inputs minus the number of channels,

808
00:50:27,179 --> 00:50:32,320
but at least every pixel should have its own class.

809
00:50:32,320 --> 00:50:38,360
So typically that would be a set of convolutions followed by deconvolutions.

810
00:50:38,360 --> 00:50:43,179
So you downsample, you upsample.

811
00:50:43,179 --> 00:50:45,739
Why am I talking about deconvolutions?

812
00:50:45,739 --> 00:50:49,219
Because we're going to try to reverse engineer

813
00:50:49,219 --> 00:50:54,820
conv networks by adding a module,

814
00:50:54,820 --> 00:51:00,659
a deconvolutional module that will take a specific activation and will reverse engineer

815
00:51:00,659 --> 00:51:06,940
the trace to verify what was the reason this activation was high.

816
00:51:06,940 --> 00:51:10,059
This idea is key not only for

817
00:51:10,059 --> 00:51:14,219
cons but for any network you'll think about in the future when you work on it,

818
00:51:14,219 --> 00:51:20,260
if you want to actually reverse engineer the reason a specific neuron has been active.

819
00:51:20,260 --> 00:51:24,900
So let's take the example of a 1D convolution.

820
00:51:24,900 --> 00:51:30,940
This is the most basic example just for the sake of understanding the math.

821
00:51:30,940 --> 00:51:38,940
And I give you an input x which has some padding with two zeros at the top,

822
00:51:38,940 --> 00:51:40,099
two zeros at the bottom,

823
00:51:40,099 --> 00:51:43,059
and then x1 through x8 values,

824
00:51:43,059 --> 00:51:51,579
and I send that input to a 1D convolution which has one single filter of size four with a stride of two.

825
00:51:51,579 --> 00:52:08,260
What's the size of my output y? Remember the formula.

826
00:52:08,260 --> 00:52:10,019
What? Five.

827
00:52:10,019 --> 00:52:12,260
Five, correct. Yes.

828
00:52:12,260 --> 00:52:17,739
I assume you did this, this.

829
00:52:17,739 --> 00:52:23,059
So you took the nx,

830
00:52:23,820 --> 00:52:26,980
you applied the formula and you floored it,

831
00:52:26,980 --> 00:52:31,619
you didn't forget to add one and then you ended up with five.

832
00:52:31,619 --> 00:52:38,960
Is that it? Yeah. Correct. Super.

833
00:52:38,960 --> 00:52:43,039
So we have our output of five,

834
00:52:43,039 --> 00:52:48,559
and let's say we define our filter size for w1, w2, w3, w4.

835
00:52:48,559 --> 00:52:51,599
It's actually easy to see that

836
00:52:51,599 --> 00:52:56,159
the conv1d can be written as a system of equations,

837
00:52:56,159 --> 00:53:02,480
where y1 equals w1 times zero plus w2 times zero because of the padding,

838
00:53:02,480 --> 00:53:09,119
plus w3 times x1 plus w2 times x- w4 times x2, right?

839
00:53:09,119 --> 00:53:17,159
You're just overlaying the filter on top of the first four indices of the inputs,

840
00:53:17,159 --> 00:53:19,719
and you're doing a dot product.

841
00:53:19,719 --> 00:53:22,079
Same thing with the second one,

842
00:53:22,079 --> 00:53:24,480
third one, all the way to the fifth one,

843
00:53:24,480 --> 00:53:27,920
and that's your system of equation that describes this conv1d.

844
00:53:27,920 --> 00:53:33,739
Now, because it's a system of equation,

845
00:53:33,739 --> 00:53:37,539
you could actually write it as a matrix multiplication.

846
00:53:37,539 --> 00:53:44,539
So you could say the conv1d is literally just a weight matrix that we multiply by the inputs.

847
00:53:44,539 --> 00:53:47,659
So the inputs and the output sizes we know,

848
00:53:47,659 --> 00:53:50,019
five-one and 12-one.

849
00:53:50,019 --> 00:53:57,539
So the weight matrix is necessarily a 5 by 12 weight matrix.

850
00:53:57,539 --> 00:54:02,780
So we just rewrote the conv1d as

851
00:54:02,780 --> 00:54:08,250
a single weight matrix that you multiply by the input, you get the output.

852
00:54:08,250 --> 00:54:12,090
If you were to draw this matrix,

853
00:54:12,090 --> 00:54:13,449
this is what it would look like.

854
00:54:13,449 --> 00:54:18,250
So it would be a matrix with values all along the diagonal,

855
00:54:18,250 --> 00:54:20,409
and the rest are zeros.

856
00:54:20,409 --> 00:54:34,219
So that's our conclusion, conv1d can be rewritten as a matrix vector multiplication.

857
00:54:34,219 --> 00:54:45,349
Everybody follows? So if you can write it as a matrix multiplication,

858
00:54:45,349 --> 00:54:48,110
remember we're talking about reverse engineering,

859
00:54:48,110 --> 00:54:53,829
then I could say d-conv is possible.

860
00:54:53,829 --> 00:54:55,989
It's possible to reverse engineer that.

861
00:54:55,989 --> 00:55:01,510
And in fact, I'm going to make a very big assumption that is not always true.

862
00:55:01,510 --> 00:55:04,550
Sometimes it's true, but for practical reasons we're in deep learning, right?

863
00:55:04,550 --> 00:55:06,389
It's an engineering field.

864
00:55:06,389 --> 00:55:10,349
We're going to assume that W is invertible.

865
00:55:10,349 --> 00:55:14,389
And so you can find a matrix H that is equal to the inverse of

866
00:55:14,389 --> 00:55:19,269
W such that X equals H Y.

867
00:55:19,269 --> 00:55:23,349
So that you're able to reverse engineer the signal.

868
00:55:23,349 --> 00:55:28,829
I'm going to make a second assumption in the sizes I printed,

869
00:55:28,829 --> 00:55:32,750
which is even bigger, is that W is not only invertible,

870
00:55:32,750 --> 00:55:34,110
it is also orthogonal,

871
00:55:34,110 --> 00:55:38,750
meaning that its inverse is its transpose.

872
00:55:38,750 --> 00:55:43,119
It happens that it's sometimes true.

873
00:55:43,119 --> 00:55:45,960
And in fact, if you think about an edge detector,

874
00:55:45,960 --> 00:55:50,159
so let's say our filter is minus 1, 0, 0, 1, 0, 0, 0, 1.

875
00:55:50,159 --> 00:55:52,440
It's an edge detector.

876
00:55:52,440 --> 00:55:54,079
Sorry, I have one too many zeros,

877
00:55:54,079 --> 00:55:56,000
but it's an edge detector.

878
00:55:56,000 --> 00:56:00,320
And it's actually, you know,

879
00:56:00,320 --> 00:56:05,539
invertible and its inverse is its transpose.

880
00:56:05,539 --> 00:56:09,619
And so that simplifies our reverse engineering,

881
00:56:09,619 --> 00:56:14,739
because we have a conv1d and we know that we can write it as

882
00:56:14,739 --> 00:56:20,019
a matrix vector multiplication and we can transpose that matrix to reverse it.

883
00:56:20,019 --> 00:56:22,500
And maybe it's not always true,

884
00:56:22,500 --> 00:56:27,420
but it's true enough for it to work in deep learning, pretty much.

885
00:56:27,420 --> 00:56:30,340
You're going to do that so many times, right?

886
00:56:30,539 --> 00:56:33,820
That's why in the literature,

887
00:56:33,820 --> 00:56:39,219
oftentimes, deconvolutions are called transposed convolutions.

888
00:56:39,219 --> 00:56:41,059
I gave you the 1D example,

889
00:56:41,059 --> 00:56:42,940
the 2D example is similar,

890
00:56:42,940 --> 00:56:44,619
it's just more complicated,

891
00:56:44,619 --> 00:56:46,860
there's more math, things mix up,

892
00:56:46,860 --> 00:56:49,539
but, you know, same idea.

893
00:56:49,900 --> 00:56:58,019
Now, there is a trick that makes it simpler to code the deconvolution,

894
00:56:58,019 --> 00:57:06,980
and to see that trick I just drew x equals w transpose y.

895
00:57:06,980 --> 00:57:10,739
So you have your x which is a vector of size 12,

896
00:57:10,739 --> 00:57:14,539
although there's two padding at the top or at the bottom.

897
00:57:14,539 --> 00:57:18,980
And then I transpose the w matrix that I was showing you,

898
00:57:18,980 --> 00:57:24,980
and then I multiply that by my vector y of size five.

899
00:57:24,980 --> 00:57:30,739
This is actually a transposed convolution with stride two,

900
00:57:30,739 --> 00:57:33,860
is equivalent to something slightly different,

901
00:57:33,860 --> 00:57:38,019
which is a sub-pixel convolution of stride one-half.

902
00:57:38,019 --> 00:57:39,780
It's a mathematical trick.

903
00:57:39,780 --> 00:57:43,050
You can do it at home,

904
00:57:43,050 --> 00:57:48,329
but you would see that these two operations are equivalent,

905
00:57:48,329 --> 00:57:52,170
meaning you can actually flip the filter.

906
00:57:52,170 --> 00:57:55,250
So you see in the left side of the screen,

907
00:57:55,250 --> 00:57:56,809
the filters are flipped.

908
00:57:56,809 --> 00:58:00,690
So if you look at the first row of my matrix,

909
00:58:00,690 --> 00:58:02,610
it's not w1 through w4,

910
00:58:02,610 --> 00:58:05,130
it's w4 through w1.

911
00:58:05,130 --> 00:58:11,730
So I flipped the filter and I scanned it all the way through the diagonal.

912
00:58:11,730 --> 00:58:15,690
I also used another trick which is my y vector,

913
00:58:15,690 --> 00:58:21,849
I inserted zeros in between the values, it's called sub-pixel.

914
00:58:21,849 --> 00:58:25,809
So I inserted zeros and I also added some padding.

915
00:58:25,809 --> 00:58:27,929
So a couple of tricks,

916
00:58:27,929 --> 00:58:31,650
but no need to remember it by heart.

917
00:58:31,650 --> 00:58:33,489
If there's anything you can remember,

918
00:58:33,489 --> 00:58:36,530
it's that implementing a deconvolution in

919
00:58:36,530 --> 00:58:40,610
the sub-pixel version I was describing is similar to a convolution.

920
00:58:40,610 --> 00:58:44,650
But what you do is you create a sub-pixel version of the inputs by

921
00:58:44,650 --> 00:58:48,530
adding zeros in between the values and padding it.

922
00:58:48,530 --> 00:58:54,420
You flip the filters and you divide the stride by two,

923
00:58:54,420 --> 00:58:56,099
and that's what a deconvolution is.

924
00:58:56,099 --> 00:59:02,539
So if you have a convolutional neural network and you want to reverse engineer it,

925
00:59:02,539 --> 00:59:04,539
you take the filters,

926
00:59:04,539 --> 00:59:10,340
you flip them, you create a sub-pixel version of the inputs,

927
00:59:10,340 --> 00:59:12,099
you divide the stride by two,

928
00:59:12,099 --> 00:59:13,219
and you run the process,

929
00:59:13,219 --> 00:59:16,219
you will have reversed that convolution.

930
00:59:16,219 --> 00:59:19,900
The reason we're doing that is because we're

931
00:59:19,900 --> 00:59:22,619
just rewriting the convolution as another convolution,

932
00:59:22,619 --> 00:59:24,579
but the hyper-parameters are different.

933
00:59:24,579 --> 00:59:27,300
But it's easy to code, right?

934
00:59:27,300 --> 00:59:30,699
You're just reusing the same code, pretty much.

935
00:59:30,699 --> 00:59:35,579
So anyway, let's get back to our example here.

936
00:59:35,579 --> 00:59:39,619
We have an image of a dog,

937
00:59:39,619 --> 00:59:43,500
we run it through a conv net,

938
00:59:43,500 --> 00:59:49,420
and we pick at some point in that conv net a feature map.

939
00:59:49,420 --> 00:59:56,460
We pick one feature map only among the 256 possible feature maps right here,

940
00:59:56,460 --> 01:00:01,219
and we're going to look at the max activation of that feature map.

941
01:00:01,219 --> 01:00:02,500
So we find the max,

942
01:00:02,500 --> 01:00:03,900
let's say it's this one.

943
01:00:03,900 --> 01:00:09,179
So row two, column three is the maximum number of that feature map.

944
01:00:09,179 --> 01:00:11,179
What does it mean?

945
01:00:11,179 --> 01:00:14,019
It means the filter that led to that feature map,

946
01:00:14,019 --> 01:00:15,900
when it looked at its input,

947
01:00:15,900 --> 01:00:18,780
it was maximal in that location.

948
01:00:18,780 --> 01:00:22,500
It's maximally activated in that location.

949
01:00:22,500 --> 01:00:33,079
We zero out every other entries of this matrix,

950
01:00:33,079 --> 01:00:35,440
and then we reverse the network.

951
01:00:35,440 --> 01:00:38,159
So we max pooled, we unpooled.

952
01:00:38,159 --> 01:00:40,239
We relued, we do the reverse.

953
01:00:40,239 --> 01:00:41,840
We do a decom instead of the conv,

954
01:00:41,840 --> 01:00:43,599
which is a transposed convolution,

955
01:00:43,599 --> 01:00:47,199
sub-pixel version, flip the filter,

956
01:00:47,199 --> 01:00:48,960
divide the stride by two.

957
01:00:48,960 --> 01:00:51,000
And we do that how many times?

958
01:00:51,000 --> 01:00:53,000
Three times because we had three blocks.

959
01:00:53,000 --> 01:01:01,400
And then we should be able to reconstruct what this activation was maximally activated for,

960
01:01:01,400 --> 01:01:03,960
and we get the cropped version as we learned,

961
01:01:03,960 --> 01:01:07,519
the cropped part of the image that this activation was looking at,

962
01:01:07,519 --> 01:01:11,280
and exactly the pixels that maximize its value.

963
01:01:11,280 --> 01:01:17,469
Does that make sense? It's pretty complex,

964
01:01:17,469 --> 01:01:20,190
but it's important to know these methods because you

965
01:01:20,190 --> 01:01:22,590
might run into something similar in the future,

966
01:01:22,590 --> 01:01:26,550
or be asked to sort of interpret certain feature maps,

967
01:01:26,550 --> 01:01:30,789
certain activation maps, etc. Okay.

968
01:01:30,789 --> 01:01:35,150
So some additional details that we'll cover is what is unpooled,

969
01:01:35,150 --> 01:01:37,949
and why do we do some relu in there.

970
01:01:37,949 --> 01:01:43,070
So very simply, let's say I take the max pooling layer,

971
01:01:43,070 --> 01:01:44,550
I max pool this,

972
01:01:44,550 --> 01:01:47,230
filter size two by two,

973
01:01:47,230 --> 01:01:49,510
stride of two.

974
01:01:50,590 --> 01:01:53,949
If you wanted to unpool this,

975
01:01:53,949 --> 01:02:05,400
how would you do it? Are pooling layers,

976
01:02:05,400 --> 01:02:09,829
max pooling layers invertible?

977
01:02:10,269 --> 01:02:21,670
No? Why? Yeah.

978
01:02:21,670 --> 01:02:27,559
Yeah, exactly. It's not invertible because if you pick,

979
01:02:27,559 --> 01:02:30,559
you write the six here on the top left,

980
01:02:30,559 --> 01:02:33,079
you can tell that the six was in one of these four,

981
01:02:33,079 --> 01:02:35,000
but you don't know where it was.

982
01:02:35,000 --> 01:02:39,440
You can't invert. And it's very important to know where it was, right?

983
01:02:40,079 --> 01:02:42,480
So it's not invertible,

984
01:02:42,480 --> 01:02:46,239
but you could actually use a trick to make it invertible,

985
01:02:46,239 --> 01:02:50,119
which is passing what we call switches.

986
01:02:50,119 --> 01:02:52,840
So during the forward propagation,

987
01:02:52,840 --> 01:02:55,199
you look at all your pooling, your max pooling,

988
01:02:55,199 --> 01:02:57,360
and you remember with the binary matrix,

989
01:02:57,360 --> 01:02:59,119
a very lightweight matrix,

990
01:02:59,119 --> 01:03:00,840
where the pooling happened,

991
01:03:00,840 --> 01:03:02,559
where the max pooling happened.

992
01:03:02,559 --> 01:03:05,079
And then when you're doing the unpooling,

993
01:03:05,079 --> 01:03:06,320
you remember those switches,

994
01:03:06,320 --> 01:03:08,960
so you keep them in memory and you pass them back,

995
01:03:08,960 --> 01:03:12,440
and that should tell you where the value came from.

996
01:03:12,440 --> 01:03:15,760
Okay. So that's what we mean by unpooling.

997
01:03:15,760 --> 01:03:18,719
Okay. So I go back to my previous map.

998
01:03:18,719 --> 01:03:21,599
The only thing I need to change to be able to reverse engineer

999
01:03:21,599 --> 01:03:26,159
my network is to pass the switches and the filters, by the way,

1000
01:03:26,159 --> 01:03:29,079
because the decom is just the flip version of the filter,

1001
01:03:29,079 --> 01:03:32,840
with subpixel and stride divided by two.

1002
01:03:32,840 --> 01:03:35,039
And so I do that.

1003
01:03:35,039 --> 01:03:38,760
So you can see, you can literally invert your network here,

1004
01:03:38,760 --> 01:03:42,480
and trace back from one activation to the input space.

1005
01:03:42,480 --> 01:03:45,079
And then for ReLU is a little odd.

1006
01:03:45,079 --> 01:03:46,639
I'm not going to spend too much time on it

1007
01:03:46,639 --> 01:03:48,599
because it's more empirical than nothing.

1008
01:03:48,599 --> 01:03:52,159
But ReLU forward is essentially zeroing out

1009
01:03:52,159 --> 01:03:55,960
every value that is negative during the forward path.

1010
01:03:56,159 --> 01:03:59,320
Technically, a ReLU backward is impossible

1011
01:03:59,320 --> 01:04:01,719
unless you have also the switches.

1012
01:04:01,719 --> 01:04:06,440
If you have the switches, you could actually, you know,

1013
01:04:06,440 --> 01:04:09,280
pass linearly back whatever was kept

1014
01:04:09,280 --> 01:04:11,079
because it's the identity function.

1015
01:04:12,079 --> 01:04:14,039
But actually that would kill

1016
01:04:14,039 --> 01:04:17,320
your positive signal coming back.

1017
01:04:17,320 --> 01:04:21,559
So instead you just reuse ReLU, basically.

1018
01:04:21,559 --> 01:04:24,760
You reuse ReLU because you want to start

1019
01:04:24,800 --> 01:04:27,360
from the activation that is the highest on your feature map

1020
01:04:27,360 --> 01:04:29,760
and to keep passing the positive signal

1021
01:04:29,760 --> 01:04:31,199
back to the input space.

1022
01:04:31,199 --> 01:04:32,920
Don't worry too much about it.

1023
01:04:32,920 --> 01:04:35,599
It's just that ReLU is just passed as a ReLU

1024
01:04:35,599 --> 01:04:37,519
during the reconstruction process,

1025
01:04:37,519 --> 01:04:39,280
not as a proper ReLU backward.

1026
01:04:41,039 --> 01:04:42,719
Okay, so here we go.

1027
01:04:42,719 --> 01:04:46,280
We send our dog through the network.

1028
01:04:46,280 --> 01:04:50,000
We look at a specific max pool output.

1029
01:04:50,000 --> 01:04:51,599
We take the feature map.

1030
01:04:51,599 --> 01:04:53,920
We find the activation that is the highest

1031
01:04:53,960 --> 01:04:54,800
in that feature map.

1032
01:04:54,800 --> 01:04:56,599
We zero out all the rest.

1033
01:04:56,599 --> 01:04:58,199
We reverse engineer our network.

1034
01:04:58,199 --> 01:05:00,559
We find the cropped part of this dog

1035
01:05:00,559 --> 01:05:02,000
with the pixels that led

1036
01:05:02,000 --> 01:05:05,079
to that specific feature map shining.

1037
01:05:05,079 --> 01:05:08,519
We are interpreting the filter that led that feature map.

1038
01:05:08,519 --> 01:05:13,519
And you can do that anywhere across the network.

1039
01:05:13,679 --> 01:05:15,239
But of course, if you're earlier,

1040
01:05:15,239 --> 01:05:17,280
the crop is gonna be even smaller.

1041
01:05:18,119 --> 01:05:26,320
If you're later, generally bigger, okay?

1042
01:05:26,320 --> 01:05:30,880
So you learned dcoms slash transposed coms.

1043
01:05:30,880 --> 01:05:34,239
Now let's look at some practical visualizations

1044
01:05:34,239 --> 01:05:37,760
from Matthew Zehler and Rob Fergus.

1045
01:05:37,760 --> 01:05:39,599
These are great researchers

1046
01:05:39,599 --> 01:05:41,639
in the space of visualizations.

1047
01:05:41,639 --> 01:05:43,320
Been making so much progress.

1048
01:05:44,800 --> 01:05:48,480
So they train the network, okay?

1049
01:05:48,480 --> 01:05:51,840
They looked at results on a validation set

1050
01:05:51,840 --> 01:05:54,320
of 50,000 images.

1051
01:05:54,320 --> 01:05:58,920
And so what you're seeing is the first layer

1052
01:05:58,920 --> 01:06:01,159
and specifically the patches.

1053
01:06:01,159 --> 01:06:02,119
What the patches are,

1054
01:06:02,119 --> 01:06:05,920
they're the top nine strongest activation per filter.

1055
01:06:08,320 --> 01:06:10,559
So for each filter in the first layer,

1056
01:06:10,559 --> 01:06:14,679
they look at the top nine strongest activation

1057
01:06:14,679 --> 01:06:17,199
and they remember the data points

1058
01:06:17,199 --> 01:06:18,480
that was leading to that.

1059
01:06:18,480 --> 01:06:20,559
So that's the data set search method

1060
01:06:20,559 --> 01:06:21,880
that we saw together.

1061
01:06:21,880 --> 01:06:26,119
What are the nine images that led to that maximum,

1062
01:06:28,239 --> 01:06:32,000
that feature map activating the most?

1063
01:06:32,000 --> 01:06:32,840
Print them.

1064
01:06:32,840 --> 01:06:35,039
Those are the patches for each filter.

1065
01:06:35,039 --> 01:06:38,599
If you do that, you can already interpret

1066
01:06:38,599 --> 01:06:42,320
some of the filters by seeing that,

1067
01:06:42,320 --> 01:06:45,480
oh, this one reacts to edges that are diagonal

1068
01:06:45,480 --> 01:06:48,760
or this one reacts to edges that are straight,

1069
01:06:48,760 --> 01:06:50,239
for example.

1070
01:06:50,239 --> 01:06:54,119
On the bottom right, we actually print the filters raw

1071
01:06:54,119 --> 01:06:56,280
and of course, because it's the first layer,

1072
01:06:56,280 --> 01:06:57,320
it is interpretable.

1073
01:06:57,320 --> 01:06:59,800
So if in fact you have an edge detector,

1074
01:06:59,800 --> 01:07:01,800
you should see when you print that matrix

1075
01:07:01,800 --> 01:07:03,639
that it looks like an edge detector.

1076
01:07:04,519 --> 01:07:07,400
That doesn't work for layers beyond one.

1077
01:07:08,840 --> 01:07:10,760
So now let's go a little deeper.

1078
01:07:10,760 --> 01:07:13,039
Now we're going layer two

1079
01:07:13,039 --> 01:07:15,039
and we're looking at the decoms.

1080
01:07:15,039 --> 01:07:16,320
So what are they doing?

1081
01:07:17,239 --> 01:07:22,239
They're essentially looking at the top one's

1082
01:07:24,239 --> 01:07:27,519
strongest activation per feature in the second layer.

1083
01:07:27,519 --> 01:07:31,239
So the second layer has 256 feature maps.

1084
01:07:31,239 --> 01:07:32,519
They're presented here.

1085
01:07:34,239 --> 01:07:36,320
You pick one feature map.

1086
01:07:36,320 --> 01:07:41,320
You look across all these 50,000 validation images.

1087
01:07:41,480 --> 01:07:44,800
You find the maximum feature map.

1088
01:07:44,800 --> 01:07:47,360
You take the specific portion of that feature map,

1089
01:07:47,360 --> 01:07:50,159
the specific entry that's maximally activated.

1090
01:07:50,159 --> 01:07:52,360
You zero out the rest.

1091
01:07:52,360 --> 01:07:55,559
You do your decom, you pass the switches, blah, blah, blah

1092
01:07:55,559 --> 01:07:57,559
and you get the cropped part of the image

1093
01:07:57,559 --> 01:07:59,800
that represents why it was activated

1094
01:07:59,800 --> 01:08:01,880
and this is printed all over here.

1095
01:08:01,880 --> 01:08:04,400
And you can see, you can start interpreting that

1096
01:08:04,400 --> 01:08:07,079
by doing the top one or you can do the top nine.

1097
01:08:07,079 --> 01:08:09,360
And if you actually do the top nine,

1098
01:08:09,360 --> 01:08:12,679
you would start seeing that in fact,

1099
01:08:12,679 --> 01:08:15,639
certain filter have very clear purposes.

1100
01:08:15,639 --> 01:08:17,199
Some filters detect circles.

1101
01:08:17,199 --> 01:08:22,199
Some filters detect odd shapes.

1102
01:08:25,130 --> 01:08:25,970
Yep.

1103
01:08:26,970 --> 01:08:30,090
If you keep doing that in layer three,

1104
01:08:30,090 --> 01:08:33,369
you would start seeing with the decom method

1105
01:08:33,369 --> 01:08:36,529
that the filters are capturing more complex information.

1106
01:08:36,529 --> 01:08:38,970
Remember in the first lecture we did together,

1107
01:08:38,970 --> 01:08:40,970
I said that the deeper you go in the network,

1108
01:08:40,970 --> 01:08:44,689
the more the information adds up

1109
01:08:44,729 --> 01:08:46,850
and you get more complex features later.

1110
01:08:46,850 --> 01:08:51,329
This is a proof of that, pretty much.

1111
01:08:51,329 --> 01:08:55,770
Now if you go to layer three and you can do all of it.

1112
01:08:55,770 --> 01:08:57,850
You can do the top nine patches

1113
01:08:57,850 --> 01:09:00,850
where if the nine patches look very similar,

1114
01:09:00,850 --> 01:09:05,289
you can probably safely say that this filter

1115
01:09:05,289 --> 01:09:09,170
was responsible for this type of shape or color

1116
01:09:09,170 --> 01:09:13,770
or salient feature

1117
01:09:13,770 --> 01:09:18,170
and then you can do the decom as well, essentially.

1118
01:09:18,170 --> 01:09:22,369
Let's watch together a very short video of Jason Yosinski

1119
01:09:23,689 --> 01:09:26,529
that shows a little bit of everything we've learned together.

1120
01:09:26,529 --> 01:09:28,930
They can recognize school buses and zebras

1121
01:09:28,930 --> 01:09:29,770
and can tell the difference

1122
01:09:29,770 --> 01:09:33,170
between Maltese terriers and Yorkshire terriers.

1123
01:09:33,170 --> 01:09:34,050
We now know what it takes

1124
01:09:34,050 --> 01:09:35,890
to train these neural networks well,

1125
01:09:35,890 --> 01:09:37,050
but we don't know so much about

1126
01:09:37,050 --> 01:09:39,770
how they're actually computing their final answers.

1127
01:09:39,770 --> 01:09:42,729
We developed this interactive deep visualization toolbox

1128
01:09:42,729 --> 01:09:44,689
to shine light into these black boxes

1129
01:09:44,689 --> 01:09:47,569
showing what happens inside of neural nets.

1130
01:09:47,569 --> 01:09:50,090
In the top left corner, we show the input to the network

1131
01:09:50,090 --> 01:09:53,609
which can be a still image or video from a webcam.

1132
01:09:53,609 --> 01:09:54,810
These black squares in the middle

1133
01:09:54,810 --> 01:09:57,289
show the activations on a single layer of a network.

1134
01:09:57,289 --> 01:09:59,090
In this case, the popular deep neural network

1135
01:09:59,090 --> 01:10:01,770
called AlexNet running in CAFE.

1136
01:10:01,770 --> 01:10:03,010
By interacting with the network,

1137
01:10:03,010 --> 01:10:06,560
we can see what some of the neurons are doing.

1138
01:10:06,560 --> 01:10:08,520
For example, on this first layer,

1139
01:10:08,520 --> 01:10:10,199
the unit in the center responds strongly

1140
01:10:10,199 --> 01:10:13,520
to light to dark edges.

1141
01:10:13,520 --> 01:10:15,319
Its neighbor, one neuron over,

1142
01:10:15,319 --> 01:10:20,380
responds to edges in the opposite direction, dark to light.

1143
01:10:20,380 --> 01:10:23,140
Using optimization, we can synthetically produce images

1144
01:10:23,140 --> 01:10:24,739
that light up each neuron on this layer

1145
01:10:24,739 --> 01:10:27,300
to see what each neuron is looking for.

1146
01:10:27,300 --> 01:10:29,140
We can scroll through every layer of the network

1147
01:10:29,140 --> 01:10:31,619
to see what it does, including convolution,

1148
01:10:31,619 --> 01:10:34,899
pooling, and normalization layers.

1149
01:10:34,899 --> 01:10:35,939
We can switch back and forth

1150
01:10:35,939 --> 01:10:38,140
between showing the actual activations

1151
01:10:38,140 --> 01:10:41,460
and showing images synthesized to produce high activation.

1152
01:10:41,460 --> 01:10:44,500
This is a class model visualization method.

1153
01:10:44,500 --> 01:10:46,779
By the time we get to the fifth convolutional layer,

1154
01:10:46,819 --> 01:10:51,479
the features being computed represent abstract concepts.

1155
01:10:51,479 --> 01:10:54,680
For example, this neuron seems to respond to faces.

1156
01:10:54,680 --> 01:10:56,119
We can further investigate this neuron

1157
01:10:56,119 --> 01:10:58,680
by showing a few different types of information.

1158
01:10:58,680 --> 01:11:00,960
First, we can artificially create optimized images

1159
01:11:00,960 --> 01:11:02,800
using new regularization techniques

1160
01:11:02,800 --> 01:11:04,039
that are described in our paper.

1161
01:11:04,039 --> 01:11:06,840
That's the class model visualization.

1162
01:11:06,840 --> 01:11:09,079
This neuron responds to a face and shoulders.

1163
01:11:09,079 --> 01:11:10,640
We can also plot the images from the training set

1164
01:11:10,640 --> 01:11:12,720
to activate the smart modes. That's the data set search.

1165
01:11:12,720 --> 01:11:14,279
As well as pixels from those images

1166
01:11:14,279 --> 01:11:16,159
most responsible for the high activations.

1167
01:11:16,159 --> 01:11:17,000
And that's the decon.

1168
01:11:17,039 --> 01:11:19,079
This is a deconvolution technique.

1169
01:11:19,079 --> 01:11:20,760
This feature responds to multiple faces

1170
01:11:20,760 --> 01:11:22,560
in different locations.

1171
01:11:22,560 --> 01:11:24,199
And by looking at the decon,

1172
01:11:25,680 --> 01:11:27,439
we can see that it would respond more strongly

1173
01:11:27,439 --> 01:11:30,319
if we had even darker eyes and rose ear lips.

1174
01:11:30,319 --> 01:11:31,720
We can also confirm that it cares

1175
01:11:31,720 --> 01:11:33,159
about the head and shoulders

1176
01:11:33,159 --> 01:11:34,800
that ignores the arms and torso.

1177
01:11:36,039 --> 01:11:37,960
We can even see that it fires to some extent

1178
01:11:37,960 --> 01:11:39,119
for cat faces.

1179
01:11:40,359 --> 01:11:42,399
Using backprop or decon,

1180
01:11:42,399 --> 01:11:44,479
we can see that this unit depends most strongly

1181
01:11:44,479 --> 01:11:47,319
on a couple units in the previous layer, con four.

1182
01:11:47,319 --> 01:11:49,560
And on about a dozen or so in con three.

1183
01:11:50,520 --> 01:11:51,520
So because of decons,

1184
01:11:51,520 --> 01:11:54,199
you can trace back the entire layers before.

1185
01:11:54,199 --> 01:11:55,039
And where the-

1186
01:11:55,039 --> 01:11:56,199
In the top nine images.

1187
01:11:56,199 --> 01:11:58,079
Okay, I'm gonna leave it to you.

1188
01:11:58,079 --> 01:12:01,600
But you get the idea that these researchers

1189
01:12:01,600 --> 01:12:03,479
built a toolkit that essentially reproduces

1190
01:12:03,479 --> 01:12:05,039
some of the methods we've seen together.

1191
01:12:05,039 --> 01:12:06,199
Although we've seen more methods

1192
01:12:06,199 --> 01:12:07,600
than what's in the toolkit.

1193
01:12:07,600 --> 01:12:11,560
And so your kit is now able to answer

1194
01:12:11,560 --> 01:12:13,880
many questions about convolutions such as,

1195
01:12:13,920 --> 01:12:16,800
hey, what part of the input is responsible for the outputs?

1196
01:12:16,800 --> 01:12:19,399
We now know that we can use occlusion sensitivity

1197
01:12:19,399 --> 01:12:21,079
or class activation maps.

1198
01:12:22,000 --> 01:12:25,279
What is the role of a neuron, filter, layer?

1199
01:12:25,279 --> 01:12:27,760
We have many methods that can allow us to do that.

1200
01:12:27,760 --> 01:12:29,720
Can we check what the network focuses on

1201
01:12:29,720 --> 01:12:30,800
given the input image?

1202
01:12:30,800 --> 01:12:32,079
We have methods to do that.

1203
01:12:32,079 --> 01:12:35,560
And how does the neural network see our world?

1204
01:12:35,560 --> 01:12:37,920
We have the gradient ascent class model

1205
01:12:37,920 --> 01:12:39,920
visualization method that allow us

1206
01:12:39,920 --> 01:12:43,760
to maximize an input image.

1207
01:12:44,760 --> 01:12:48,119
Find the input image that maximizes a certain activation.

1208
01:12:48,119 --> 01:12:48,960
Super.

1209
01:12:48,960 --> 01:12:51,560
So that was the first part.

1210
01:12:51,560 --> 01:12:55,039
And then we're gonna move toward frontier ideas.

1211
01:12:57,159 --> 01:12:59,720
Any questions on CNNs?

1212
01:12:59,720 --> 01:13:01,119
Do you feel like you have a better idea

1213
01:13:01,119 --> 01:13:08,810
of how to look inside a CNN?

1214
01:13:08,810 --> 01:13:13,810
So let me start by comparing CNNs

1215
01:13:14,210 --> 01:13:19,210
to more modern frontier networks.

1216
01:13:20,329 --> 01:13:23,770
The core distinction is going to be

1217
01:13:23,770 --> 01:13:28,649
that CNNs deal with localized information.

1218
01:13:28,649 --> 01:13:31,729
They visualize edges, textures, and shapes

1219
01:13:31,729 --> 01:13:34,850
when in modern, call it LLMs,

1220
01:13:36,090 --> 01:13:39,369
we visualize relationships and meanings

1221
01:13:39,369 --> 01:13:44,579
between concepts or between tokens.

1222
01:13:44,579 --> 01:13:49,220
And this is because transformers are based on attention.

1223
01:13:50,220 --> 01:13:54,180
And that started with the attention is all you need paper,

1224
01:13:54,180 --> 01:13:57,699
which essentially explained why attention

1225
01:13:57,699 --> 01:14:00,619
on its own is highly performant

1226
01:14:00,619 --> 01:14:04,140
and can allow us to model very complex relationships.

1227
01:14:05,619 --> 01:14:07,380
So, you know, by the way,

1228
01:14:07,380 --> 01:14:10,420
this is just the first figure

1229
01:14:10,420 --> 01:14:12,220
of the attention is all you need paper,

1230
01:14:12,220 --> 01:14:14,140
which you should all be able to read

1231
01:14:14,140 --> 01:14:16,420
and understand by now in the class.

1232
01:14:17,180 --> 01:14:21,899
And transformers really represent language

1233
01:14:21,899 --> 01:14:25,659
using two very simple ideas that are visualizable.

1234
01:14:25,659 --> 01:14:29,619
We can interpret them to a certain extent.

1235
01:14:29,619 --> 01:14:33,939
The first one is the attention patterns.

1236
01:14:33,939 --> 01:14:35,939
You know, attention looks

1237
01:14:35,939 --> 01:14:37,739
at the relationship between tokens.

1238
01:14:37,739 --> 01:14:39,659
So you look at a specific token,

1239
01:14:39,659 --> 01:14:43,380
which can be a word, a subword, or a syllable.

1240
01:14:43,380 --> 01:14:46,260
You know, I'm gonna simplify by saying it's a word.

1241
01:14:46,260 --> 01:14:50,300
And its relationship with other words in the training set.

1242
01:14:50,300 --> 01:14:53,619
And that's the attention that the transformer looking at it.

1243
01:14:55,020 --> 01:14:58,539
Each attention head learns different patterns.

1244
01:14:58,539 --> 01:15:01,340
So it can learn things like linking pronouns to nouns

1245
01:15:01,340 --> 01:15:04,939
or tracking structures or enforcing a certain ordering.

1246
01:15:04,939 --> 01:15:09,020
And then I really like this visualization,

1247
01:15:09,020 --> 01:15:13,619
which is from Jesse Vig in 2019.

1248
01:15:14,020 --> 01:15:16,779
And this visualization essentially shows you

1249
01:15:16,779 --> 01:15:19,659
there is a very nice blog post that he wrote

1250
01:15:19,659 --> 01:15:22,380
with a few figures where you can see

1251
01:15:22,380 --> 01:15:26,340
he presents how attention can be visualized

1252
01:15:26,340 --> 01:15:27,420
in simple ways.

1253
01:15:27,420 --> 01:15:30,340
What is the connection between a fixed token

1254
01:15:30,340 --> 01:15:32,859
with the surrounding tokens, let's say.

1255
01:15:35,340 --> 01:15:39,899
So this is essentially the transformer analog

1256
01:15:39,899 --> 01:15:44,899
to the CNN saliency maps that we look at, pretty much.

1257
01:15:48,380 --> 01:15:51,180
The other things that transformers

1258
01:15:51,180 --> 01:15:54,619
or more modern language model uses embeddings.

1259
01:15:54,619 --> 01:15:56,220
You know, during the pre-training phase,

1260
01:15:56,220 --> 01:15:58,140
you're also learning embeddings.

1261
01:15:59,619 --> 01:16:02,260
You are ready to read the birth paper, in fact,

1262
01:16:02,260 --> 01:16:04,460
now with the baggage you have from the class.

1263
01:16:05,699 --> 01:16:08,739
And what's interesting about embeddings,

1264
01:16:08,739 --> 01:16:13,739
and I printed a picture here from Garg in 2021.

1265
01:16:13,859 --> 01:16:18,060
I also encourage you to see that short blog post

1266
01:16:18,060 --> 01:16:20,779
where he uses a visualization method called TSNI.

1267
01:16:20,779 --> 01:16:22,380
It's a dimensionally reduction method.

1268
01:16:22,380 --> 01:16:24,180
We're not gonna present it in the class,

1269
01:16:24,180 --> 01:16:27,819
but it's taught actually a lot in biotech

1270
01:16:27,819 --> 01:16:30,460
and healthcare, it's used very extensively

1271
01:16:30,460 --> 01:16:33,220
for those of you who work with Stanford Hospital.

1272
01:16:34,539 --> 01:16:36,859
And it allows you to visualize embeddings.

1273
01:16:36,859 --> 01:16:38,539
And embeddings are sort of how

1274
01:16:38,539 --> 01:16:40,819
the language model perceives our words.

1275
01:16:40,819 --> 01:16:44,300
So you would expect tokens that should have

1276
01:16:44,300 --> 01:16:46,779
similar semantic meanings to be next to each other

1277
01:16:46,779 --> 01:16:49,539
in that space, or tokens that have nothing to do

1278
01:16:49,539 --> 01:16:51,020
with each other to be far away

1279
01:16:51,020 --> 01:16:52,619
from each other in distance.

1280
01:16:52,619 --> 01:16:55,539
And that can be a way to sanity check

1281
01:16:55,539 --> 01:16:57,380
that your model actually is learning

1282
01:16:57,380 --> 01:16:59,739
meaningful representations.

1283
01:17:01,420 --> 01:17:04,619
So together, attention and embeddings

1284
01:17:04,619 --> 01:17:06,539
are what let large language models

1285
01:17:06,539 --> 01:17:08,779
track relationship and meanings.

1286
01:17:08,779 --> 01:17:12,979
And you can visualize your embeddings

1287
01:17:12,979 --> 01:17:14,539
with dimensionally reduction tool.

1288
01:17:14,539 --> 01:17:19,420
You can visualize attention relationships as well.

1289
01:17:19,420 --> 01:17:22,579
Unfortunately, the modern transformers

1290
01:17:22,579 --> 01:17:27,579
are so complicated that even the cutting edge research

1291
01:17:27,699 --> 01:17:32,699
is only able to interpret those relationships

1292
01:17:32,699 --> 01:17:37,180
with two-layer transformers, pretty much.

1293
01:17:37,180 --> 01:17:40,060
The best you find out there is probably Anthropic's work,

1294
01:17:40,060 --> 01:17:41,939
so I linked two papers.

1295
01:17:41,939 --> 01:17:42,979
The first one is called

1296
01:17:42,979 --> 01:17:45,619
The Mathematical Framework for Transformer Circuits,

1297
01:17:45,619 --> 01:17:47,819
which is essentially explaining

1298
01:17:47,819 --> 01:17:50,180
how the different components within a transformer

1299
01:17:50,180 --> 01:17:51,819
interact with each other.

1300
01:17:51,819 --> 01:17:53,899
And they introduce the concept of a circuit.

1301
01:17:53,899 --> 01:17:57,539
And then the second one is a follow-up to that paper

1302
01:17:57,539 --> 01:18:00,579
called In-Context Learning with Induction Heads.

1303
01:18:00,579 --> 01:18:02,500
Induction Heads are probably the best tool

1304
01:18:03,060 --> 01:18:04,100
we have to sort of visualize

1305
01:18:04,100 --> 01:18:06,220
what's happening inside a transformer.

1306
01:18:06,220 --> 01:18:07,380
It's pretty complex.

1307
01:18:07,380 --> 01:18:09,859
You should be able to understand it by now,

1308
01:18:09,859 --> 01:18:12,619
but you'd have to spend quite some time

1309
01:18:12,619 --> 01:18:13,739
to go deeper into it.

1310
01:18:13,739 --> 01:18:14,939
I just will link them.

1311
01:18:14,939 --> 01:18:21,539
We're not gonna talk about it for the sake of time.

1312
01:18:21,539 --> 01:18:24,539
Let's get to some fun stuff.

1313
01:18:24,539 --> 01:18:27,859
Training and scaling diagnostics.

1314
01:18:27,859 --> 01:18:31,300
So how do labs, frontier labs,

1315
01:18:31,300 --> 01:18:33,659
check if a model is training well?

1316
01:18:33,659 --> 01:18:35,899
We've talked about it in the first case study,

1317
01:18:35,899 --> 01:18:39,779
but one very natural way is to look at our loss curves.

1318
01:18:39,779 --> 01:18:41,819
You can look at the training loss,

1319
01:18:41,819 --> 01:18:42,979
at the validation loss,

1320
01:18:42,979 --> 01:18:47,260
and make sure that they follow sort of a smooth trajectory.

1321
01:18:47,260 --> 01:18:48,340
And if it's not smooth,

1322
01:18:48,340 --> 01:18:50,539
there's probably something that went wrong.

1323
01:18:50,539 --> 01:18:52,260
You've probably trained your own network

1324
01:18:52,260 --> 01:18:54,659
where some loss functions look very funky.

1325
01:18:54,659 --> 01:18:56,739
I remember back in the days there was even blogs

1326
01:18:56,739 --> 01:19:00,619
where people would post their ugliest loss functions.

1327
01:19:00,619 --> 01:19:02,420
And there was a lot on there.

1328
01:19:03,819 --> 01:19:06,100
You might find sudden jumps on the loss.

1329
01:19:06,100 --> 01:19:09,539
That means maybe the batch that has been processed

1330
01:19:09,539 --> 01:19:11,180
has been corrupted.

1331
01:19:11,180 --> 01:19:12,899
Maybe you're doing extremely well on it

1332
01:19:12,899 --> 01:19:16,659
when you should actually not do that well,

1333
01:19:16,659 --> 01:19:18,140
and it might raise a flag.

1334
01:19:19,300 --> 01:19:21,979
You might find bugs in your code because of that.

1335
01:19:21,979 --> 01:19:24,340
You might find gradients that are exploding,

1336
01:19:24,340 --> 01:19:26,539
gradients that are vanishing.

1337
01:19:26,539 --> 01:19:28,739
All of that you could visualize

1338
01:19:28,739 --> 01:19:30,779
at the loss function level.

1339
01:19:30,779 --> 01:19:34,899
Now note that loss functions can be run at a global level

1340
01:19:34,899 --> 01:19:38,260
or on a specific subset of your data.

1341
01:19:38,260 --> 01:19:40,619
We're gonna talk about it in data diagnostics.

1342
01:19:41,699 --> 01:19:44,140
The other things that are interesting to track

1343
01:19:44,140 --> 01:19:47,500
also sometimes referred to in the community

1344
01:19:47,500 --> 01:19:51,739
as training telemetry is to watch

1345
01:19:51,739 --> 01:19:53,779
and track your gradient norms,

1346
01:19:54,699 --> 01:19:56,539
look at your learning rate schedule,

1347
01:19:57,460 --> 01:20:01,020
or even look at hardware efficiency metrics

1348
01:20:01,020 --> 01:20:03,460
to feel if you've underutilized compute,

1349
01:20:03,460 --> 01:20:06,779
which we talked about again in the first part.

1350
01:20:06,779 --> 01:20:10,420
So imagine that if you're working at a major frontier lab

1351
01:20:10,420 --> 01:20:12,619
you probably have a dashboard

1352
01:20:12,619 --> 01:20:14,579
that tracks your different loss function

1353
01:20:14,579 --> 01:20:16,500
for different subsets of the data,

1354
01:20:16,500 --> 01:20:19,619
your checkpoints, all of that.

1355
01:20:19,619 --> 01:20:20,819
You would have all of that.

1356
01:20:20,819 --> 01:20:23,380
Unfortunately, very few of these are published

1357
01:20:23,380 --> 01:20:25,619
because they're IP.

1358
01:20:25,619 --> 01:20:28,579
They can't really, they don't want to give it out

1359
01:20:28,579 --> 01:20:31,420
because it will leak essentially information

1360
01:20:31,420 --> 01:20:33,340
about their architecture, about what's going well,

1361
01:20:33,340 --> 01:20:34,659
what's not going well, et cetera.

1362
01:20:34,659 --> 01:20:39,060
And that's why you find very few information on this.

1363
01:20:39,060 --> 01:20:42,300
The one thing that you do find some charts on

1364
01:20:42,300 --> 01:20:44,699
that are really helpful is scaling laws.

1365
01:20:44,699 --> 01:20:47,819
So scaling laws, which we've talked about

1366
01:20:47,819 --> 01:20:49,100
in a previous lecture,

1367
01:20:50,100 --> 01:20:52,460
is essentially trying to understand

1368
01:20:52,460 --> 01:20:55,819
the relationship between our model performance

1369
01:20:55,819 --> 01:21:00,619
and some other, call it hyperparameters,

1370
01:21:00,619 --> 01:21:04,300
such as the model capacity, so the size of the model,

1371
01:21:04,300 --> 01:21:06,380
the amount of compute that being used,

1372
01:21:06,380 --> 01:21:08,739
or the data set size.

1373
01:21:08,739 --> 01:21:11,739
DeepMind has done amazing work,

1374
01:21:11,739 --> 01:21:15,659
I think it was in 2022, a couple of years ago,

1375
01:21:15,659 --> 01:21:16,819
with Chinchilla.

1376
01:21:17,819 --> 01:21:22,220
This chart is borrowed from the Chinchilla paper

1377
01:21:22,939 --> 01:21:25,300
where essentially what I want you to look at here

1378
01:21:25,300 --> 01:21:27,819
is they're comparing the Chinchilla model,

1379
01:21:27,819 --> 01:21:30,180
the green star, to other models,

1380
01:21:30,180 --> 01:21:35,180
including GPT-3, which came up a little before.

1381
01:21:35,180 --> 01:21:36,899
And what they're showing is that

1382
01:21:38,380 --> 01:21:41,539
the scaling law is actually slightly different

1383
01:21:41,539 --> 01:21:43,939
than what OpenAI thought.

1384
01:21:43,939 --> 01:21:48,140
And they analyzed GPT-3 and they said GPT-3

1385
01:21:48,140 --> 01:21:50,619
is actually not performing well enough

1386
01:21:50,619 --> 01:21:52,460
for the size that it is at.

1387
01:21:52,460 --> 01:21:56,859
And it was found that GPT-3 was not trained for long enough.

1388
01:21:56,859 --> 01:21:59,140
They essentially explained that if you kept training

1389
01:21:59,140 --> 01:22:00,579
GPT-3 for longer,

1390
01:22:01,539 --> 01:22:03,939
you would have had way better performance.

1391
01:22:03,939 --> 01:22:05,619
And it was not about the model size.

1392
01:22:05,619 --> 01:22:07,899
In fact, the model was underutilized.

1393
01:22:08,819 --> 01:22:10,539
So they plotted these lines.

1394
01:22:10,539 --> 01:22:13,859
So the dotted line is what probably we thought

1395
01:22:13,859 --> 01:22:18,100
in 2020, 21, the scaling law, the power law would be.

1396
01:22:18,180 --> 01:22:20,859
And they showed that it's actually not exactly that.

1397
01:22:20,859 --> 01:22:25,859
Where essentially the idea is they plot the full line here

1398
01:22:26,779 --> 01:22:30,140
and they say this is our analysis of the scaling law.

1399
01:22:30,140 --> 01:22:33,619
If your star is above that line,

1400
01:22:33,619 --> 01:22:37,739
it's probably that your model should be trained longer.

1401
01:22:39,060 --> 01:22:40,340
If it's on the line,

1402
01:22:40,340 --> 01:22:43,340
it's respecting the scaling laws that they're finding.

1403
01:22:43,340 --> 01:22:50,489
And that's what's interesting about this Chinchilla paper.

1404
01:22:50,569 --> 01:22:53,329
So that's after the GPT-3 paper.

1405
01:22:53,329 --> 01:22:55,569
Chinchilla came in 2022 saying

1406
01:22:55,569 --> 01:22:57,130
you should have trained GPT-3 longer,

1407
01:22:57,130 --> 01:22:58,289
you would have done better.

1408
01:22:58,289 --> 01:23:01,130
And here's Chinchilla model

1409
01:23:01,130 --> 01:23:04,890
that has less parameters than GPT-3.

1410
01:23:04,890 --> 01:23:08,250
So 70 billion versus 175 billion.

1411
01:23:08,250 --> 01:23:13,300
And that is performing better.

1412
01:23:13,300 --> 01:23:14,140
Yes.

1413
01:23:15,699 --> 01:23:18,819
To go deeper, I also put a few charts

1414
01:23:19,420 --> 01:23:23,699
from that show you the power laws

1415
01:23:23,699 --> 01:23:26,659
between the loss function.

1416
01:23:26,659 --> 01:23:30,300
So on the vertical axis, you have the test loss, okay?

1417
01:23:30,300 --> 01:23:33,899
That shows essentially your performance on the test set.

1418
01:23:33,899 --> 01:23:36,500
And then on the horizontal axis, on the x-axis,

1419
01:23:36,500 --> 01:23:39,500
you have compute, data set size, and parameters.

1420
01:23:39,500 --> 01:23:41,300
So how are those scaling laws established?

1421
01:23:41,300 --> 01:23:42,979
Essentially, they fix two of them

1422
01:23:42,979 --> 01:23:44,220
and they vary the third one

1423
01:23:44,220 --> 01:23:46,460
and they see if things are respected.

1424
01:23:46,460 --> 01:23:50,460
Meaning, let's say you're increasing the,

1425
01:23:50,460 --> 01:23:52,380
you're keeping the same compute,

1426
01:23:52,380 --> 01:23:55,619
the same data set size,

1427
01:23:55,619 --> 01:23:57,460
but you're training a model

1428
01:23:57,460 --> 01:24:02,460
that is twice as big in logarithms.

1429
01:24:02,659 --> 01:24:05,180
What does it tell you about the performance,

1430
01:24:05,180 --> 01:24:07,500
essentially, are those scaling laws respected?

1431
01:24:07,500 --> 01:24:08,460
And so what's nice now

1432
01:24:08,460 --> 01:24:10,859
is that we have a precedent for scaling laws.

1433
01:24:10,859 --> 01:24:14,859
So if you were actually training such big models,

1434
01:24:14,859 --> 01:24:16,899
you would compare to the scaling laws

1435
01:24:16,899 --> 01:24:18,699
that are available out there.

1436
01:24:19,939 --> 01:24:21,699
Remember, another reason these are important

1437
01:24:21,699 --> 01:24:25,579
is because models training runs are so expensive.

1438
01:24:25,579 --> 01:24:26,739
It wasn't shared publicly,

1439
01:24:26,739 --> 01:24:29,460
but we estimate that GPT-5

1440
01:24:29,460 --> 01:24:32,380
is probably in the hundreds of millions.

1441
01:24:33,420 --> 01:24:34,979
And so you wanna know,

1442
01:24:34,979 --> 01:24:37,739
should I train that model twice longer or not?

1443
01:24:37,739 --> 01:24:40,140
Because that's a big financial decision, right?

1444
01:24:40,140 --> 01:24:42,819
And the scaling laws allow you to determine,

1445
01:24:42,819 --> 01:24:44,100
should I invest in compute?

1446
01:24:44,100 --> 01:24:46,939
Should I invest in growing my data set,

1447
01:24:46,939 --> 01:24:48,140
so finding more data?

1448
01:24:48,140 --> 01:24:50,140
Or should I invest in model capacity,

1449
01:24:50,140 --> 01:24:56,720
making my model bigger?

1450
01:24:56,720 --> 01:25:01,960
Okay?

1451
01:25:01,960 --> 01:25:02,800
So yeah, together,

1452
01:25:02,800 --> 01:25:09,359
these form sort of a health dashboard for the model.

1453
01:25:09,359 --> 01:25:11,640
So that's training and scaling diagnostic,

1454
01:25:11,640 --> 01:25:14,760
loss functions, et cetera.

1455
01:25:14,760 --> 01:25:17,119
Health dashboard, scaling laws,

1456
01:25:17,119 --> 01:25:19,760
all of that are things that researchers might use

1457
01:25:19,760 --> 01:25:23,199
to get a broad sense of where to invest more

1458
01:25:23,199 --> 01:25:25,760
in terms of improving their model.

1459
01:25:25,800 --> 01:25:28,359
The other one is something we've already seen

1460
01:25:28,359 --> 01:25:30,840
to a certain extent is how models,

1461
01:25:30,840 --> 01:25:34,439
how labs evaluate model capabilities and safety,

1462
01:25:34,439 --> 01:25:36,319
and they might do it with benchmarks.

1463
01:25:36,319 --> 01:25:40,199
So capability benchmark might be evaluating the model

1464
01:25:40,199 --> 01:25:43,840
in tasks like reasoning or coding or math

1465
01:25:43,840 --> 01:25:46,640
or multi-lingual tasks, et cetera.

1466
01:25:46,640 --> 01:25:49,119
It might also be comparing checkpoints

1467
01:25:49,119 --> 01:25:52,760
that help you understand how your model

1468
01:25:52,760 --> 01:25:54,520
is improving over time,

1469
01:25:54,520 --> 01:25:56,359
depending on what you're feeding it

1470
01:25:56,359 --> 01:25:59,760
or depending on some hyperparameters that you're tweaking.

1471
01:26:01,439 --> 01:26:05,039
And also error clusters.

1472
01:26:05,039 --> 01:26:08,800
So just to tell you a little more about error clusters,

1473
01:26:08,800 --> 01:26:10,239
if you actually use benchmarks

1474
01:26:10,239 --> 01:26:12,119
across a wide variety of tasks,

1475
01:26:12,119 --> 01:26:15,720
you might see that all the model checkpoint number five

1476
01:26:15,720 --> 01:26:17,920
is actually doing very poorly at reasoning.

1477
01:26:17,920 --> 01:26:19,680
Let's see why is that.

1478
01:26:20,680 --> 01:26:22,399
So here are some examples

1479
01:26:23,760 --> 01:26:28,119
from a competitive math benchmark 2025,

1480
01:26:28,119 --> 01:26:32,960
aim published by OpenAI on GPT-5,

1481
01:26:32,960 --> 01:26:34,560
and actually the bottom one is from today.

1482
01:26:34,560 --> 01:26:38,960
This morning, Mistral announced their third generation

1483
01:26:38,960 --> 01:26:41,199
model, so I thought I'd pull it to show you

1484
01:26:41,199 --> 01:26:42,920
that how real-time these things are,

1485
01:26:42,920 --> 01:26:44,159
and just published today.

1486
01:26:46,079 --> 01:26:47,920
And they're comparing across reasoning

1487
01:26:47,920 --> 01:26:51,239
and math and et cetera against benchmarks.

1488
01:26:51,239 --> 01:26:55,560
Now the risk is, are these benchmarks contaminated?

1489
01:26:55,560 --> 01:26:57,800
How can a benchmark be contaminated?

1490
01:27:00,920 --> 01:27:02,399
Yeah, if it was in the training data.

1491
01:27:02,399 --> 01:27:03,880
The problem is these models are trained

1492
01:27:03,880 --> 01:27:06,039
on so much data online, you don't know.

1493
01:27:06,039 --> 01:27:07,880
Maybe it was trained on a blog post

1494
01:27:07,880 --> 01:27:09,880
where someone actually was presenting a benchmark

1495
01:27:09,880 --> 01:27:11,880
and describing what the benchmark was about.

1496
01:27:11,880 --> 01:27:13,319
Maybe it was trained on GitHub

1497
01:27:13,319 --> 01:27:16,079
and there was a text file in a very shady part

1498
01:27:16,079 --> 01:27:17,680
of the GitHub that was listing some

1499
01:27:17,680 --> 01:27:20,479
of the test set information.

1500
01:27:20,479 --> 01:27:22,560
All of those might contaminate benchmarks.

1501
01:27:22,560 --> 01:27:26,159
How would you identify that a benchmark

1502
01:27:26,159 --> 01:27:31,760
has been contaminated?

1503
01:27:31,760 --> 01:27:50,659
Test set has been contaminated.

1504
01:27:50,659 --> 01:27:53,060
Yeah, Lama 4 you brought up just to repeat.

1505
01:27:54,260 --> 01:27:55,500
Looked good on benchmark,

1506
01:27:55,500 --> 01:27:59,100
looked poor in practice after the community tested it.

1507
01:27:59,100 --> 01:28:01,859
The general consensus, I mean my opinion is

1508
01:28:01,859 --> 01:28:04,779
I actually don't look too much at the benchmarks

1509
01:28:04,779 --> 01:28:07,500
when a foundation model provider publishes them,

1510
01:28:07,500 --> 01:28:09,739
or in other words I would look at them

1511
01:28:09,739 --> 01:28:12,979
in relative value between models rather than absolute value.

1512
01:28:13,979 --> 01:28:16,100
And you'll wait for the community to test it

1513
01:28:16,100 --> 01:28:18,739
on agentic workflows, on their tasks

1514
01:28:18,739 --> 01:28:20,420
and then people will sort of get a taste

1515
01:28:20,420 --> 01:28:23,300
for if it works or not and on what it works.

1516
01:28:23,300 --> 01:28:24,699
So it took some time, for example,

1517
01:28:24,699 --> 01:28:25,819
for the community to realize

1518
01:28:25,819 --> 01:28:28,020
how good Claude was at coding.

1519
01:28:29,020 --> 01:28:31,300
It was clear from the benchmark

1520
01:28:31,300 --> 01:28:33,180
but others were also clearly good.

1521
01:28:33,180 --> 01:28:34,819
But over time people felt like,

1522
01:28:34,819 --> 01:28:37,460
oh wow, it's actually really good at coding.

1523
01:28:38,420 --> 01:28:39,260
You had a question or no?

1524
01:28:39,260 --> 01:28:41,500
Okay, cool.

1525
01:28:41,500 --> 01:28:43,380
So, oh yeah, contamination.

1526
01:28:43,380 --> 01:28:48,380
So how to detect if a test set has been exposed.

1527
01:28:50,300 --> 01:28:51,659
Few methods.

1528
01:28:53,739 --> 01:28:58,739
Some researchers might do a search within the data set.

1529
01:28:59,260 --> 01:29:01,100
So let's say you have a training set

1530
01:29:01,100 --> 01:29:03,260
and you have a held out test set

1531
01:29:03,260 --> 01:29:05,220
and you actually look for n-grams.

1532
01:29:05,220 --> 01:29:09,060
So you take sequences of tokens size seven, eight

1533
01:29:09,060 --> 01:29:10,819
and you search through the train set

1534
01:29:10,819 --> 01:29:12,979
and you find that their same n-grams

1535
01:29:12,979 --> 01:29:15,140
is also found in the test set.

1536
01:29:15,140 --> 01:29:17,500
There's a chance some of it has been contaminated.

1537
01:29:17,500 --> 01:29:21,779
You can do it also with hashes or with embeddings.

1538
01:29:21,779 --> 01:29:24,460
Actually, maybe the test set has been contaminated

1539
01:29:24,460 --> 01:29:26,300
but not word for word.

1540
01:29:26,300 --> 01:29:29,060
Maybe semantically and so you might do the same thing

1541
01:29:29,060 --> 01:29:30,460
with an embedding and run a search

1542
01:29:30,460 --> 01:29:34,260
and say that, oh wow, this specific example

1543
01:29:34,300 --> 01:29:37,979
from the test set is found in the training set

1544
01:29:37,979 --> 01:29:40,579
semantically, very similar stuff.

1545
01:29:40,579 --> 01:29:43,180
So you might actually run analysis

1546
01:29:43,180 --> 01:29:45,939
to figure out if it's contaminated

1547
01:29:45,939 --> 01:29:48,899
and if you find that your test set has been exposed

1548
01:29:48,899 --> 01:29:51,939
or you have a reason to think it's been exposed,

1549
01:29:51,939 --> 01:29:55,420
what you do, you would usually fix the test set.

1550
01:29:55,420 --> 01:29:56,739
The test set is smaller,

1551
01:29:56,739 --> 01:29:58,180
you would remove all those examples

1552
01:29:58,180 --> 01:30:00,140
that you think are exposed

1553
01:30:00,140 --> 01:30:02,739
and you would replace them with brand new ones

1554
01:30:02,739 --> 01:30:04,819
that are completely held offline

1555
01:30:04,819 --> 01:30:08,899
in a folder that is separate, not available online, et cetera.

1556
01:30:12,779 --> 01:30:15,779
Then there's the problem of safety evaluations.

1557
01:30:15,779 --> 01:30:17,180
Not gonna spend too much time on it

1558
01:30:17,180 --> 01:30:22,180
but safety is important to foundation model providers.

1559
01:30:23,939 --> 01:30:25,500
They stress test their model

1560
01:30:25,500 --> 01:30:29,100
and their many adversarial attacks, jailbreaking,

1561
01:30:29,100 --> 01:30:32,340
social engineering, misuses.

1562
01:30:32,340 --> 01:30:35,899
They also assess harmful content generation,

1563
01:30:35,899 --> 01:30:38,699
hallucination, privacy leakage, et cetera, et cetera.

1564
01:30:40,100 --> 01:30:44,060
And then they also look at how the foundation model

1565
01:30:44,060 --> 01:30:47,779
behaves in an agentic workflow, as I was saying earlier.

1566
01:30:47,779 --> 01:30:52,779
So not only evaluating it sort of one shot

1567
01:30:52,939 --> 01:30:57,779
but evaluating it in a workflow.

1568
01:30:57,779 --> 01:31:01,899
Here are some examples of actually very nice

1569
01:31:01,899 --> 01:31:05,859
joint collaboration between OpenAI and Anthropic

1570
01:31:05,859 --> 01:31:09,060
from last year where they worked together

1571
01:31:09,060 --> 01:31:11,420
to assess the safety of their models

1572
01:31:11,420 --> 01:31:14,180
and they tried to jailbreak the model

1573
01:31:14,180 --> 01:31:17,260
to social engineer and they published some

1574
01:31:19,140 --> 01:31:23,579
findings on password protection or phrase protection.

1575
01:31:23,579 --> 01:31:24,939
I linked it and I would encourage you

1576
01:31:24,939 --> 01:31:26,220
to quickly look at it.

1577
01:31:26,220 --> 01:31:28,939
They wrote prompts to try to extract

1578
01:31:28,939 --> 01:31:32,739
a password from a model and see if the model was good

1579
01:31:32,739 --> 01:31:35,979
at not leaking the password.

1580
01:31:40,340 --> 01:31:43,300
So these dashboards essentially inform

1581
01:31:43,300 --> 01:31:48,300
the go, no go decision for releasing

1582
01:31:48,460 --> 01:31:52,739
or for determining what the RLHF will be based on.

1583
01:31:52,739 --> 01:31:55,699
So if you're actually going to do supervised fine tuning

1584
01:31:55,699 --> 01:32:00,100
or reinforcement learning from human feedback,

1585
01:32:00,100 --> 01:32:02,460
it's expensive and you wanna do it

1586
01:32:02,460 --> 01:32:04,659
on the stuff that's failing.

1587
01:32:04,659 --> 01:32:07,859
So if you identify exactly which evals are failing,

1588
01:32:07,859 --> 01:32:09,899
you will then use that information

1589
01:32:09,899 --> 01:32:13,420
to focus the RLHF on that specific problem

1590
01:32:13,420 --> 01:32:15,760
and save a lot of money and human time.

1591
01:32:20,300 --> 01:32:22,460
Lastly, let me see if there is anything else

1592
01:32:22,460 --> 01:32:24,220
I wanted to mention here.

1593
01:32:26,020 --> 01:32:34,010
Yeah, yeah.

1594
01:32:35,810 --> 01:32:37,850
Let's talk about the data diagnostic

1595
01:32:37,850 --> 01:32:39,210
and we'll end on that.

1596
01:32:40,449 --> 01:32:43,449
So data diagnostics.

1597
01:32:43,449 --> 01:32:45,050
This is probably the last area

1598
01:32:45,050 --> 01:32:47,770
that frontier models are very focused on.

1599
01:32:47,770 --> 01:32:49,890
So how labs detect data issues.

1600
01:32:52,489 --> 01:32:55,289
There are many things you can do

1601
01:32:55,289 --> 01:32:56,970
but I really like distribution checks.

1602
01:32:56,970 --> 01:33:00,949
So I pulled this chart from a paper called Pile,

1603
01:33:00,949 --> 01:33:03,250
the Pile from 2020,

1604
01:33:03,250 --> 01:33:05,729
where the Pile is a very large data set,

1605
01:33:05,729 --> 01:33:10,250
800 gigabytes that is made from diverse texts

1606
01:33:10,250 --> 01:33:11,890
and they kept the data domain.

1607
01:33:11,890 --> 01:33:14,369
So they explain using that figure

1608
01:33:14,369 --> 01:33:16,310
what the data set is made of.

1609
01:33:16,310 --> 01:33:20,750
So the data set might be made of information from free law

1610
01:33:20,750 --> 01:33:22,569
or it might be made of Wikipedia,

1611
01:33:22,569 --> 01:33:25,050
stack exchange, GitHub, with coding.

1612
01:33:25,050 --> 01:33:28,210
And so you have different domains within that data set.

1613
01:33:28,210 --> 01:33:30,510
And in fact, when you train a model on that,

1614
01:33:30,510 --> 01:33:33,189
you can plot the loss function across the entire data set

1615
01:33:33,189 --> 01:33:35,050
or you can plot the loss function

1616
01:33:35,050 --> 01:33:37,850
across different domains within that data set,

1617
01:33:37,850 --> 01:33:39,489
which gives you more intuition

1618
01:33:39,489 --> 01:33:43,689
for where it might be failing or working.

1619
01:33:43,689 --> 01:33:46,430
And so you might wanna track domain proportions

1620
01:33:46,430 --> 01:33:48,250
in your data set.

1621
01:33:48,250 --> 01:33:50,609
And domain proportions also matter

1622
01:33:50,609 --> 01:33:55,609
because it is observed that if certain domains

1623
01:33:57,329 --> 01:34:00,109
are underrepresented in the data,

1624
01:34:00,109 --> 01:34:02,970
the performance of the model on that domain

1625
01:34:02,970 --> 01:34:06,390
is likely going to drop in comparison to another domain.

1626
01:34:07,630 --> 01:34:09,130
So all things are not equal.

1627
01:34:10,130 --> 01:34:12,090
If you actually have so much,

1628
01:34:12,090 --> 01:34:14,210
we remember with the speech recognition example

1629
01:34:14,210 --> 01:34:16,850
where I said you have too many zeros and too few ones

1630
01:34:16,850 --> 01:34:19,250
and so the model just doesn't learn the ones.

1631
01:34:25,859 --> 01:34:28,199
This is also a problem with online learning.

1632
01:34:28,199 --> 01:34:30,500
So imagine those frontier model,

1633
01:34:30,500 --> 01:34:32,020
they're learning live, right?

1634
01:34:32,020 --> 01:34:35,340
Like oftentimes they're just being fed data constantly

1635
01:34:35,340 --> 01:34:38,819
and maybe the batch from the last month

1636
01:34:38,819 --> 01:34:40,979
had very little coding data.

1637
01:34:40,979 --> 01:34:44,180
And so the last portion of the training,

1638
01:34:44,180 --> 01:34:46,819
the distribution of the coding domain

1639
01:34:46,819 --> 01:34:49,539
or the frequency was lower than other domains

1640
01:34:49,539 --> 01:34:52,779
and so sometimes you might see a drop in performance

1641
01:34:52,779 --> 01:34:55,340
for a specific domain if you're not careful.

1642
01:34:56,180 --> 01:34:59,180
That can be fixed with sampling, like smart sampling.

1643
01:34:59,180 --> 01:35:01,220
You remember what we saw in reinforcement learning

1644
01:35:01,220 --> 01:35:02,979
with the experience replay

1645
01:35:02,979 --> 01:35:04,800
where we actually kept experiences

1646
01:35:04,800 --> 01:35:06,739
and we put them in a replay memory

1647
01:35:06,739 --> 01:35:08,340
and then we sampled from that.

1648
01:35:08,340 --> 01:35:11,380
Those are the types of method, sampling methods

1649
01:35:11,380 --> 01:35:14,060
that allow a model provider to make sure

1650
01:35:14,060 --> 01:35:17,439
they keep the frequency of data domain the same

1651
01:35:17,439 --> 01:35:21,359
at different portions of the training.

1652
01:35:23,159 --> 01:35:26,279
Token statistics, just to mention it a little bit.

1653
01:35:29,079 --> 01:35:32,479
You wanna count the frequency changes

1654
01:35:32,479 --> 01:35:35,319
for key tokens which I was mentioning.

1655
01:35:35,319 --> 01:35:40,319
So let's say a math token is under-represented,

1656
01:35:41,720 --> 01:35:43,399
that will be a problem.

1657
01:35:43,399 --> 01:35:46,199
The derivative symbol is under-represented.

1658
01:35:46,199 --> 01:35:49,659
That might actually lead to way worse performance

1659
01:35:49,659 --> 01:35:52,159
for that specific task where you ask the model

1660
01:35:52,159 --> 01:35:53,460
to make derivatives.

1661
01:35:56,859 --> 01:36:00,899
And so labs oftentimes monitor token drift

1662
01:36:00,899 --> 01:36:03,359
or distribution or the frequency per token

1663
01:36:03,359 --> 01:36:05,279
and they use sampling to fix it.

1664
01:36:05,279 --> 01:36:07,039
And finally, the contamination checks

1665
01:36:07,039 --> 01:36:09,760
which we have talked about earlier.

1666
01:36:09,760 --> 01:36:11,600
I also give you concrete examples.

1667
01:36:11,600 --> 01:36:13,479
Not gonna go through all of them

1668
01:36:13,479 --> 01:36:18,479
but these are examples of token distribution drift report,

1669
01:36:18,739 --> 01:36:23,739
tokenizer statistics, issues that I raise here,

1670
01:36:25,539 --> 01:36:30,539
or some anomalies on corrupt data detection.

1671
01:36:31,260 --> 01:36:33,579
So if I read one of the examples for you,

1672
01:36:35,159 --> 01:36:38,140
let's say, let's pick this one maybe.

1673
01:36:42,130 --> 01:36:45,689
Non-English tokens increase from 12% to 19%

1674
01:36:45,689 --> 01:36:48,489
after a new web crawl where that might mean

1675
01:36:48,489 --> 01:36:51,869
that the domain of that specific language

1676
01:36:51,869 --> 01:36:53,609
is increasing relative to others

1677
01:36:53,609 --> 01:36:57,970
and that might lead to an increase in performance

1678
01:36:57,970 --> 01:37:00,869
or a drop in performance for a different language.

1679
01:37:00,869 --> 01:37:06,510
As simple as that.

1680
01:37:06,510 --> 01:37:08,069
Okay.

1681
01:37:08,069 --> 01:37:09,710
To summarize everything,

1682
01:37:11,470 --> 01:37:14,710
what are examples of things that frontier labs monitor?

1683
01:37:15,569 --> 01:37:17,550
Global training loss, validation loss,

1684
01:37:17,550 --> 01:37:19,510
both global and domain specific

1685
01:37:19,510 --> 01:37:21,869
on the subset of the data.

1686
01:37:21,869 --> 01:37:25,149
Scaling curve alignment, comparing your test loss

1687
01:37:25,149 --> 01:37:28,029
to your compute, to your data set

1688
01:37:28,029 --> 01:37:29,770
or to your model capacity.

1689
01:37:30,770 --> 01:37:32,829
Oh, we didn't talk too much about router

1690
01:37:32,829 --> 01:37:34,829
but mixture of experts.

1691
01:37:34,829 --> 01:37:37,489
I imagine you have a lot of the models

1692
01:37:37,489 --> 01:37:40,350
that are top models right now are mixtures of experts,

1693
01:37:40,350 --> 01:37:43,390
meaning that in your transformer block

1694
01:37:44,310 --> 01:37:46,050
for the multi-layer perceptron,

1695
01:37:46,050 --> 01:37:47,630
there's actually multiple experts

1696
01:37:47,630 --> 01:37:49,829
that are being trained in parallel.

1697
01:37:49,829 --> 01:37:52,850
And there's a router that will route

1698
01:37:52,850 --> 01:37:55,949
that batch of data to the right experts,

1699
01:37:55,949 --> 01:37:58,710
to top one, top two, top three experts.

1700
01:37:58,710 --> 01:38:01,829
And it's very common for the router to fail

1701
01:38:01,829 --> 01:38:05,430
or to always exploit the same mixture of experts.

1702
01:38:05,430 --> 01:38:08,149
You need a mechanism to detect when this happens.

1703
01:38:08,149 --> 01:38:10,470
And so you might have in your health dashboard

1704
01:38:10,470 --> 01:38:13,069
some sort of a router information

1705
01:38:13,069 --> 01:38:15,149
or whether the mixture of experts

1706
01:38:15,149 --> 01:38:17,189
are all used as much as each other.

1707
01:38:17,189 --> 01:38:19,590
Sometimes certain experts are gonna be so specialized

1708
01:38:19,590 --> 01:38:21,510
that they're never gonna be used almost.

1709
01:38:21,510 --> 01:38:22,869
And you wanna avoid that.

1710
01:38:22,869 --> 01:38:25,470
And you might do some load balancing to avoid that.

1711
01:38:27,510 --> 01:38:31,329
Gradient norms, learning rates,

1712
01:38:31,329 --> 01:38:33,130
checkpoint to checkpoint, eval benchmark,

1713
01:38:33,130 --> 01:38:34,970
token distribution, tokenizer statistic.

1714
01:38:34,970 --> 01:38:37,489
We covered all of that at a high level.

1715
01:38:37,489 --> 01:38:40,130
And as I said earlier, Frontier Labs

1716
01:38:40,130 --> 01:38:43,689
rarely publish those dashboards because it's IP

1717
01:38:43,689 --> 01:38:46,670
and because it can leak certain deep information

1718
01:38:46,670 --> 01:38:49,609
about their IP and how their models are trained.

1719
01:38:49,609 --> 01:38:52,729
And so you usually learn about these things year after.

1720
01:38:52,729 --> 01:38:54,770
You might learn about these things

1721
01:38:54,770 --> 01:38:57,369
on a model that came out three years ago

1722
01:38:57,369 --> 01:39:00,489
or four years ago and they're okay now with sharing it.

1723
01:39:01,329 --> 01:39:02,850
So it's pretty common.

1724
01:39:02,850 --> 01:39:03,689
Okay.

1725
01:39:05,529 --> 01:39:08,409
So closing remarks, any questions first?

1726
01:39:08,409 --> 01:39:09,250
Yeah.

1727
01:39:26,920 --> 01:39:29,800
Yeah, so you're asking if Entropic

1728
01:39:29,800 --> 01:39:32,979
is training load code, do they care mostly

1729
01:39:32,979 --> 01:39:37,800
about coding data or do they also add other data?

1730
01:39:37,800 --> 01:39:39,479
Yeah, it's a tough question.

1731
01:39:39,479 --> 01:39:41,119
I don't have the exact answer.

1732
01:39:41,119 --> 01:39:43,220
It's been shown that certain domains

1733
01:39:43,220 --> 01:39:45,399
might improve the performance of other domains.

1734
01:39:45,439 --> 01:39:50,119
So I imagine that in coding, if you have math data,

1735
01:39:50,119 --> 01:39:53,199
maybe math data actually helps the performance of coding,

1736
01:39:53,199 --> 01:39:54,960
especially for functional programming.

1737
01:39:54,960 --> 01:39:58,640
Let's say coding languages that are more mathematical.

1738
01:39:58,640 --> 01:40:00,840
But I could clearly see that

1739
01:40:00,840 --> 01:40:04,439
if they were training load code on web crawl

1740
01:40:04,439 --> 01:40:06,199
and everything, it would not perform well

1741
01:40:06,199 --> 01:40:08,000
because you would have so much crap data

1742
01:40:08,000 --> 01:40:09,560
that is not relevant for what you're trying

1743
01:40:09,560 --> 01:40:10,800
to get the model to do.

1744
01:40:10,800 --> 01:40:21,750
And so I think there is a balance between those things.

1745
01:40:21,750 --> 01:40:23,229
I think you could run experiments.

1746
01:40:23,229 --> 01:40:25,710
So would you include neighboring domains?

1747
01:40:25,710 --> 01:40:29,670
If I were training load code,

1748
01:40:29,670 --> 01:40:31,390
and I had a lot of money to do that,

1749
01:40:31,390 --> 01:40:36,149
I would probably, maybe you start with the Python language

1750
01:40:36,149 --> 01:40:36,989
and you get as much,

1751
01:40:36,989 --> 01:40:38,529
and there's so much data on Python language.

1752
01:40:38,529 --> 01:40:40,710
So you probably have enough already there.

1753
01:40:40,710 --> 01:40:42,510
But then you want a model that scales

1754
01:40:42,510 --> 01:40:43,869
to other programming languages.

1755
01:40:43,869 --> 01:40:46,270
Well, probably Python is useful for C++.

1756
01:40:46,270 --> 01:40:47,989
C++ is useful for Java.

1757
01:40:47,989 --> 01:40:49,489
And then functional programming,

1758
01:40:49,489 --> 01:40:53,189
if you're going to Elixir, Scala, things like that,

1759
01:40:53,189 --> 01:40:56,350
they're helping each other probably to a certain extent.

1760
01:40:56,350 --> 01:40:59,350
But you will need to have a presentation of those.

1761
01:40:59,350 --> 01:41:03,869
I could see that, I'm pretty sure,

1762
01:41:03,869 --> 01:41:05,590
and I don't work at Anthropic, so I don't know,

1763
01:41:05,590 --> 01:41:07,069
but I'm pretty sure,

1764
01:41:07,069 --> 01:41:10,510
let's say a language like Rust increases in popularity,

1765
01:41:10,510 --> 01:41:12,510
which is the case, right?

1766
01:41:12,510 --> 01:41:14,229
And then in the data distribution,

1767
01:41:14,229 --> 01:41:18,670
you start seeing more frequency of those tokens from Rust.

1768
01:41:18,670 --> 01:41:22,850
Does that affect the performance on other languages?

1769
01:41:22,850 --> 01:41:25,170
Probably yes, that's my guess.

1770
01:41:25,170 --> 01:41:26,250
And how do you track it?

1771
01:41:26,250 --> 01:41:30,289
This is all of what we talked about.

1772
01:41:30,289 --> 01:41:39,859
Yeah, yeah.

1773
01:41:39,859 --> 01:41:44,100
Question is, let's say we trained on everything online

1774
01:41:44,100 --> 01:41:47,380
and now real data, and now what about synthetic data?

1775
01:41:47,380 --> 01:41:48,260
Should we use it a lot?

1776
01:41:48,260 --> 01:41:50,840
Should we use it strategically?

1777
01:41:50,840 --> 01:41:53,060
What's the future of that?

1778
01:41:53,060 --> 01:41:56,500
So, it depends of,

1779
01:41:56,500 --> 01:41:59,020
are you talking about general purpose models or not,

1780
01:41:59,020 --> 01:42:00,460
specialized models?

1781
01:42:00,500 --> 01:42:04,380
In general, it is a good idea to do data augmentation,

1782
01:42:04,380 --> 01:42:05,859
to use synthetic data,

1783
01:42:05,859 --> 01:42:09,779
although I would always watch the token frequency,

1784
01:42:09,779 --> 01:42:13,659
meaning you can't, because synthetic data is way cheaper.

1785
01:42:13,659 --> 01:42:17,020
And so, if you actually generate so much synthetic data

1786
01:42:17,020 --> 01:42:18,579
of a certain data domain,

1787
01:42:18,579 --> 01:42:20,180
and then it impedes on the rest

1788
01:42:20,180 --> 01:42:21,560
and lowers the performance on the rest

1789
01:42:21,560 --> 01:42:23,260
because the model just is always trained

1790
01:42:23,260 --> 01:42:25,819
on that synthetic data, then that's a problem.

1791
01:42:25,819 --> 01:42:28,659
In practice, I think also the returns of synthetic data

1792
01:42:28,659 --> 01:42:31,319
might be plateauing at some point.

1793
01:42:32,340 --> 01:42:34,539
The recent news, I guess,

1794
01:42:34,539 --> 01:42:38,779
and if you look at the DeepMind paper,

1795
01:42:38,779 --> 01:42:41,300
it's probably that we're lacking high-quality data

1796
01:42:41,300 --> 01:42:43,100
more than we're lacking synthetic data

1797
01:42:43,100 --> 01:42:45,260
for most domains right now,

1798
01:42:45,260 --> 01:42:47,020
but who knows if it's gonna be the case.

1799
01:42:47,020 --> 01:42:49,579
Some other people would say what we're actually lacking

1800
01:42:49,579 --> 01:42:52,939
is letting these agents play in RL environments

1801
01:42:52,939 --> 01:42:55,779
in the wild and generate their own synthetic data

1802
01:42:55,939 --> 01:42:59,060
or real data, but part of the game.

1803
01:42:59,060 --> 01:43:00,619
Nobody has quite the answer.

1804
01:43:00,619 --> 01:43:03,100
I would just say the trend has gone from,

1805
01:43:04,260 --> 01:43:07,859
actually, you should look at a paper from Epoch AI.

1806
01:43:07,859 --> 01:43:09,300
Maybe you've seen that already,

1807
01:43:09,300 --> 01:43:12,300
but Epoch AI has a really nice research report

1808
01:43:12,300 --> 01:43:15,760
which says by, I forgot the exact numbers,

1809
01:43:15,760 --> 01:43:20,760
but it's in there, by 2025 the world would have,

1810
01:43:21,300 --> 01:43:24,220
the frontiers would have exhausted low-quality data

1811
01:43:24,220 --> 01:43:25,399
available online.

1812
01:43:26,119 --> 01:43:27,600
In text.

1813
01:43:27,600 --> 01:43:31,000
By 2027, low-quality data in audio, image, and video

1814
01:43:31,000 --> 01:43:32,180
would have been exhausted.

1815
01:43:32,180 --> 01:43:35,760
By 2030, high-quality data would also have been exhausted

1816
01:43:35,760 --> 01:43:38,000
and at that point, it's like what's next?

1817
01:43:39,359 --> 01:43:41,060
Probably, by that time,

1818
01:43:41,060 --> 01:43:43,319
data is not gonna be the bottleneck anymore

1819
01:43:43,319 --> 01:43:46,500
and it's gonna be more about model architecture.

1820
01:43:56,340 --> 01:43:59,500
Are we producing more data than we're using?

1821
01:43:59,500 --> 01:44:02,100
Probably yes, but it doesn't mean

1822
01:44:02,100 --> 01:44:04,180
the models are not plateauing.

1823
01:44:04,180 --> 01:44:07,699
The data that you go and you code in Python,

1824
01:44:07,699 --> 01:44:11,100
your Python code is gonna be already online somewhere,

1825
01:44:11,100 --> 01:44:13,380
most likely, or 99% of it,

1826
01:44:13,380 --> 01:44:16,380
so the model is actually not learning that much from it.

1827
01:44:16,380 --> 01:44:19,220
It's just more data, not higher-quality data,

1828
01:44:19,220 --> 01:44:21,420
and that's why I think the plateau is there.

1829
01:44:21,420 --> 01:44:24,260
Maybe the best radiologist in the world

1830
01:44:24,260 --> 01:44:27,500
is producing a research paper that is so unique

1831
01:44:27,500 --> 01:44:30,539
that it's high-quality by definition

1832
01:44:30,739 --> 01:44:32,979
what the models feel is high-quality today,

1833
01:44:32,979 --> 01:44:50,470
but how much of that can we expect?

1834
01:44:50,470 --> 01:44:51,630
Yeah, it's risky.

1835
01:44:51,630 --> 01:44:53,510
Is it risky for the model to,

1836
01:44:53,510 --> 01:44:55,189
like the model, let's say, is online learning,

1837
01:44:55,189 --> 01:44:58,829
so it's learning from new data being produced by everyone.

1838
01:44:58,829 --> 01:45:03,350
Is that gonna risk the model performance to drop?

1839
01:45:03,350 --> 01:45:04,829
Essentially, that's what you're asking?

1840
01:45:04,829 --> 01:45:11,079
Or?

1841
01:45:11,079 --> 01:45:13,359
Yeah, yeah, in that case, yeah.

1842
01:45:13,359 --> 01:45:15,520
Yeah, the data produced is also coming out of AI,

1843
01:45:15,520 --> 01:45:17,800
yeah, for sure, more today than before.

1844
01:45:17,800 --> 01:45:21,319
Coding data today is increasingly used by,

1845
01:45:24,039 --> 01:45:27,119
is it increasingly generated, and so it's just fed back.

1846
01:45:27,119 --> 01:45:28,399
So just long story short,

1847
01:45:28,399 --> 01:45:31,279
it's not that interesting for training.

1848
01:45:39,069 --> 01:45:43,390
So closing remarks and reminder on what's next.

1849
01:45:43,390 --> 01:45:45,750
So by the way, I hope you feel after this lecture

1850
01:45:45,750 --> 01:45:48,390
that you have a better understanding of the techniques

1851
01:45:48,390 --> 01:45:50,590
that you can use in order to look inside a model,

1852
01:45:50,590 --> 01:45:53,069
look outside a model, both for CNNs

1853
01:45:53,069 --> 01:45:55,630
and for frontier models.

1854
01:45:55,630 --> 01:45:58,510
Again, it's just a two-hour lecture.

1855
01:45:58,510 --> 01:46:00,189
We don't have time to go so deep

1856
01:46:00,189 --> 01:46:02,989
as much as I would like it in each of these domains.

1857
01:46:05,510 --> 01:46:08,750
It's my last lecture this quarter,

1858
01:46:08,750 --> 01:46:10,630
and so thank you for participating.

1859
01:46:10,630 --> 01:46:14,470
I enjoyed spending time with you all.

1860
01:46:14,470 --> 01:46:17,069
I hope you spend time on your projects.

1861
01:46:17,069 --> 01:46:20,829
Projects can be very delightful in CS230.

1862
01:46:20,869 --> 01:46:23,750
Over the years, I've seen people use their products

1863
01:46:23,750 --> 01:46:27,550
to get a job, to start a company, to make friends,

1864
01:46:27,550 --> 01:46:29,149
and so I don't think you will regret

1865
01:46:29,149 --> 01:46:32,149
putting time and effort into your projects,

1866
01:46:32,149 --> 01:46:35,029
even if we don't have too much time left.

1867
01:46:35,029 --> 01:46:37,829
Those are the last milestones

1868
01:46:37,829 --> 01:46:40,949
or deliverables for the class.

1869
01:46:40,949 --> 01:46:42,029
I hope you enjoyed the class.

1870
01:46:42,029 --> 01:46:44,470
We're always looking for feedback,

1871
01:46:44,470 --> 01:46:46,550
and so I'm eager to hear from you all.

1872
01:46:46,550 --> 01:46:47,390
Thank you.

