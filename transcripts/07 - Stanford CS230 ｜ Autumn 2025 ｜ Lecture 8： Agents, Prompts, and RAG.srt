1
00:00:05,169 --> 00:00:12,830
Hi, everyone. Welcome to another lecture for CS230 Deep Learning. Today, we're going to

2
00:00:12,830 --> 00:00:21,210
talk about enhancing large language model applications. And I call this lecture Beyond

3
00:00:21,210 --> 00:00:32,109
LLM. It has a lot of newer content. And the idea behind this lecture is we started

4
00:00:32,109 --> 00:00:36,909
to learn about neurons, and then we learned about layers, and then we learned about deep

5
00:00:36,909 --> 00:00:44,950
neural networks. And then we learned a little bit about how to structure projects in C3.

6
00:00:44,950 --> 00:00:51,429
And now we're going one level beyond into what would it look like if you were building

7
00:00:51,429 --> 00:01:00,750
agent AI systems at work, in a startup, in a company. And it's probably one of the

8
00:01:00,869 --> 00:01:05,950
more practical lectures. Again, the goal is not to build a product end to end in the

9
00:01:05,950 --> 00:01:12,109
next hour or so, but rather to tell you all the techniques that AI engineers have

10
00:01:12,109 --> 00:01:17,790
cracked, figured out or exploring, so that after the class, you have sort of the breadth

11
00:01:17,790 --> 00:01:24,230
of view of different prompting techniques, different agent workflows, multi agency stands,

12
00:01:24,230 --> 00:01:28,549
evals. And then when you want to dive deeper, you have the baggage to dive deeper

13
00:01:28,709 --> 00:01:36,030
and learn faster about it. Okay, let's try to make it as interactive as possible as

14
00:01:36,030 --> 00:01:43,750
usual. When we look at the agenda, the agenda is going to start with the core idea

15
00:01:43,750 --> 00:01:50,709
behind challenges and opportunities for augmenting LLMs. So we start from a base model,

16
00:01:50,709 --> 00:01:56,870
how do we maximize the performance of that base model, then we dive deep into the

17
00:01:56,870 --> 00:02:03,030
first line of optimization, which is prompting methods, and we see a variety of them. Then

18
00:02:03,030 --> 00:02:07,430
we go slightly deeper if we were to get our hands under the hood and do some fine

19
00:02:07,430 --> 00:02:12,150
tuning, what would it look like? I'm not a fan of fine tuning. And I talk a lot about

20
00:02:12,150 --> 00:02:18,870
that. But I'll explain why I try to avoid fine tuning as much as possible. And then

21
00:02:18,870 --> 00:02:24,509
we'll do a section four on retrieval, augmented generation or RAG, which you've

22
00:02:24,509 --> 00:02:29,949
probably heard of in the news. Maybe some of you have played with RAGs, we're going to sort of

23
00:02:29,949 --> 00:02:37,469
unpack what a RAG is and how it works and then the different methods within RAGs. And then we

24
00:02:37,469 --> 00:02:45,550
talk about agent AI workflows. I define it, Andrew Eng is one of the colleagues, first ones

25
00:02:45,550 --> 00:02:51,629
who have called this trend a agent AI workflows. And so we look at the definition that Andrew

26
00:02:51,629 --> 00:02:59,389
gives to agent AI workflows. And then we start seeing examples. The section six is very practical.

27
00:02:59,389 --> 00:03:07,310
It's a case study where we will think about an agent AI workflow and we'll ask you to measure

28
00:03:08,590 --> 00:03:14,830
if the agent actually works. And we brainstorm how we can measure if an agent AI workflow

29
00:03:14,830 --> 00:03:19,229
is working the way you want it to work. There's plenty of methods called evals that

30
00:03:20,189 --> 00:03:25,789
solve that problem. And then we look briefly at multi-agent workflow and then we can have a

31
00:03:26,349 --> 00:03:30,430
sort of open-ended discussion where I'll share some thoughts on what's next in AI.

32
00:03:31,789 --> 00:03:34,909
And I'm looking forward to hearing from you all as well on that one.

33
00:03:35,550 --> 00:03:43,469
Okay, so let's get started with the problem of augmenting LLM. So open-ended question for you.

34
00:03:44,430 --> 00:03:50,669
You are all familiar with pre-trained models like GPT 3.5 turbo or GPT 4.0.

35
00:03:52,030 --> 00:03:58,669
What's the limitation of using just a base model? What are the typical issues that

36
00:03:58,669 --> 00:04:11,280
might arise as you're using a vanilla pre-trained model? Yes. Lacks some domain

37
00:04:11,280 --> 00:04:16,079
knowledge. You're perfectly right. You know, you, we had a group of students a few years ago,

38
00:04:16,079 --> 00:04:24,879
was not LLM related, but you know, they were building an autonomous farming device or vehicle

39
00:04:24,879 --> 00:04:31,439
that had a camera underneath taking pictures of crops to determine if the crop is sick or not,

40
00:04:31,439 --> 00:04:35,439
if it should be thrown away, like if it should be, if it should be used or not.

41
00:04:36,000 --> 00:04:43,199
And that data set is not a data set you find out there. And the base model or a pre-trained

42
00:04:43,839 --> 00:04:58,110
computer vision model would lack that knowledge, of course. What else? Yes. Okay. Maybe the,

43
00:04:58,110 --> 00:05:03,470
you're saying, so just to repeat for people online, you're saying the model might have

44
00:05:03,470 --> 00:05:08,430
been trained on high quality data, but the data in the wild is actually not that high quality.

45
00:05:08,430 --> 00:05:14,430
And in fact, yes, the distribution of the real world might differ as we've seen with GANs

46
00:05:14,430 --> 00:05:18,910
from the training sets. And that might create an issue with pre-trained models. Although

47
00:05:18,910 --> 00:05:25,709
pre-trained LLMs are getting better at, you know, handling all sorts of data inputs. Yes.

48
00:05:28,269 --> 00:05:34,750
Like what? Lacks current information. The LLM is not up to date. And in fact,

49
00:05:34,750 --> 00:05:39,149
you're right. Imagine you have to retrain from scratch your LLM every couple of months.

50
00:05:39,949 --> 00:05:45,310
One story that I found funny, it's from probably three years ago, or maybe more five

51
00:05:45,310 --> 00:05:53,949
years ago, where during his first presidency, President Trump one day tweeted Cove Fefe. You

52
00:05:53,949 --> 00:05:59,949
remember that tweet or no? Just Cove Fefe. And it was probably a typo or it was in his pocket.

53
00:05:59,949 --> 00:06:05,709
I don't know. But that word did not exist. The LLMs, in fact, that Twitter was running

54
00:06:05,709 --> 00:06:11,149
at the time, could not recognize that word. And so the recommender system sort of went

55
00:06:11,149 --> 00:06:16,589
wild. Because suddenly everybody was making fun of that tweet using the word Cove Fefe.

56
00:06:16,589 --> 00:06:21,389
And the LLM was so confused on, you know, what does that mean? Where should we show it? To whom

57
00:06:21,389 --> 00:06:27,149
should we show it? And it's an example of nowadays, especially on social media, there's

58
00:06:27,149 --> 00:06:33,149
so many new trends. And it's very hard to retrain an LLM to match the new trend and

59
00:06:33,149 --> 00:06:38,029
understand the new words out there. I mean, you know, you oftentimes hear Gen Z words like

60
00:06:38,589 --> 00:06:45,230
or mid or whatever, I don't know all of them. But you probably want to find a way that

61
00:06:45,870 --> 00:06:51,629
can allow the LLM to understand those trends without retraining the LLM from scratch. What

62
00:06:51,629 --> 00:07:01,449
else? It's trained to have a breadth of knowledge. Yeah, it might be trained on a

63
00:07:01,449 --> 00:07:07,050
breadth of knowledge, but it might fail or not perform adequately on a narrow task that

64
00:07:07,050 --> 00:07:13,290
is very well defined. Think about enterprise applications that, yeah, enterprise application,

65
00:07:13,290 --> 00:07:18,970
you need high precision, high fidelity, low latency, and maybe the model is not great

66
00:07:18,970 --> 00:07:23,449
at that specific thing. It might do fine, but just not good enough. And you might want to

67
00:07:23,449 --> 00:07:36,959
augment it in a certain way. Yeah. So maybe it has a lot of broad domain knowledge that

68
00:07:36,959 --> 00:07:41,519
might not be needed for your application. And so you're using a massive heavy model

69
00:07:41,519 --> 00:07:45,439
when you actually are only using 2% of the model capability, you're perfectly right.

70
00:07:45,439 --> 00:07:50,079
You might not need all of it. So you might find ways to prune, quantize the model,

71
00:07:50,079 --> 00:07:54,959
modify it. All of these are good points. I'm going to add a few more as well.

72
00:07:55,839 --> 00:08:00,959
LLMs are very difficult to control. Your last point is actually an example of that. You want

73
00:08:00,959 --> 00:08:05,439
to control the LLM to use a part of its knowledge, but it's not. It's in fact getting

74
00:08:05,439 --> 00:08:13,360
confused. We've seen that in history. In 2016, Microsoft created a notorious Twitter bot

75
00:08:13,920 --> 00:08:20,480
that learned from users and it quickly became a racist jerk. Microsoft ended up removing the

76
00:08:20,480 --> 00:08:26,319
bot 16 hours after launching it. The community was really fast at determining that this was a

77
00:08:26,319 --> 00:08:32,159
racist bot. And you can empathize with Microsoft in the sense that it is actually

78
00:08:32,159 --> 00:08:37,679
hard to control an LLM. They might have done a better job to qualify before launching,

79
00:08:37,679 --> 00:08:43,679
but it is really hard to control an LLM. Even more recently, this is a tweet from Sam Altman

80
00:08:44,320 --> 00:08:51,519
last November, where there was this debate between Elon Musk and Sam Altman on whose

81
00:08:51,519 --> 00:08:57,360
LLM is the left-wing propaganda machine or the right-wing propaganda machine. And they

82
00:08:57,360 --> 00:09:02,159
were hating on each other's LLMs, but that tells you at the end of the day that even those

83
00:09:02,159 --> 00:09:07,919
two teams, Grok and OpenAI, which are probably the best-funded team with a lot of talent,

84
00:09:08,559 --> 00:09:11,440
are not doing a great job at controlling their LLMs.

85
00:09:14,080 --> 00:09:20,000
And from time to time, if you hang out on X, you might see screenshots of users

86
00:09:20,000 --> 00:09:27,200
interacting with LLMs and the LLMs saying something really controversial or racist or

87
00:09:27,600 --> 00:09:34,399
something that would not be considered great by social standards, I guess. And that tells

88
00:09:34,399 --> 00:09:41,919
you that the model is really hard to control. The second aspect of it is something that you've

89
00:09:41,919 --> 00:09:48,879
mentioned earlier. LLMs may underperform in your task, and that might include specific

90
00:09:48,879 --> 00:09:53,200
knowledge gaps such as medical diagnosis. If you're doing medical diagnosis, you would rather

91
00:09:53,200 --> 00:09:58,159
have an LLM that is specialized for that and is great at it. And in fact, something that we

92
00:09:58,159 --> 00:10:03,919
haven't mentioned as a group has sources. So the answer is sourced specifically. You have a hard

93
00:10:03,919 --> 00:10:08,559
time believing something unless you have the actual source of the research that backs it up.

94
00:10:10,240 --> 00:10:14,320
Inconsistencies in style and format. So imagine you're building a legal AI

95
00:10:15,120 --> 00:10:22,879
agentic workflow. Legal has a very specific way to write and read where every word counts.

96
00:10:22,879 --> 00:10:28,399
If you're negotiating a large contract, every word on that contract might mean something else

97
00:10:28,399 --> 00:10:33,039
when it comes to the court. And so it's very important that you use an LLM that is very good

98
00:10:33,039 --> 00:10:38,559
at it. The precision matters. And then task-specific understandings such as doing

99
00:10:38,559 --> 00:10:44,159
a classification on a niche field. Here I pulled an example where let's say a biotech

100
00:10:44,159 --> 00:10:51,759
product is trying to use an LLM to categorize user reviews into positive, neutral, or

101
00:10:51,759 --> 00:11:00,799
negative. You know, maybe for that company something that would be considered a negative

102
00:11:00,799 --> 00:11:06,159
review typically is actually considered a neutral review because the NPS of that industry

103
00:11:06,159 --> 00:11:12,559
tends to be way lower than other industries, let's say. That's a task-specific understanding

104
00:11:12,559 --> 00:11:16,799
and the LLM needs to be aligned to what the company believes is the categorization that it

105
00:11:16,799 --> 00:11:20,720
wants. We will see an example of how to solve that problem in a second.

106
00:11:21,519 --> 00:11:28,080
And then limited context handling. A lot of AI applications, especially in the enterprise,

107
00:11:28,720 --> 00:11:35,039
have required data that has a lot of context. Just to give you a simple example,

108
00:11:35,039 --> 00:11:39,759
knowledge management is an important space that enterprises buy a lot of knowledge management

109
00:11:39,759 --> 00:11:44,559
tools. When you go on your drive and you have all your documents, ideally you could have

110
00:11:44,559 --> 00:11:50,000
an LLM running on top of that drive. You can ask any question and it will read immediately

111
00:11:50,559 --> 00:11:57,600
thousands of documents and answer what was our Q4 performance in sales? It was X dollars.

112
00:11:58,240 --> 00:12:03,919
It finds it super quickly. In practice, because LLMs do not have a large enough context,

113
00:12:03,919 --> 00:12:08,480
you cannot use a standalone vanilla pre-trained LLM to solve that problem.

114
00:12:08,480 --> 00:12:15,519
You will have to augment it. Does that make sense? The other aspect around context windows

115
00:12:15,519 --> 00:12:20,879
is they are in fact limited. If you look at the context windows of the models from

116
00:12:20,879 --> 00:12:29,919
the last five years, even the best models today will range in context window or number

117
00:12:29,919 --> 00:12:36,559
of tokens it can take as input, somewhere in the hundreds of thousands of tokens max.

118
00:12:36,559 --> 00:12:44,240
Just to give you a sense, 200,000 tokens is roughly two books. That's how much you can upload

119
00:12:45,279 --> 00:12:50,720
and it can read pretty much. You can imagine that when you're dealing with video understanding

120
00:12:50,720 --> 00:12:58,000
or heavier data files, that is of course an issue. You might have to chunk it. You

121
00:12:58,000 --> 00:13:02,720
might have to embed it. You might have to find other ways to get the LLM to handle

122
00:13:02,720 --> 00:13:11,320
larger contexts. The attention mechanism is also powerful but problematic because it does not

123
00:13:11,320 --> 00:13:17,960
do a great job at attending in very large contexts. There is actually an interesting

124
00:13:19,080 --> 00:13:25,639
problem called needle in a haystack. It's an AI problem or call it a benchmark where

125
00:13:26,759 --> 00:13:32,919
in order to test if your LLM is good at putting attention on a very specific fact

126
00:13:32,919 --> 00:13:41,240
within a large corpus, researchers might randomly insert in a book one sentence that

127
00:13:43,399 --> 00:13:49,480
outlines a certain fact, such as Arun and Max are having coffee at Blue Bottle in the middle

128
00:13:49,480 --> 00:13:58,519
of the Bible, let's say, or some very long text. Then you ask the LLM what were Arun and

129
00:13:58,519 --> 00:14:05,240
Max having at Blue Bottle and you see if it remembers that it was coffee. It's actually

130
00:14:05,240 --> 00:14:09,399
a complex problem not because the question is complex but because you're asking the model to

131
00:14:09,399 --> 00:14:18,360
find a fact within a very large corpus and that's complicated. Again, this is a limiting

132
00:14:18,360 --> 00:14:24,200
factor for LLMs. We'll talk about RAG in a second but I want to preview. There is

133
00:14:24,840 --> 00:14:31,399
debates around whether RAG is the right long-term approach for AI systems. As a high level idea,

134
00:14:31,960 --> 00:14:37,799
RAG is a mechanism, if you will, that embeds documents that an LLM can retrieve

135
00:14:38,759 --> 00:14:45,080
and then add as context to its initial prompt and answer a question. It has lots of

136
00:14:45,080 --> 00:14:49,559
applications. Knowledge management is an example. Imagine you have your drive again but every

137
00:14:49,559 --> 00:14:56,679
document is sort of compressed in representation and the LLM has access to that lower dimensional

138
00:14:56,679 --> 00:15:06,759
representation. The debate that this tweet from Yao Fu outlines is in theory, if we have

139
00:15:06,759 --> 00:15:13,559
infinite compute, then RAG is useless because you can just read a massive corpus immediately

140
00:15:13,559 --> 00:15:19,799
and answer your question. Even in that case, latency might be an issue. Imagine the time it

141
00:15:19,799 --> 00:15:24,759
takes for an AI to read all your drive every single time you ask a question. It doesn't make

142
00:15:24,759 --> 00:15:32,759
sense. RAG has other advantages beyond even the accuracy. On top of that, the sourcing

143
00:15:32,759 --> 00:15:40,039
matters as well. RAG allows you to source. We'll talk about all that later. There's always

144
00:15:40,039 --> 00:15:46,919
this debate in the community whether a certain method is actually future proof because in practice

145
00:15:46,919 --> 00:15:52,200
as compute power doubles every year, let's say, some of the methods we're learning right now

146
00:15:52,200 --> 00:16:01,559
might not be relevant three years from now. We don't know essentially. The analogy that

147
00:16:01,559 --> 00:16:07,399
he makes on context windows and why RAG approaches might be relevant even a long time

148
00:16:07,399 --> 00:16:15,080
from now is search. When you search on a search engine, you still find sources of information.

149
00:16:15,080 --> 00:16:22,120
In fact, in the background, there is very detailed traversal algorithms that rank and find

150
00:16:22,840 --> 00:16:28,759
the specific links that might be the best to present you versus if you had to read,

151
00:16:28,759 --> 00:16:32,519
imagine you had to read the entire web every single time you're doing a search query

152
00:16:33,159 --> 00:16:40,200
without being able to narrow to certain portion of the space that might again not be reasonable.

153
00:16:42,679 --> 00:16:50,039
When we're thinking of improving LLMs, the easiest way we think of it is two dimensions.

154
00:16:50,039 --> 00:16:55,320
One dimension is we are going to improve the foundation model itself. For example,

155
00:16:55,320 --> 00:17:06,039
we move from GPT 3.5 turbo to GPT 4 to GPT 4.0 to GPT 5. Each of that is supposed to improve the

156
00:17:06,039 --> 00:17:12,440
base model. GPT 5 is another debate because it's sort of packaging other models within itself,

157
00:17:12,440 --> 00:17:17,480
but if you're thinking about 3.5, 4, and 4.0, that's really what it is. The pre-trained

158
00:17:17,480 --> 00:17:21,880
model improves, and so you should see your performance improve on your tasks.

159
00:17:22,839 --> 00:17:30,440
The other dimension is we can actually engineer, leverage the LLM in a way that makes it better.

160
00:17:30,440 --> 00:17:38,519
You can prompt simply GPT 4.0. You can change some prompts and improve the prompt, and it

161
00:17:38,519 --> 00:17:43,559
will improve the performance. It's shown. You can even put a rag around it. You can

162
00:17:43,559 --> 00:17:49,160
put an agentic workflow around it. You can even put a multi-agent system around it,

163
00:17:49,160 --> 00:17:54,200
and that is another dimension for you to improve performance. That's how I want you to think about

164
00:17:54,200 --> 00:17:58,039
it. Which LLM I'm using, and then how can I maximize the performance of that LLM?

165
00:17:59,480 --> 00:18:05,000
This lecture is about the vertical axis. Those are the methods that we will see together.

166
00:18:08,839 --> 00:18:13,160
Sounds good for the introduction. Let's move to prompt engineering.

167
00:18:14,599 --> 00:18:19,400
I'm going to start with an interesting study just to motivate why prompt engineering matters.

168
00:18:19,559 --> 00:18:29,400
There is a study from HPS, UPenn, as well as Harvard Business School, and others also in

169
00:18:29,400 --> 00:18:35,799
Wharton that took a subset of BCG consultants, individual contributors, and split them into three

170
00:18:35,799 --> 00:18:42,759
groups. One group had no access to AI. One group had access to, I think it was GPT 4.0,

171
00:18:43,480 --> 00:18:49,720
and then one group had access to the LLM, but also a training on how to prompt better.

172
00:18:50,839 --> 00:18:56,039
Then they observed the performance of these consultants across a wide variety of tasks.

173
00:18:56,039 --> 00:18:59,319
There are a few things that they noticed that I thought was interesting.

174
00:18:59,319 --> 00:19:05,880
One is something they call the JAG frontier, meaning that certain tasks that consultants are

175
00:19:05,880 --> 00:19:17,079
doing fall beyond the JAG frontier, meaning AI is not good enough. It's not improving

176
00:19:17,079 --> 00:19:22,680
human performance. In fact, it's actually making it worse. Some tasks are within the frontier,

177
00:19:23,319 --> 00:19:29,559
meaning that AI is actually significantly improving the performance, the speed, the quality

178
00:19:29,640 --> 00:19:36,119
of the consultants. Many tasks fell within and many tasks fell without, and they shared their

179
00:19:36,839 --> 00:19:43,720
insights, but the TLDR is there is a frontier within which AI is absolutely helping and one

180
00:19:43,720 --> 00:19:50,599
where they call out this behavior of falling asleep at the wheel, where people relied on AI

181
00:19:50,599 --> 00:19:56,519
on a task that was beyond the frontier. In fact, it ended up going worse because the human

182
00:19:56,519 --> 00:20:05,079
was not reviewing the outputs carefully enough. They did note that the group that was trained was

183
00:20:05,079 --> 00:20:09,400
the best, better than the group that was not trained on prompt engineering,

184
00:20:09,400 --> 00:20:15,160
which also motivates why this lecture matters so that you're within that group afterwards.

185
00:20:15,880 --> 00:20:21,960
One other insight were the centaurs and the cyborgs. They noticed that consultants had the

186
00:20:21,960 --> 00:20:28,200
tendency to work with AI in one of two ways, and you might yourself be part of one of these

187
00:20:28,200 --> 00:20:41,000
groups. The centaurs are mythical creatures that are half-human, half-horses, half-something,

188
00:20:42,119 --> 00:20:47,160
and those were individuals that would divide and delegate. They might give a pretty big

189
00:20:47,160 --> 00:20:51,960
task to the AI, so imagine you're working on a PowerPoint, which consultants are known to do.

190
00:20:52,759 --> 00:20:56,839
You might actually write a very long prompt on how you want it to do your PowerPoint,

191
00:20:56,839 --> 00:20:59,960
and then let it work for some time, and then come back and it's done.

192
00:20:59,960 --> 00:21:05,799
When others would act as cyborgs, cyborgs are fully blended, bionic human robots,

193
00:21:07,240 --> 00:21:13,079
and robots augmented with robotic parts, and those individuals would not delegate fully a

194
00:21:13,079 --> 00:21:17,720
task. They would actually work super quickly with the model and back and forth. I find that

195
00:21:17,720 --> 00:21:23,799
a lot of students are actually more working like cyborgs than centaurs, but why maybe in the

196
00:21:23,799 --> 00:21:27,880
enterprise when you're trying to automate the workflow, you're thinking more like a centaur.

197
00:21:29,319 --> 00:21:32,839
That's just something good to keep in mind. Also, a lot of companies will tell you,

198
00:21:32,839 --> 00:21:38,039
we're hiring prompt engineers, etc. It's a curer. I don't buy that. I think it's just a skill

199
00:21:38,039 --> 00:21:41,960
that everybody should have. You're not going to make a curer out of prompt engineering,

200
00:21:41,960 --> 00:21:46,519
but you're probably going to use it as a very powerful skill in your curer.

201
00:21:49,950 --> 00:21:55,230
Let's talk about basic prompt design principles. I'm giving you a very simple prompt here.

202
00:21:55,950 --> 00:22:00,269
Summarize this document, and then the document is uploaded alongside it,

203
00:22:00,269 --> 00:22:06,670
and the model has not much context around what should be the summary, how long should

204
00:22:06,670 --> 00:22:11,309
be the summary, what should it talk about, etc. You can actually improve these prompts

205
00:22:12,190 --> 00:22:19,470
by doing something like summarize this 10-page scientific paper on renewable energy in five

206
00:22:19,470 --> 00:22:25,470
bullet points focusing on key findings and implications for policymakers. That's already

207
00:22:25,470 --> 00:22:30,670
better. You're sharing the audience, and it's going to tailor it to the audience. You're

208
00:22:30,670 --> 00:22:34,990
saying that you want five bullet points, and you want to focus only on key findings.

209
00:22:35,950 --> 00:22:41,789
That's a better prompt, you would argue. How could you even make these prompts better?

210
00:22:41,789 --> 00:22:46,190
What are other techniques that you've heard of or tried yourself that could make this

211
00:22:46,190 --> 00:23:02,490
one-shot prompt better? Write examples. Here is an example of a great summary.

212
00:23:02,809 --> 00:23:04,250
Yeah, you're right, that's a good idea.

213
00:23:08,880 --> 00:23:16,160
Very popular technique. Act like a renewable energy expert giving a conference at Davos,

214
00:23:16,160 --> 00:23:25,599
let's say. Yeah, that's great. You are the best in the world at this.

215
00:23:25,599 --> 00:23:31,759
Explain. Yeah, actually, these things work. It's funny, but it does work to say act

216
00:23:31,759 --> 00:23:36,880
like XYZ. It's a very popular prompt template. We see a few examples. What else could you do?

217
00:23:41,000 --> 00:23:50,359
Yes. Critique your own project, so you're using reflection. You might actually do one output

218
00:23:50,359 --> 00:23:54,440
and then ask it to critique it and then give it back. Yeah, we see that. That's a great

219
00:23:54,440 --> 00:23:58,920
one. That's the one that probably works best within those typically, but we see some

220
00:23:58,920 --> 00:24:05,920
examples. What else? Yeah, okay. Break the task down in two steps. Do you know how that

221
00:24:05,920 --> 00:24:14,319
is called? Chain of thoughts. This is actually a popular method that's been shown in research

222
00:24:14,319 --> 00:24:18,799
that it improves. You could actually give a clear instruction and also encourage the

223
00:24:18,799 --> 00:24:24,240
model to think step-by-step. Approach the task step-by-step and do not steep any step.

224
00:24:24,240 --> 00:24:29,200
Then you give it some steps such as step one, identify the three most important findings.

225
00:24:29,200 --> 00:24:34,400
Step two, explain how key each finding impacts renewable energy policy. Step three,

226
00:24:34,400 --> 00:24:41,359
write the five bullet summary with each point addressing a finding, etc. Chain of thoughts.

227
00:24:41,359 --> 00:24:47,920
I linked the paper from 2023 that popularized chain of thoughts. Chain of thoughts is very

228
00:24:47,920 --> 00:24:51,759
popular right now, especially in AI startups that are trying to control their LLMs.

229
00:24:56,349 --> 00:25:01,869
To go back to your examples about act like XYZ, what I like to do,

230
00:25:01,869 --> 00:25:08,430
Andrew also talks about that, is to look at other people's prompts. In fact, online you have a lot

231
00:25:08,430 --> 00:25:15,309
of prompt repositories for free on GitHub. In fact, I linked the awesome prompt template repo

232
00:25:15,309 --> 00:25:20,430
on GitHub where you have so many examples of great prompt that engineers have built.

233
00:25:20,430 --> 00:25:24,990
They said it works great for us and they published it online. A lot of them starts

234
00:25:25,789 --> 00:25:32,990
with act as, act as a Linux terminal, act as an English translator, act like a position

235
00:25:32,990 --> 00:25:41,200
interviewer, etc. The advantage of a prompt template is that you can actually put it in your

236
00:25:41,200 --> 00:25:47,440
code and scale it for many user requests. Let me give you an example from Workera.

237
00:25:48,559 --> 00:25:52,960
Workera evaluates skills. Some of you have taken the assessments already

238
00:25:52,960 --> 00:25:59,440
and tries to personalize it to the user. In fact, if you actually read in an HR system

239
00:25:59,440 --> 00:26:06,160
in an enterprise in the HR system, you might have Jane is a product manager level three

240
00:26:06,160 --> 00:26:12,640
and she is in the US and her preferred language is English. Actually, that metadata

241
00:26:12,640 --> 00:26:18,400
can be inserted in a prompt template that we personalize for Jane. Similarly, for Joe,

242
00:26:18,400 --> 00:26:25,759
whose preferred language is Spanish, it will tailor it to Joe and that's called a prompt template.

243
00:26:36,170 --> 00:26:41,369
The question is, do the foundation models use a prompt template or do you have to

244
00:26:41,369 --> 00:26:47,450
integrate it yourself? The foundation models probably use a system prompt that you don't see.

245
00:26:47,450 --> 00:26:54,569
When actually you type on chat GPT, it is possible, it's not public, that OpenAI

246
00:26:54,569 --> 00:27:00,410
behind the scene act like a very helpful assistant for this user. By the way,

247
00:27:00,410 --> 00:27:05,849
here is your memories about the user that we kept in our database. You can actually check

248
00:27:05,849 --> 00:27:10,730
your memories and then your prompt goes under and then the generation starts. Probably they're

249
00:27:10,730 --> 00:27:16,250
using something like that, but it doesn't mean you can't add one yourself. In fact,

250
00:27:16,890 --> 00:27:20,170
if you think about a prompt template for the work here example I was showing,

251
00:27:20,730 --> 00:27:26,250
maybe it starts when you call OpenAI by act like a helpful assistant and then underneath,

252
00:27:26,250 --> 00:27:32,650
it's like act like a great AI mentor that helps people in their career and OpenAI's

253
00:27:32,650 --> 00:27:36,809
prompt template also has follow the instruction from the creator or something like that.

254
00:27:37,450 --> 00:27:44,680
It's possible. Questions about prompt templates? Again, I would encourage you to go and read

255
00:27:44,680 --> 00:27:50,519
examples of prompts. Some of them are quite thoughtful. Let's talk about zero-shot versus

256
00:27:50,519 --> 00:27:55,559
few-shot prompting. It came up earlier. Here's an example again going back to the

257
00:27:55,559 --> 00:28:02,039
categorization of product reviews. Let's say that we're working on a task where the prompt

258
00:28:02,039 --> 00:28:08,519
is classified the tone of the sentence as positive, negative, or neutral and then you

259
00:28:08,519 --> 00:28:17,230
paste the review, which is the product is fine, but I was expecting more. If I were to survey

260
00:28:17,230 --> 00:28:22,269
the room, I would bet that some of you would say it's negative. Some of you would say it's

261
00:28:22,269 --> 00:28:27,390
neutral because you actually have a first part that is relatively positive. It's fine.

262
00:28:28,430 --> 00:28:32,349
And then the second part I was expecting more, which is relatively negative. So where do you

263
00:28:32,349 --> 00:28:37,390
land? This can be a subjective question and maybe in one industry this would be considered

264
00:28:37,390 --> 00:28:41,789
amazing and in another one it would be considered really bad because people are used to really

265
00:28:41,789 --> 00:28:48,109
flourishing reviews. The way you can actually align the model to your task is by converting

266
00:28:48,109 --> 00:28:52,509
that zero-shot prompt. Zero-shot refers to the fact that it's not beginning given any example

267
00:28:53,549 --> 00:28:59,549
into a few-shot prompt where the model is given in the prompt a set of examples to

268
00:28:59,549 --> 00:29:04,670
align it to what you want it to do. The example here is again you paste the same prompt

269
00:29:04,670 --> 00:29:10,430
as before with the user review and then you add here are examples of tone classifications.

270
00:29:10,430 --> 00:29:16,990
This exceeded my expectation completely. Positive. It's okay, but I wish it had more features.

271
00:29:17,710 --> 00:29:24,990
Negative. The service was adequate, neither good nor bad. Mutual. Now classify the tone of

272
00:29:24,990 --> 00:29:29,950
this sentence after you've heard about these things and the model then says

273
00:29:30,589 --> 00:29:37,950
negative and the reason it says negative of course is likely because of the second example

274
00:29:37,950 --> 00:29:42,589
which was it's okay but I wish it had more features which we told the model that was

275
00:29:42,589 --> 00:29:46,589
negative because the model saw that it's aligned now with your expectations.

276
00:29:47,549 --> 00:29:54,509
Few-shot prompts are very popular and in fact for AI startups that are slightly more sophisticated

277
00:29:54,509 --> 00:30:00,829
you might see them keep a prompt up to date whenever a user says something and they might

278
00:30:00,829 --> 00:30:06,750
have a human label it and then add it as a few shots in their relevant prompts in their code

279
00:30:06,750 --> 00:30:11,710
base. You can think of that as almost building a data set but instead of actually building a

280
00:30:11,710 --> 00:30:16,349
separate data set like we've seen with supervised fine-tuning and then fine-tuning

281
00:30:16,349 --> 00:30:20,670
the model on it you're just putting it directly in the prompt and turns out it's probably

282
00:30:21,230 --> 00:30:24,829
faster to do that if you want to experiment quickly because you don't touch the model

283
00:30:24,829 --> 00:30:30,589
parameters you just update your prompts and you know if it's text examples you can actually

284
00:30:30,589 --> 00:30:36,269
you know concatenate so many examples in a single prompt. At some point it will be too long

285
00:30:36,269 --> 00:30:40,670
and you will not have the necessary context window but it's a pretty strong approach

286
00:30:40,670 --> 00:30:59,599
that is quick to align an LLM. Okay, yes. So the question was is there any research

287
00:30:59,599 --> 00:31:04,880
on how long the prompt can be before the model essentially uses itself or doesn't follow

288
00:31:04,880 --> 00:31:11,279
instructions anymore? There is. The problem is that research is outdated every few months

289
00:31:11,920 --> 00:31:17,359
because models get better and so I don't know where the state of the art is you can probably

290
00:31:17,359 --> 00:31:24,319
find it online on benchmarks on like we see that I use an example on the Workera product

291
00:31:24,319 --> 00:31:29,759
you have a voice conversation for some of you that have tried it where you you're asked

292
00:31:29,759 --> 00:31:32,799
explain what is the prompt and then you explain and then there's a scoring algorithm in the

293
00:31:32,799 --> 00:31:39,759
line. We know that after eight turns the model loses itself after eight turns because

294
00:31:39,759 --> 00:31:45,519
you always paste the previous user response it just starts going wild and so the techniques

295
00:31:45,519 --> 00:31:50,240
we use in the background is we actually create chapters of the conversation maybe one chapter

296
00:31:50,240 --> 00:31:54,880
is the first eight prompt and then you actually start over from another prompt you can summarize

297
00:31:54,880 --> 00:31:59,920
the first part of the conversation insert the summary and then keep going you know those are

298
00:31:59,920 --> 00:32:04,480
engineering hacks that engineers might have figured out in the background yeah because

299
00:32:04,480 --> 00:32:15,950
the eight turns makes a prompt quite long actually. Let's move on to chaining. Chaining

300
00:32:15,950 --> 00:32:21,789
is the most popular technique out of everything we've seen so far in prompt engineering.

301
00:32:22,750 --> 00:32:26,990
It's not chain of thought so chain of thought we've seen is think step by step step one step two

302
00:32:26,990 --> 00:32:32,589
step three do not skip any step this is different this is chaining complex prompt

303
00:32:32,589 --> 00:32:38,829
to improve performance and this is what it looks like. You take a single step prompt such

304
00:32:38,829 --> 00:32:44,109
as read this customer review and write a professional response that acknowledges their

305
00:32:44,109 --> 00:32:49,869
concern explains the issue offers a resolution and then you paste the customer review which is

306
00:32:49,869 --> 00:32:56,029
I ordered a laptop it arrived three days late the packaging was damaged very disappointing

307
00:32:56,029 --> 00:33:02,910
I needed that urgently for work and then the output is an email that is immediately given to

308
00:33:02,910 --> 00:33:12,109
you by the LLM after it reads the prompt. So this might work but it might be hard to

309
00:33:12,109 --> 00:33:18,430
control you know because think about it there's multiple steps that you have listed and

310
00:33:18,509 --> 00:33:23,789
everything is embedded in the same prompt and if you wanted to debug step by step and know which

311
00:33:23,789 --> 00:33:29,549
step is weaker you couldn't you would have everything mixed together. So one advantage of

312
00:33:29,549 --> 00:33:33,950
chaining is you know you would you would separate the prompts so that you can debug

313
00:33:33,950 --> 00:33:40,109
them separately and it will also lead to an easier manner to improve your workflow.

314
00:33:41,309 --> 00:33:46,190
Let's say a first prompt is extract the key issues identify the key concerns mentioned in

315
00:33:46,190 --> 00:33:53,069
this customer review paste a customer review. Second prompt using these issues so you paste back

316
00:33:53,069 --> 00:33:58,910
the issues draft an outline for a professional response that acknowledges concerns explains

317
00:33:58,910 --> 00:34:07,880
possible reasons and offer a resolution. So this is not you know prompt number three

318
00:34:07,880 --> 00:34:13,639
write the full response so using the outline write the professional response

319
00:34:14,360 --> 00:34:21,559
and then you get your final output. So in theory you can't tell me oh the second approach is

320
00:34:21,559 --> 00:34:27,079
better than the first one at first but what you can notice is that we can actually test those

321
00:34:27,079 --> 00:34:33,000
three prompts separately from each other and determine if we will get the most gains out of

322
00:34:34,360 --> 00:34:39,880
engineering the first prompt optimizing it or the second one or the third one. We now have

323
00:34:40,360 --> 00:34:45,880
three prompts that are independent from each other and you know maybe if the outline was

324
00:34:45,880 --> 00:34:53,480
better the performance of the email the email how much it will the open rate will be or the

325
00:34:53,480 --> 00:35:00,039
user satisfaction on the response will actually get higher you know and so chaining improves

326
00:35:00,039 --> 00:35:05,719
performance but most importantly helps you control your workflow and debug it more seamlessly.

327
00:35:07,849 --> 00:35:33,400
Yes so let me try to rephrase you say let's say we look at the first prompt which has

328
00:35:33,400 --> 00:35:40,360
all three tasks built in that prompt but what exactly you mean you mean like if we evaluate

329
00:35:40,360 --> 00:35:47,239
the output and we measure some user insight satisfaction etc why don't we just modify that

330
00:35:47,239 --> 00:35:52,119
prompt and essentially see how it improves user satisfaction. Yeah it's not going to give you

331
00:35:52,199 --> 00:36:00,039
that process. I see why do we need the three steps yeah I mean think about it the intermediate

332
00:36:00,039 --> 00:36:07,079
output is what you want to see like if I'm debugging the first approach the way I would

333
00:36:07,079 --> 00:36:11,559
do it is I would capture user insights like here's the email how good was the response

334
00:36:11,559 --> 00:36:18,440
thumbs up thumbs down was your issue resolved thumbs up thumbs down those would tell me how

335
00:36:18,440 --> 00:36:22,519
good is my prompt and I can engineer that prompt optimize it and I would probably drive some gains

336
00:36:23,480 --> 00:36:29,159
but I will not be able easily to trace back to what the problem was while in the second

337
00:36:29,159 --> 00:36:33,960
approach not only I can use the end-to-end metrics to improve my process I can also

338
00:36:33,960 --> 00:36:38,679
use the intermediate steps for example if I look at prompt two and I look at the outline

339
00:36:38,679 --> 00:36:43,639
and I see the outline is actually meh is not great then I think I can get a lot of gains

340
00:36:43,719 --> 00:36:49,400
out of the outline or the outline is actually really good but the last prompt doesn't do a good

341
00:36:49,400 --> 00:36:54,519
job at translating it into an email so the outline is exactly what I want the LLM to do

342
00:36:54,519 --> 00:36:59,400
but the the translation in a customer facing email is not good in fact it doesn't follow

343
00:36:59,400 --> 00:37:03,880
our vocabulary internally internally then I know the third prompt is where I would get

344
00:37:03,880 --> 00:37:09,000
the most gains so that that's what it allows me to do have intermediate steps to review

345
00:37:09,719 --> 00:37:18,429
yeah we'll talk about it are there any latency concerns yes in certain applications

346
00:37:19,230 --> 00:37:24,349
you don't want to use a chain or you don't want to use a long chain because it adds latency

347
00:37:26,030 --> 00:37:31,949
we'll talk about that later good point so practically this is what chaining complex

348
00:37:31,949 --> 00:37:37,469
prompts look like you have your first prompt with your first task it outputs the output is

349
00:37:37,469 --> 00:37:42,670
pasted in the second prompt with the second task being defined the output is then pasted

350
00:37:42,670 --> 00:37:47,710
into the third prompt with the third task being defined and so on that's what it looks like in

351
00:37:47,710 --> 00:37:59,610
practice super we'll talk more later about testing your prompts but there are methods

352
00:37:59,610 --> 00:38:05,050
now to do it and we will see later in this lecture with our case study how we can test

353
00:38:05,210 --> 00:38:12,650
prompts but here is an example of how you might do it you might have a

354
00:38:14,010 --> 00:38:21,690
summarization workflow you know prompts that is the baseline it's a single prompt you might

355
00:38:21,690 --> 00:38:28,809
have a refined summarization which is a modified prompt of this or workflow with a chain you

356
00:38:28,809 --> 00:38:36,730
know and then you have your test case which is the input that you want to summarize let's say

357
00:38:36,730 --> 00:38:42,409
and then you have the generated outputs and you can have humans go and rate these outputs

358
00:38:42,409 --> 00:38:48,250
and you would notice that the baseline is better or worse than the refined prompt of course

359
00:38:48,250 --> 00:38:55,769
this manual approach takes time but it's a good way to start and usually the advice is

360
00:38:55,769 --> 00:38:59,449
get hands-on at the beginning because you would quickly notice some issues and it will give you

361
00:38:59,449 --> 00:39:04,969
better intuition on what tweaks can lead to better performance however if you wanted to scale that

362
00:39:04,969 --> 00:39:09,929
system across many products many parts of your code base you might want to find a way to do

363
00:39:09,929 --> 00:39:16,489
that automatically without asking humans to review and grade summaries right one approach

364
00:39:16,489 --> 00:39:23,449
is to use you know platforms like at Porchera our team uses a platform called TrumpFoo that

365
00:39:23,449 --> 00:39:30,650
allows you to actually automate part of this testing in a nutshell what it does is it can

366
00:39:30,650 --> 00:39:36,809
allow you to run the same prompts with five different LLMs immediately put everything in

367
00:39:36,809 --> 00:39:42,730
a table that makes it super easy for a human to grade let's say or alternatively it might

368
00:39:42,730 --> 00:39:49,449
allow you to define define LLM judges LLM judges can come in different flavors

369
00:39:50,090 --> 00:39:56,010
for example I can have an LLM judge that does a pairwise comparison so what the LLM is asked to

370
00:39:56,010 --> 00:40:00,010
do is here are two summaries just tell me which one is better than the other one

371
00:40:01,130 --> 00:40:05,449
that's what the LLM does and that can be used as a proxy for how good the summarization

372
00:40:05,449 --> 00:40:12,969
baseline versus the refined version is another way to do an LLM judge is if you do it for a

373
00:40:12,969 --> 00:40:19,210
single answer grading so here's a summary rated from one to five you know and then you can go

374
00:40:19,210 --> 00:40:26,409
even deeper and do a reference guided pairwise comparison or you add also a rubric you say

375
00:40:26,409 --> 00:40:32,010
a five is when a summary is below a hundred characters I'm just making up below a hundred

376
00:40:32,010 --> 00:40:37,530
characters mentions at least three key points that are distinct and starts with a first

377
00:40:37,530 --> 00:40:41,449
sentence that displays the overview and then goes into detail that's a great summary number

378
00:40:41,449 --> 00:40:48,969
five out of five zero is the LLM failed to summarize and actually was very verbose let's

379
00:40:48,969 --> 00:40:55,369
say and so you put a rubric behind it and you have an LLM as just finding the rubric of course

380
00:40:55,369 --> 00:40:59,289
you can now pair different techniques you can do a few shots for the rubric you can actually

381
00:40:59,289 --> 00:41:04,329
give examples of them five out of fives four out of fours three out of threes because now

382
00:41:04,329 --> 00:41:14,039
you know multiple techniques okay does that make sense okay so that was the second section

383
00:41:14,039 --> 00:41:21,559
on prompt engineering or the first line of optimization now let's say you've exhausted

384
00:41:21,559 --> 00:41:26,119
all your chances for prompt engineering and you're thinking about actually touching the

385
00:41:26,119 --> 00:41:33,480
model modifying its weights or fine tuning it in other words I was telling you I'm not a fan

386
00:41:33,480 --> 00:41:42,119
of fine tuning there's a few reasons why one it requires substantial labeled data typically

387
00:41:42,119 --> 00:41:48,920
to fine tune although now there are approaches that are getting better at fine tuning that look

388
00:41:48,920 --> 00:41:55,000
more like few shot prompting actually than fine tuning it's sort of merging although one

389
00:41:55,000 --> 00:41:59,960
modifies the weight the other doesn't modify the weights fine tune models may also overfeed

390
00:41:59,960 --> 00:42:05,960
to specific data we're going to see a funny example actually losing their general purpose

391
00:42:05,960 --> 00:42:10,840
utility so you might fine tune a model and actually when someone asks a pretty generic

392
00:42:10,840 --> 00:42:14,599
question it doesn't do well anymore you know it might do well on your task so it might be

393
00:42:14,599 --> 00:42:19,960
relevant or not and then it's it's time and cost intensive that's my main problem and you

394
00:42:19,960 --> 00:42:25,800
know I work here we don't we don't we steer away from fine tuning as much as possible

395
00:42:26,360 --> 00:42:30,440
because by the time you're done fine tuning your model the next model is out and it's actually

396
00:42:30,440 --> 00:42:35,480
beating your fine-tuned version of the previous model so I would steer away from fine tuning

397
00:42:35,480 --> 00:42:40,039
as much as you can the advantage of the prompt engineering methods we've seen is you can put

398
00:42:40,039 --> 00:42:46,360
the next best pre-trained model directly in your code it will update everything immediately

399
00:42:46,360 --> 00:42:53,179
fine tuning doesn't work like that there are advantages though where it still makes sense

400
00:42:53,179 --> 00:42:58,460
if the task requires repeated high precision outputs such as legal scientific explanation

401
00:42:58,460 --> 00:43:05,019
and if the general purpose llm struggles with domain specific language so let's look at a

402
00:43:05,019 --> 00:43:13,739
quick example together which is an example from ross lazarovitz I think it was a couple of

403
00:43:13,739 --> 00:43:24,380
years ago september 23 where ross tried to do slack fine tuning so he looked at a lot of slack

404
00:43:24,380 --> 00:43:30,460
messages within his company and he was like I'm going to fine tune a model that speaks like us

405
00:43:30,460 --> 00:43:35,099
or operates like us because this is how we work right this is the data that represents how

406
00:43:35,099 --> 00:43:41,659
people work at the company and so if he actually went ahead and fine tuned the model

407
00:43:42,460 --> 00:43:48,300
gave it a prompt like hey write a you know he was delegating to the model write a 500 word

408
00:43:48,300 --> 00:43:53,579
blog post on prompt engineering and the model responded I shall work on that in the morning

409
00:43:55,900 --> 00:44:02,059
and and then he tries to push the model a little further and say it's morning now and

410
00:44:02,059 --> 00:44:09,340
the model said I'm writing right now it's 6 30 a.m here write it now okay please

411
00:44:09,340 --> 00:44:14,619
okay I shall write it now I actually don't know what you would like me to say about prompt

412
00:44:14,619 --> 00:44:19,019
engineering I can only describe the process the only thing that comes to mind for a headline

413
00:44:19,019 --> 00:44:25,099
is how do we build prompt you know it's kind of a funny example for fine tuning because

414
00:44:25,099 --> 00:44:30,219
it's true that it went wrong like he was supposed to think like I want the model to

415
00:44:30,219 --> 00:44:35,980
speak like us at work and it ended up acting like people and not actually following

416
00:44:35,980 --> 00:44:47,579
instructions so one example why I would steer away from fine tuning super

417
00:44:51,579 --> 00:44:57,579
let's talk about rags rags is important it's important to know out there and at least having

418
00:44:57,579 --> 00:45:01,340
the basics it's a very common interview question by the way if you go interview

419
00:45:01,980 --> 00:45:06,460
for a job they might ask you to explain in a nutshell to a five-year-old what is a rag

420
00:45:06,539 --> 00:45:12,539
and hopefully after that you'll be able to do it so we've seen some of the challenges

421
00:45:12,539 --> 00:45:19,739
with standalone LLMs those challenges include the context window being small the fact that

422
00:45:19,739 --> 00:45:26,619
it's hard to remember details within a large context window knowledge gaps you know cutoff

423
00:45:26,619 --> 00:45:30,380
dates you mentioned earlier the model might be trained up to a date and then it cannot

424
00:45:30,380 --> 00:45:36,780
follow the trends or be up to date hallucinations there are some fields think about medical

425
00:45:36,780 --> 00:45:41,659
diagnosis where hallucination are very costly you can't afford a hallucination you know even

426
00:45:41,659 --> 00:45:48,139
in education imagine deploying a model for the us youth education and it hallucinates and it

427
00:45:48,139 --> 00:45:53,900
teaches millions of people something completely wrong it's a problem and then lack of sources

428
00:45:54,860 --> 00:46:02,460
a lot of fields love sources research fields love sources education love sources legal

429
00:46:02,460 --> 00:46:09,500
loves sources as well and so the pre-trained LLM doesn't do a good job to source and in fact

430
00:46:09,500 --> 00:46:15,260
if you if you have tried to find sources on a plain LLM it actually hallucinates a lot it

431
00:46:15,260 --> 00:46:21,179
makes up research papers it just lists like completely fake stuff so how do we solve that

432
00:46:22,139 --> 00:46:29,179
with a rag rag integrates with external knowledge sources databases documents

433
00:46:29,179 --> 00:46:37,019
apis it ensures that answers are more accurate up to date and grounded because you can actually

434
00:46:37,019 --> 00:46:42,300
update your document your drive is always up to date i mean ideally you're always pushing

435
00:46:42,300 --> 00:46:48,059
new documents to it and when you query what is our queue for performance in sales hopefully

436
00:46:48,139 --> 00:46:53,179
there is the last board deck in the drive and it can read the last board deck you know

437
00:46:54,619 --> 00:47:01,019
and more developer control we'll see why rags allow for targeted customization without actually

438
00:47:01,019 --> 00:47:05,179
requiring the retraining of the model in fact you don't touch the model with rags

439
00:47:05,179 --> 00:47:11,500
it's really a technique that is put on top of the model so to see an example of a rag

440
00:47:11,500 --> 00:47:19,739
this is a question answering application where we're in the medical field and a user is asking

441
00:47:20,860 --> 00:47:28,699
a query what are the side effects of drug x is an important question you can't hallucinate you

442
00:47:28,699 --> 00:47:35,739
need to source you need to be up to date maybe there is a new update to that drug that is now

443
00:47:35,739 --> 00:47:39,980
in the database and you need to read that so you have to like a rag is a great example of

444
00:47:39,980 --> 00:47:45,019
what you would want to use here the way it works is you have your knowledge base of a bunch of

445
00:47:45,019 --> 00:47:52,780
documents what you do is you use an embedding to embed those documents into lower dimensional

446
00:47:52,780 --> 00:48:01,099
representations so for example if the document is a pdf a long pdf you might you know read

447
00:48:01,099 --> 00:48:06,380
the pdf understand it and then embed it we've seen plenty of embedding approaches together

448
00:48:06,380 --> 00:48:13,019
triplet loss etc you remember so imagine one of them here for lms is embedding those documents

449
00:48:13,019 --> 00:48:20,059
into lower representation if the representation is too small you will lose information if it's

450
00:48:20,059 --> 00:48:28,300
too big you will add latency right so trade-off you will store typically those representation

451
00:48:28,300 --> 00:48:33,980
into a database called a vector database there's a lot of vector database providers

452
00:48:34,619 --> 00:48:42,699
out there you know i think i've listed a couple that are very common no i haven't listed but i

453
00:48:42,699 --> 00:48:47,340
can i can share afterwards the vector database is essentially storing those vector in a very

454
00:48:47,340 --> 00:48:54,059
efficient manner allowing the fast retrieval with a certain distance metric so what you

455
00:48:54,059 --> 00:49:02,059
do is you also embed usually with the same algorithm the user prompts and you run a retrieval

456
00:49:02,059 --> 00:49:09,420
process which is essentially saying based on the embedding from the user query and the vector

457
00:49:09,420 --> 00:49:14,460
database find the relevant documents based on the distance between those embeddings

458
00:49:15,420 --> 00:49:21,019
once you found the relevant documents you pull them and then you add them to the user query

459
00:49:21,019 --> 00:49:28,460
with a system prompt or a prompt template on top so the prompt template can be answer user

460
00:49:28,460 --> 00:49:37,179
query based on list of documents if answer not in the document say i don't know that's your

461
00:49:37,179 --> 00:49:43,179
prompt templates where the user query is pasted the documents are pasted and then your output

462
00:49:44,059 --> 00:49:48,699
should be what you want because it's now grounded in the documents you can also add

463
00:49:48,699 --> 00:49:54,059
to these prompt templates tell me the exact page chapter line of the document that was

464
00:49:54,059 --> 00:50:04,440
relevant and in fact link it as well just to be more precise any question on rags is a simple

465
00:50:05,000 --> 00:50:19,710
vanilla rag yeah yes question is do the document embeddings still retain information of the

466
00:50:20,269 --> 00:50:23,710
location of the information within that document especially in big documents

467
00:50:24,750 --> 00:50:29,550
great question we get to it in the in a second because you're right that the vanilla

468
00:50:29,550 --> 00:50:34,750
rag might not do a good job with very large documents so let's say you know when you open

469
00:50:34,750 --> 00:50:41,469
a medication box and you have this gigantic white paper with all the information and it's

470
00:50:42,190 --> 00:50:47,949
very long maybe a vanilla rag would not cut it so what people have figured out is a bunch

471
00:50:47,949 --> 00:50:52,590
of techniques to improve rags and in fact chunking is a great technique that is very

472
00:50:52,590 --> 00:50:56,670
popular so you might actually store in the vector database the embedding of the full

473
00:50:56,670 --> 00:51:03,150
documents and on top of that you will also store a chapter level vector you know and when

474
00:51:03,150 --> 00:51:07,389
you retrieve you retrieve the document you retrieve the chapter and that allows you to be

475
00:51:07,389 --> 00:51:13,949
more precise with the sourcing it's one example another technique that's popular is hide

476
00:51:16,110 --> 00:51:24,110
hypothetical document embeddings where a group of researchers published a paper showing that

477
00:51:24,829 --> 00:51:30,670
when you get your user query one of the main problem is the user query actually does not look

478
00:51:30,670 --> 00:51:36,510
like your documents for example the user query might be what are the side effects of drug x

479
00:51:36,510 --> 00:51:41,789
when actually in the document in the vector database the vectors will present very long

480
00:51:41,789 --> 00:51:46,190
documents so how do you guarantee that the vector embedding is going to be close to the

481
00:51:46,190 --> 00:51:52,110
document embedding what they do is they use the user query to generate a fake hallucinated

482
00:51:52,110 --> 00:52:00,429
document they embed that document and then they compare to the vector in the vector database

483
00:52:01,309 --> 00:52:07,150
that makes sense so for example the user says what is the side effect of drug x there's a

484
00:52:07,150 --> 00:52:11,949
prompt that this is given to another prompt that says based on this user query generates

485
00:52:11,949 --> 00:52:20,190
a five page report answering the user query it generates potentially a completely fake answer

486
00:52:20,909 --> 00:52:25,869
you embed that and it will be closer to the document that you're looking for likely

487
00:52:28,800 --> 00:52:34,559
it's one example of a of a rag approach again the purpose of this lecture is not to go through

488
00:52:34,559 --> 00:52:39,199
all this tree and explain you every single method that has been discovered for rags but

489
00:52:39,199 --> 00:52:44,639
i just wanted to show you how much research has been done between 2020 and 2025 in rags

490
00:52:44,639 --> 00:52:51,440
and how many branches of research you you now have that you can learn from the survey paper

491
00:52:51,440 --> 00:53:07,260
is linked in the slides by the way and i'll share them after the lecture super so we've

492
00:53:07,260 --> 00:53:12,300
made some progress hopefully now you feel like if you were to start an LLM application

493
00:53:12,860 --> 00:53:16,619
you know how to do better prompts you know how to do chains you know how to do fine tuning

494
00:53:17,179 --> 00:53:21,420
you also know how to do retrieval and you have the baggage of techniques that you can go

495
00:53:21,420 --> 00:53:26,539
and read and find the code base pull the code vibe code it but you have the breath now

496
00:53:30,219 --> 00:53:36,539
the next set of topics we're going to see is around the question of how could

497
00:53:36,539 --> 00:53:41,500
we extend the capabilities of LLMs from performing single tasks enhanced with external

498
00:53:41,500 --> 00:53:49,340
knowledge to handling multi-step autonomous workflows and this is where we get into proper

499
00:53:49,340 --> 00:53:59,000
agent ii so let's talk about agent ii workflows towards autonomous and specialized systems

500
00:54:00,039 --> 00:54:04,840
then we'll talk about evals then we'll see multi-agent systems and we'll end with the

501
00:54:05,960 --> 00:54:14,559
with a little thoughts on what's next in ai so and rewring actually

502
00:54:14,960 --> 00:54:23,599
um coined the term agent ii workflows and his reason was that a lot of companies use

503
00:54:23,599 --> 00:54:30,639
uh say agents agents agents everywhere agents everywhere if you go and work at these companies

504
00:54:30,639 --> 00:54:34,480
you will notice that they mean very different things by agent some people actually have a

505
00:54:34,480 --> 00:54:41,039
prompt and they call it an agent you know other people they have a very complex multi-agent

506
00:54:41,039 --> 00:54:45,440
system they call it an agent and so calling everything an agent doesn't do it justice

507
00:54:45,440 --> 00:54:52,639
so andrew says let's call it agent workflows because in practice it's a bunch of prompts

508
00:54:52,639 --> 00:54:59,360
with tools with additional resources api calls that ultimately are put in a workflow

509
00:54:59,360 --> 00:55:05,599
and you can call that workflow agent ii so it's all about the multi-step process

510
00:55:06,559 --> 00:55:14,699
um to complete the task also calling it agent ii workflow allows us to not mix it up with what

511
00:55:14,699 --> 00:55:20,460
i called agent the line the last lecture with reinforcement learning because in rl agent has

512
00:55:20,460 --> 00:55:25,099
a very specific definition interacts with an environment passes from one state to the other

513
00:55:25,099 --> 00:55:33,690
has a reward and an observation you remember that chart right so um here's an example of

514
00:55:33,849 --> 00:55:41,369
how we move from a one-step bomb to a multi-step agent workflow let's say a user queries a

515
00:55:44,329 --> 00:55:51,530
product what is your refund policy on a chat bot and the response using a rag says refunds

516
00:55:51,530 --> 00:55:55,610
are available within 30 days of purchase and maybe the rag can even look linked to the

517
00:55:55,610 --> 00:56:03,210
policy documents that's what we learned so far instead an agent workflow can function like

518
00:56:03,210 --> 00:56:11,210
this the user says can i get a refund for my order and the response via the agent workflow

519
00:56:11,210 --> 00:56:15,849
is the agent retrieves the refund policy using a rag the agent then follows up with

520
00:56:15,849 --> 00:56:21,769
the users and says can you provide your order number then the agent queries an api to check

521
00:56:21,769 --> 00:56:27,369
the order details and finally it comes back to the user and confirms your order qualifies for

522
00:56:27,369 --> 00:56:32,489
a refund the amount will be processed in three to five business days this is much more thoughtful

523
00:56:32,489 --> 00:56:39,449
than the first version which is sort of vanilla right so that's what we're going to talk about in

524
00:56:39,449 --> 00:56:42,969
the next a couple of slides is how do we get from the first one to the second one

525
00:56:46,570 --> 00:56:51,530
there are plenty of specialized agent workflows online you know you've heard and if you hang

526
00:56:51,530 --> 00:56:57,210
out in sf you probably see a bunch of billboards you know ai software engineer ai skills

527
00:56:57,289 --> 00:57:04,409
mentor you've interacted within the class to work here ai sdr ai lawyers ai you know

528
00:57:05,530 --> 00:57:10,730
specialized cloud engineer you know it would be a stretch to say that everything works but

529
00:57:10,730 --> 00:57:19,530
there's work being done towards that you know i'm not personally a fan of putting a

530
00:57:19,530 --> 00:57:23,849
face behind those things i think it's gimmicky and i think in a few years from now actually

531
00:57:23,849 --> 00:57:29,769
very few products will have a human face behind it but might be a marketing tactic

532
00:57:29,769 --> 00:57:37,050
from some startups it's more scary than it is engaging frankly um okay i want to talk about

533
00:57:37,050 --> 00:57:42,170
the pirating shift uh that's especially useful let's say you're a software engineer or you're

534
00:57:42,170 --> 00:57:46,489
planning to be a software engineer because software engineering as a discipline is sort

535
00:57:46,489 --> 00:57:51,769
of shifting or at least the best engineers i've worked with are able to move from a

536
00:57:51,769 --> 00:57:57,849
deterministic mindset to a fuzzy mindset and balance between the two whenever they need to

537
00:57:57,849 --> 00:58:03,289
get something done so here's the paradigm shift between traditional software and agent tki

538
00:58:03,289 --> 00:58:09,690
software the first one is the way you handle data traditional software deals with structured

539
00:58:09,690 --> 00:58:16,329
data you have json's you have databases they're pasted in a very structured manner in a data

540
00:58:16,329 --> 00:58:21,449
engineering pipeline and then they're used to be displayed on a certain interface the user

541
00:58:21,449 --> 00:58:26,010
might fill a form that is then retrieved and pasted in the database all of that

542
00:58:26,010 --> 00:58:32,170
historically has been structured data now more and more companies are handling freeform text

543
00:58:32,730 --> 00:58:39,849
images and all of that requires dynamic interpretation to transform an input into

544
00:58:39,849 --> 00:58:46,489
an output the software itself used to be deterministic now you have a lot of software

545
00:58:46,489 --> 00:58:53,050
that is fuzzy and fuzzy software creates so many issues i mean imagine if you let your

546
00:58:53,050 --> 00:58:59,130
user ask anything on your website the chances that it breaks is tremendous the chances that

547
00:58:59,130 --> 00:59:03,929
you're attacked is tremendous the chances it's really really complicated it's more complicated

548
00:59:03,929 --> 00:59:11,369
than people make it seem on twitter fuzzy engineering is truly hard you know you might

549
00:59:11,369 --> 00:59:15,769
get hate as a company because one user did something that you authorize them to do that

550
00:59:15,769 --> 00:59:19,610
ended up breaking the database and ended up you know we've seen that with many companies

551
00:59:19,610 --> 00:59:24,170
in the last couple of years so it takes a very specialized engineering mindset to do

552
00:59:24,170 --> 00:59:30,489
fuzzy engineering but also know when you need to be deterministic the other thing i call is

553
00:59:31,210 --> 00:59:38,969
with agent AI software you sort of want to think about your software as like your manager

554
00:59:38,969 --> 00:59:45,769
so you're familiar with the monolith or or you know microservices approaches in software

555
00:59:45,769 --> 00:59:51,050
you know where you structure your software in different you know boxes that can talk to each

556
00:59:51,050 --> 00:59:57,050
other and it allows teams to debug one section at a time you know now the equivalent with

557
00:59:57,050 --> 01:00:02,409
agent AI is you think as a manager so you think okay if i was to delegate my products

558
01:00:02,409 --> 01:00:07,289
to be done by a group of humans what would be those roles would i have a graphic designer

559
01:00:07,289 --> 01:00:11,769
that then you know puts together a chart and then sends it to a marketing manager that

560
01:00:11,849 --> 01:00:16,570
converts it into a nice blog post that then gives it to the performance marketing expert

561
01:00:16,570 --> 01:00:21,449
that then publishes the work the blog post and then optimizes an ab test then to a data

562
01:00:21,449 --> 01:00:27,210
scientist that analyzes the data and then puts hypotheses and validates them or invalidates them

563
01:00:27,210 --> 01:00:31,050
that's how you typically think if you're building an agent AI software

564
01:00:32,570 --> 01:00:36,570
when actually the equivalent of that in traditional software might be completely

565
01:00:36,570 --> 01:00:42,570
different it might be we have a data engineer box right here that handles all our data engineering

566
01:00:42,570 --> 01:00:48,170
and then here we have the UI UX stuff everything UI UX related goes here and you know

567
01:00:48,809 --> 01:00:52,570
companies might structure it in very different ways and here is the business logic that we

568
01:00:52,570 --> 01:00:59,440
want to care about and there's five engineers working on the business logic let's say okay

569
01:01:01,119 --> 01:01:06,400
testing and debugging is also very different and we'll talk about it in the next section

570
01:01:09,199 --> 01:01:17,280
uh the other thing that uh i feel matters is with AI in engineering the cost of experimentation

571
01:01:17,280 --> 01:01:23,519
is going down drastically and so people i feel should be more comfortable throwing away codes

572
01:01:23,519 --> 01:01:28,960
you know it's it's like in traditional software engineering you probably don't throw away code

573
01:01:28,960 --> 01:01:33,440
at on you you build a code and it's solid and it's bulletproof and then you you update

574
01:01:33,440 --> 01:01:39,280
it over time when we've seen AI companies be more comfortable throwing away codes

575
01:01:40,320 --> 01:01:45,920
which has advantages in terms of the speed at which you move but also disadvantages in

576
01:01:45,920 --> 01:01:54,170
terms of the quality of your software that can break more okay so anyway just wanted to do

577
01:01:54,170 --> 01:02:01,519
an apartheid on the the the paradigm shift from deterministic to fuzzy engineering um

578
01:02:04,400 --> 01:02:10,320
oh and actually i can give you an example from uh from work here that we learned uh probably

579
01:02:10,320 --> 01:02:14,719
over the last 12 months is like if you if you've used work here you might have seen that the

580
01:02:14,719 --> 01:02:20,559
interface has um asks you sometimes multiple choice questions and sometimes it asks you

581
01:02:20,559 --> 01:02:25,280
multiple select and sometimes it asks you drag and drop ordering matching whatever right

582
01:02:25,280 --> 01:02:30,480
those are examples of deterministic item types meaning you answer the question on a

583
01:02:30,480 --> 01:02:35,920
multiple choice there's one correct answer it's fully deterministic on the other hand you sometimes

584
01:02:35,920 --> 01:02:42,000
have voice questions where you go to a role play or you have voice plus coding questions

585
01:02:42,000 --> 01:02:48,719
where your code is being read by the interface or whatever those are fuzzy meaning the scoring

586
01:02:48,719 --> 01:02:54,800
algorithm might actually make mistakes and those mistakes might be costly and so companies

587
01:02:54,800 --> 01:02:59,760
have to figure out a human in the loop system which you might have seen the appeal

588
01:02:59,760 --> 01:03:02,880
feature at the end so at the end of the assessment you have an appeal feature where

589
01:03:02,880 --> 01:03:07,920
it allows you to say i want to appeal the agent because i want to challenge what the

590
01:03:07,920 --> 01:03:12,320
agent said on my answer because i thought that was better than what the agent thought

591
01:03:12,320 --> 01:03:15,920
and then you bring your human in the loop that then can fix the agent can tell the agent

592
01:03:15,920 --> 01:03:21,920
actually you were too harsh on the answer of this person and you know that's an example of

593
01:03:21,920 --> 01:03:28,239
a fuzzy engineered system that then adds a human in the loop to make it more aligned and

594
01:03:28,239 --> 01:03:32,800
so if you're building a company i would encourage you to think about what can i get done with

595
01:03:32,800 --> 01:03:38,400
determinism and let's get that done and then the fuzzy stuff i want to do fuzzy because it

596
01:03:38,400 --> 01:03:43,599
allows more interaction it allows more back and forth but i need to put guardrails around it

597
01:03:43,599 --> 01:03:50,159
and how am i going to design those guardrails pretty much all right here's another example

598
01:03:50,960 --> 01:03:58,559
from enterprise workflows which are likely to change due to agent TKi this is a paper from

599
01:03:58,559 --> 01:04:04,480
McKinsey i believe from last year where they looked at a financial institution and they said

600
01:04:04,480 --> 01:04:09,360
that you know we observed that they often spend one to four weeks to create a credit risk

601
01:04:09,360 --> 01:04:17,199
memo and here's the process a relationship manager gathers data from 15 and more than

602
01:04:17,199 --> 01:04:24,480
15 sources on the borrower loan type other factors then the relationship manager and the

603
01:04:24,480 --> 01:04:29,840
credit analyst collaboratively analyze that data from these sources then the credit analyst

604
01:04:29,840 --> 01:04:36,239
typically spends you know 20 more 20 hours or more writing a memo and then goes back to the

605
01:04:36,239 --> 01:04:40,960
relationship manager they give feedback and then they go through this loop again and again

606
01:04:41,519 --> 01:04:49,360
and it takes a long time to get a credit memo out and then then run a research study where

607
01:04:49,360 --> 01:04:55,440
they change the process they said gene AI agents could actually cut time by 20 to 60

608
01:04:55,440 --> 01:05:01,519
percent on credit risk memos and the process has changed to the relationship manager directly

609
01:05:01,519 --> 01:05:06,400
work with the gene AI agent system provides relevant materials that needs to produce the

610
01:05:06,400 --> 01:05:12,239
memo the agent subdivises the project into tasks that are assigned to specialist agents

611
01:05:12,239 --> 01:05:17,840
gathers and analyzes the data from multiple sources drafts a memo then the relationship

612
01:05:17,840 --> 01:05:22,320
manager and the credit analyst sit down together review the memo give feedback to the agent

613
01:05:22,320 --> 01:05:28,400
and within you know 20 to 60 percent less time are done and so this is an example where

614
01:05:28,400 --> 01:05:32,639
you're actually not changing the human stakeholders you're just changing the process

615
01:05:33,119 --> 01:05:39,440
and adding gene AI to reduce the time it takes to get a credit memo out it turns out that's

616
01:05:41,039 --> 01:05:46,719
imagine you're an enterprise and you have you know 100,000 employees and there's a lot of

617
01:05:46,719 --> 01:05:53,280
enterprises with 100,000 employees out there you are currently under crisis in terms of redesigning

618
01:05:53,280 --> 01:05:58,639
your workflows you you are you know it turns out that if you actually pull the job descriptions

619
01:05:58,639 --> 01:06:04,400
from the HR system and you interpret them you also pull the business process workflows that

620
01:06:04,400 --> 01:06:11,920
you have encoded in your drive you actually can find gains in multiple places and in the next

621
01:06:11,920 --> 01:06:17,599
few years you're probably going to see workflows being more optimized to add gene AI even if that

622
01:06:17,599 --> 01:06:22,719
happens the hardest part is changing people what we know this is this is great in theory

623
01:06:22,719 --> 01:06:29,360
but now let's try to fit that second workflow for 10,000 credits risk analysts

624
01:06:29,360 --> 01:06:36,000
and relationship managers my guess is it will take years it will take 10 20 years to get to

625
01:06:36,000 --> 01:06:42,320
this being actually done at scale within an organization because change is so hard you know

626
01:06:42,320 --> 01:06:49,360
so hard to rewire business workflows job descriptions incentivize people to do different

627
01:06:49,360 --> 01:06:55,039
and be different and train them and so so you know this is what the world is going towards

628
01:06:55,039 --> 01:07:02,079
but it's going to take a long time I think okay then I want to talk about how the agent

629
01:07:02,079 --> 01:07:09,840
actually works and what are the core components of an agent imagine a travel booking AI agent

630
01:07:09,840 --> 01:07:14,159
that's an easy example you've all thought about I still haven't been able to get

631
01:07:14,159 --> 01:07:18,719
an agent to book a trip for me or or I was scared because it was going to book a

632
01:07:18,719 --> 01:07:25,360
very expensive or long trip but in theory you can you can have a travel booking agent that

633
01:07:25,360 --> 01:07:29,840
has prompts so the prompts we've seen we know the methods to optimize those prompts

634
01:07:30,480 --> 01:07:35,679
that travel agent also has a content management context management system which is essentially

635
01:07:35,679 --> 01:07:41,119
the memory of what it knows about the user that context management system might include

636
01:07:41,119 --> 01:07:47,840
a core memory or working memory and an archival memory okay what the difference is

637
01:07:48,639 --> 01:07:56,480
within memory is not every memory needs to be fast to access like think about it you

638
01:07:56,480 --> 01:08:01,440
you're more than on a product and the first question is hi what's your name and I say

639
01:08:01,440 --> 01:08:06,320
my name is Keon that's probably going to sit in the working memory because the agent every

640
01:08:06,320 --> 01:08:10,000
time he's going to talk to me he's going to want to use my name right but then maybe the

641
01:08:10,000 --> 01:08:14,800
second question is what's your birthday and I give it my birthday does it need my birthday

642
01:08:14,800 --> 01:08:19,439
every day probably not so it's probably going to park it on the long-term memory or the

643
01:08:19,439 --> 01:08:26,720
archival memory and those memories are slower to access they're farther down the stack

644
01:08:26,720 --> 01:08:31,279
and you know that structure allows agent to determine what's the working memory and what's

645
01:08:31,279 --> 01:08:36,000
a long-term memory you know and that makes it easier for the agent to retrieve super fast

646
01:08:36,000 --> 01:08:40,960
because think about it when you interact with gpt you feel that it's very personal at times

647
01:08:40,960 --> 01:08:46,560
right you feel like it understands you imagine every time you call it it has to read the

648
01:08:46,560 --> 01:08:52,079
memories right and that can be costly it's like a very it's a very burdensome cost because it

649
01:08:52,079 --> 01:08:57,680
happens every time you talk to it so you want to be highly optimized with the working memory

650
01:08:58,640 --> 01:09:02,319
you know if it takes three seconds to look in the memory every time you're going to talk to

651
01:09:02,319 --> 01:09:07,520
your LLM it's going to take three seconds which you don't want so anyway and then you have the

652
01:09:07,520 --> 01:09:14,239
tools the tools can include APIs like a flight search API hotel booking API car rental API

653
01:09:14,239 --> 01:09:21,680
weather API and then the payment processing API and typically you would want to tell your agent

654
01:09:21,680 --> 01:09:28,640
how that API works it turns out that agents or LLMs I should say are very good at reading API

655
01:09:28,640 --> 01:09:33,920
documentation so you're giving the API documentation and it reads the json and it reads what does it

656
01:09:33,920 --> 01:09:39,119
get requests look like and this is the format that I need to push and then it pushes it in

657
01:09:39,119 --> 01:09:46,329
that format let's say and then it retrieves something does that make sense those different

658
01:09:46,329 --> 01:09:54,170
components you know entropic also talks about resources resources is data that is sitting

659
01:09:54,170 --> 01:09:59,529
somewhere that you might let your agent read for example if you're building your startups

660
01:09:59,529 --> 01:10:05,689
you have a CRM a CRM has data in it and you want to use lookups in that data you will probably

661
01:10:05,689 --> 01:10:11,609
give a lookup tool and you will give access to the resource and it will do lookups whenever

662
01:10:11,609 --> 01:10:21,020
you want super fast this type of architecture can be built with different degrees of autonomy

663
01:10:21,020 --> 01:10:24,859
from the least autonomous to the most autonomous and I'll give you a few examples

664
01:10:25,340 --> 01:10:31,180
uh less autonomous would be you've hardcoded the steps so let's say I tell the travel agent

665
01:10:32,619 --> 01:10:40,699
first identify the intent then look up in the database the history of this customer with us

666
01:10:40,699 --> 01:10:46,220
and their preferences then go to the flight API blah blah blah then go to the that I would

667
01:10:46,220 --> 01:10:52,859
hardcode the steps okay that's the least autonomous the semi-autonomous is I might

668
01:10:52,859 --> 01:10:58,300
hardcode the tools but I'm not going to hardcode the steps so I'm going to tell the agent

669
01:10:59,340 --> 01:11:09,100
you're act like a travel agent and um and uh you your task is to help the person book

670
01:11:09,100 --> 01:11:13,899
a travel and these are the tools that you have accessible to yourself and so I'm not

671
01:11:13,899 --> 01:11:17,420
hardcoding the steps I'm just hardcoding the tools that you have access to yourself

672
01:11:17,420 --> 01:11:24,779
for yourself the more autonomous is the agent decides the steps and can create the tools so

673
01:11:24,779 --> 01:11:29,659
that's where you might give actually access to a code editor to the agent and the agent might

674
01:11:29,659 --> 01:11:35,899
actually be able to ping any API in the web perform some web search it might even be able

675
01:11:35,899 --> 01:11:41,260
to create some code to display data to the user it might even be able to perform some

676
01:11:41,260 --> 01:11:46,699
calculations like oh I'm going to calculate the fastest route to get from San Francisco to New

677
01:11:46,699 --> 01:11:52,859
York and which one might be the most appropriate for what the user is looking for and then I want

678
01:11:52,859 --> 01:11:57,100
to calculate the distance between the airport and that hotel versus that hotel and I'm going

679
01:11:57,100 --> 01:12:05,340
to write code to do that so it's actually fully autonomous from that perspective okay

680
01:12:06,859 --> 01:12:16,619
so yeah remember those keywords memory prompts tools etc now I presented the flight

681
01:12:16,699 --> 01:12:21,819
API but it does not have to be an API you probably have heard the term mcp

682
01:12:21,819 --> 01:12:27,340
or model context protocol that was coined by entropic I pasted the seminal article

683
01:12:27,340 --> 01:12:32,220
on mcp at the bottom of this slide but let me explain in a nutshell why those things would

684
01:12:32,220 --> 01:12:43,420
differ in the API case you would actually teach your LLM to ping an API so you would say this

685
01:12:43,420 --> 01:12:49,579
is how you ping this API and this is the data that it will send you back and you would have

686
01:12:49,579 --> 01:12:54,779
to do that in a one-off manner so you would have to build or sort of give the API documentation

687
01:12:54,779 --> 01:13:01,979
of your flight API your booking hotel API your car rental API and then you would give

688
01:13:01,979 --> 01:13:10,060
tools for your model to communicate with those APIs it doesn't scale very well you know versus

689
01:13:10,060 --> 01:13:19,899
mcp mcp it's really about you know putting a system in the middle sort of that would make

690
01:13:19,899 --> 01:13:26,619
it simpler for your LLM to communicate with that end point so for instance you might

691
01:13:26,619 --> 01:13:31,020
you know have an mcp server and mcp client where you're trying to communicate with that

692
01:13:31,020 --> 01:13:38,300
travel database or the flight API or mcp and your agent might actually just communicate with

693
01:13:38,300 --> 01:13:44,619
it and say hey what do you need in order to give me more flight information and that that agent

694
01:13:44,619 --> 01:13:49,340
will respond by I would like you to tell me where is the origin flight where is the destination

695
01:13:49,340 --> 01:13:53,420
and what you're looking for at a high level this is my requirement okay let me get back to

696
01:13:53,420 --> 01:13:58,939
you with my requirement oh you forgot to tell me your budget whatever oh let me give you my

697
01:13:58,939 --> 01:14:06,779
budget etc and it's it's it's agent to agent communication which allows more scalability you

698
01:14:06,779 --> 01:14:12,779
don't need to hardcode everything companies have displayed their mcps out there and you can

699
01:14:12,779 --> 01:14:17,180
your agent can communicate with them and figure out how to get the data it needs does that make

700
01:14:17,180 --> 01:14:41,079
sense yeah oh sorry yes I think it is ultimately the question is isn't it shifting the issue

701
01:14:41,079 --> 01:14:45,720
because anyway if an API has to be updated the mcp has to be updated is what you say right

702
01:14:45,720 --> 01:14:51,560
yes that's correct but at least it allows the agent to sort of go back and forth and figure out

703
01:14:51,560 --> 01:14:56,520
what the requirements are but at the end of the day ideally if you're a startup you have

704
01:14:56,520 --> 01:15:01,319
some documentation and automatically you have an agent or an LLM workflow that reads that

705
01:15:01,319 --> 01:15:06,199
documentation and updates the code accordingly you know but I agree it's not it's not something

706
01:15:06,199 --> 01:15:16,109
that is fully autonomous yeah which should security specifically

707
01:15:18,989 --> 01:15:27,229
yeah so are there security issues with mcps so think about it this way mcps depending on

708
01:15:27,229 --> 01:15:31,310
the data that you get access to might have different requirements lower stake or higher

709
01:15:31,310 --> 01:15:35,710
stake I'm not an expert that you know the full range but it wouldn't surprise me that

710
01:15:36,350 --> 01:15:44,590
you know when you expose an mcp to an I think you would a lot of mcps have

711
01:15:44,590 --> 01:15:48,829
authentication so you know you might actually need a code to actually talk to it just like

712
01:15:48,829 --> 01:15:54,989
you would with an API or a key yeah but that's a good question I'm you know I'm not an expert

713
01:15:54,989 --> 01:16:04,079
at the security of these systems but you know we can look into any other questions on

714
01:16:04,079 --> 01:16:11,279
what we've seen with the agentic workflows APIs tools mcps memory all of that is under

715
01:16:11,279 --> 01:16:15,199
progress so even memory is not a solved problem by any mean it's pretty hard actually

716
01:16:16,479 --> 01:16:46,159
yes exactly exactly yeah is mcp about efficiency or accessing more data it's about efficiency

717
01:16:46,800 --> 01:16:53,600
it's like you know let's say you have a coding agent and you know it has an mcp clients and

718
01:16:53,600 --> 01:16:59,199
there's multiple mcp servers that are exposed out there that agent can communicate very

719
01:16:59,199 --> 01:17:05,680
efficiently with them and find what it needs and it's a more efficient process than actually

720
01:17:06,640 --> 01:17:11,279
in displaying APIs and the APIs on that side and how to ping them and what the protocol

721
01:17:12,079 --> 01:17:15,279
but you know it's not about the data that is being exposed because ultimately you control

722
01:17:15,279 --> 01:17:22,479
the data is being exposed you probably you know depending on how the mcp is built my guess is you

723
01:17:22,479 --> 01:17:31,039
probably expose yourself to other risks because your your mcp server can can see any inputs

724
01:17:31,039 --> 01:17:40,399
pretty much from another llm and so it has to be robust yeah super so let's look at an

725
01:17:40,399 --> 01:17:47,199
example of a step-by-step workflow for the travel agent so let's say the user

726
01:17:47,199 --> 01:17:57,039
says i want to plan a plan a trip to paris from december 15 to 20th with flights hotels near the

727
01:17:57,039 --> 01:18:03,920
fl tower and then an itinerary of must visit places that's the task to the travel agent

728
01:18:03,920 --> 01:18:09,119
step two the agent plans the steps so it says i'm going to find flights use the flight

729
01:18:09,119 --> 01:18:15,520
search api to get option for december 15 search hotels generate recommendation for places to visit

730
01:18:16,079 --> 01:18:24,000
validate preferences budget etc book the trip with the payment processing api step three

731
01:18:24,000 --> 01:18:29,199
that's just the planning by the way step three execute the plan use your tools combine the

732
01:18:29,199 --> 01:18:35,439
results and then proactive user interaction and booking it might make a first proposal to the

733
01:18:35,439 --> 01:18:41,279
user and ask the user to validate or invalidate and then may repeat that planning and execution

734
01:18:41,279 --> 01:18:47,600
process and then finally it might actually update the memory it might say oh i just

735
01:18:47,600 --> 01:18:53,119
learned through this interaction that the user only likes direct flights next time i'll only

736
01:18:53,119 --> 01:19:01,680
give direct flights or i notice users are fine with three star hotels or four star hotels and

737
01:19:01,680 --> 01:19:05,279
in fact they're they don't want to go above budget or something like that

738
01:19:08,000 --> 01:19:12,720
so that hopefully makes sense by now and you know how you might do that my question for you

739
01:19:12,720 --> 01:19:18,640
is how would you know if this works and if you had such a system running in production

740
01:19:19,279 --> 01:19:35,600
how would you improve it yeah so that's an example so let users rate their experience at

741
01:19:35,600 --> 01:19:41,439
the end that would be an end-to-end test right you're looking at the user experience

742
01:19:42,079 --> 01:19:47,359
through the steps and say how good was it from one to five let's say yeah it's a good way and

743
01:19:47,359 --> 01:20:00,329
then if you learn that a user says one what how do you improve the the workflow okay so you

744
01:20:00,329 --> 01:20:06,890
would go down three and say okay you said one what what was your issue and then the user

745
01:20:06,890 --> 01:20:12,250
says the prices were too high let's say and then you would go back and fix that specific

746
01:20:13,689 --> 01:20:31,420
tool or prompt or yeah okay any other ideas yeah good so that's a good insight separate the

747
01:20:31,420 --> 01:20:36,380
llm related stuff from the non-llm related stuff the deterministic stuff the deterministic

748
01:20:36,380 --> 01:20:43,899
stuff you might be able to fix it you know more objectively essentially yeah it was what else

749
01:20:56,649 --> 01:21:03,050
so give me an example of an objective issue that you can notice and how you would fix it

750
01:21:03,050 --> 01:21:19,260
versus a subjective issue yeah okay so let's say you say there's the same flight but one

751
01:21:19,260 --> 01:21:23,979
is cheaper than the other let's say it's objectively worst and so you can capture

752
01:21:23,979 --> 01:21:28,619
that almost automatically yeah so you could actually build evals that are objective

753
01:21:29,340 --> 01:21:35,819
that are tracked across your users and you might actually run an analysis after and see that

754
01:21:35,819 --> 01:21:43,500
for the objective stuff we notice that our llm ai agent again workflow is bad with pricing

755
01:21:43,500 --> 01:21:47,260
it just doesn't read price as well because it always gives a more expensive option

756
01:21:48,140 --> 01:22:00,359
yeah you're perfectly right how about the subjective stuff yeah like do you choose a

757
01:22:00,359 --> 01:22:07,550
direct or indirect flight if the indirect yeah good one do you do you choose a direct flight

758
01:22:07,550 --> 01:22:11,630
or an indirect flight if the indirect is cheaper but the direct is more comfortable

759
01:22:12,909 --> 01:22:19,149
yeah that's a good one actually so how would you capture that information let's say this

760
01:22:19,149 --> 01:22:31,119
is used by thousands of users could you feed something in yeah i mean you could you could

761
01:22:31,119 --> 01:22:37,439
uh could you feed something in about the user preferences well you could you could build a

762
01:22:37,439 --> 01:22:43,760
data set that has some of that information so you build 10 prompts where the user is asking

763
01:22:43,760 --> 01:22:49,920
specifically for direct is saying that i prefer direct flights because i care about my time

764
01:22:49,920 --> 01:22:54,800
i say and then you look at the output and you actually give a good the example of a good

765
01:22:54,800 --> 01:23:00,079
output and you probably are able to capture the performance of your

766
01:23:00,079 --> 01:23:06,399
agentic workflow on this specific eval whether does it prioritize does it understand price

767
01:23:06,399 --> 01:23:14,380
conscious is it price conscious essentially and comfort conscious yeah what about the tone let's

768
01:23:14,380 --> 01:23:22,060
say let's say the llm right now is not very friendly how would you notice that and how would

769
01:23:22,060 --> 01:23:36,619
you fix it yeah okay have a test user run the prompt and see if there's something wrong with

770
01:23:36,619 --> 01:23:40,859
that tell me about the last step how would you notice that something is wrong so

771
01:23:51,560 --> 01:23:56,359
yeah i agree with your approach have llm judges that evaluate the response against a certain

772
01:23:56,359 --> 01:24:00,520
rubric of what politeness looks like so here in this case you could actually start

773
01:24:01,399 --> 01:24:06,520
with error analysis so you start you have a thousand users and you know you can pull up

774
01:24:06,520 --> 01:24:12,840
20 user interaction and read through it and you might notice at first sight the llm seems to be

775
01:24:13,399 --> 01:24:19,399
very rude you know it's just super super short in its answers and it's not very helpful

776
01:24:20,359 --> 01:24:24,680
you notice that with your error analysis manually then you go to the next stage you

777
01:24:24,680 --> 01:24:32,600
actually put evals behind it you say i'm going to create a set of a set of lm judges

778
01:24:33,239 --> 01:24:36,680
that are going to look at the user interaction and are going to rate

779
01:24:37,319 --> 01:24:41,880
how polite it is and i'm going to give it a rubric then what i'm going to do is i'm going

780
01:24:41,880 --> 01:24:46,920
to flip my llm instead of using gpt4 i'm going to use grok and instead of using rock i'm going

781
01:24:46,920 --> 01:24:52,840
to use llama and then i'm going to run those three llm side by side give it to my llm judges

782
01:24:53,560 --> 01:25:02,359
and then get my subjective score at the end to say oh x model was more polite on average

783
01:25:02,760 --> 01:25:06,920
perfectly right that's an example of an eval that is very specific and allows you to choose between

784
01:25:06,920 --> 01:25:12,840
elements you could actually do the same eval not across llms but fix the llm change the prompts

785
01:25:12,840 --> 01:25:17,239
you actually instead of saying act like a travel agent you say act like a helpful travel

786
01:25:17,239 --> 01:25:22,359
agent and then you see the influence of that word on your eval with the llm as judges does

787
01:25:22,359 --> 01:25:29,560
that make sense okay super so let's let's move forward and do a case study with evals and

788
01:25:29,560 --> 01:25:37,479
then we're we're almost done for today let's say your product managers manager asks you to build

789
01:25:37,479 --> 01:25:44,439
an ai agent for customer support okay where do you start and here is an example of the user

790
01:25:44,439 --> 01:25:50,039
prompt i need to change my shipping address for order blah blah i move to a new address

791
01:25:51,720 --> 01:26:16,760
so what would you start if i'm giving you that project you know yes all right so do

792
01:26:16,760 --> 01:26:22,520
some research see benchmarks and how different models perform at customer support and then pick

793
01:26:22,520 --> 01:26:27,880
a model that's what you mean yeah you it's true you could do that what what else could you do

794
01:26:27,880 --> 01:26:39,789
yeah okay yeah i like that try to decompose the different tasks that it will need

795
01:26:39,789 --> 01:26:43,710
and try to guess which ones will be more of a struggle which ones should be fuzzy which

796
01:26:43,789 --> 01:26:59,279
should be deterministic yeah you're right yeah similar to what you said that's what i would

797
01:26:59,279 --> 01:27:03,600
recommend as well you say i would sit down with a customer support agent for a day or two

798
01:27:03,600 --> 01:27:07,439
and i would decompose the tasks they're going through i will ask them where do they struggle

799
01:27:07,439 --> 01:27:12,640
how much time it takes yes that's usually where you want to start with task decomposition

800
01:27:12,640 --> 01:27:18,079
so let's say we've done that work and we have this list i'm simplifying but the customer

801
01:27:18,079 --> 01:27:24,239
support agent human typically would extract info then look up in the database to retrieve

802
01:27:24,239 --> 01:27:30,079
the customer record then check the policy you know are we allowed to update the address or

803
01:27:30,079 --> 01:27:35,600
is it a fixed data point and then draft a response email and send the email okay so

804
01:27:35,600 --> 01:27:44,000
we've decomposed that task once you've decomposed that task how do you design

805
01:27:44,000 --> 01:28:20,590
your agent workflow yes exactly so to repeat i'm going to you're going to look at the

806
01:28:20,590 --> 01:28:26,750
decomposition of tasks get an instinct of what's fuzzy what's deterministic and then

807
01:28:26,750 --> 01:28:34,270
determine which line is going to be an llm one shot which one will require maybe a rag

808
01:28:34,270 --> 01:28:39,149
which one will require a tool which one will require memory which one so you will start

809
01:28:39,149 --> 01:28:44,189
designing that map completely right that's also what i would recommend you you might actually

810
01:28:44,829 --> 01:28:51,869
draft it and say okay i take the user prompt and the first step of my task deposit

811
01:28:51,869 --> 01:28:58,350
decomposition was extract information that seems to be a vanilla llm you you can

812
01:28:58,350 --> 01:29:03,310
guess that the vanilla lm would probably be good enough at extracting the user wants to

813
01:29:03,310 --> 01:29:07,069
change their address and this is the order number and this is the new address you probably

814
01:29:07,069 --> 01:29:13,710
don't need too much technology there other than the llm the next step it feels like you need a

815
01:29:13,710 --> 01:29:20,109
tool because you're actually going to have to look up in the database and also update the address

816
01:29:21,310 --> 01:29:25,710
so that might be a tool and you might have to build a custom tool for the llm to say

817
01:29:25,710 --> 01:29:30,109
let me connect you to that database or let me give you access to that resource with an mcp

818
01:29:30,109 --> 01:29:36,560
you know after that you probably need an lm again to draft the email but you would

819
01:29:36,560 --> 01:29:40,479
probably paste confirmation you paste the confirmation that your address has been

820
01:29:40,479 --> 01:29:46,239
updated from x to y and then the llm will draft an answer and of course just to not forget

821
01:29:46,239 --> 01:29:53,359
you might need a tool to send the email you know you might actually need to you know post

822
01:29:53,359 --> 01:29:59,840
something to for the email to to go out and then you'll get the output does that make sense

823
01:29:59,840 --> 01:30:06,239
so exactly what you described okay now moving to the next step once we have the composer tasks

824
01:30:06,239 --> 01:30:11,520
then we have designed an agentic workflow around it it took us five minutes in practice it will

825
01:30:11,520 --> 01:30:14,880
take you more if you're building your startup on that you want to make sure your task decomposition

826
01:30:14,880 --> 01:30:19,840
is accurate your thing is accurate here and then you can have a lot of work done on every

827
01:30:19,840 --> 01:30:24,720
tool and optimize it and latency and cost but let's say and now we want to know how

828
01:30:26,079 --> 01:30:32,399
if it works you know and i'm going to assume that you have llm traces llm traces are very

829
01:30:32,399 --> 01:30:37,600
important actually if you're interviewing with an ai startup i would recommend you in the interview

830
01:30:37,600 --> 01:30:43,359
process to ask them do you have llm traces because if they don't have lm traces it is

831
01:30:43,359 --> 01:30:48,640
pretty hard to debug an lm system you know because you don't have visibility on the chain

832
01:30:48,640 --> 01:30:55,119
of complex prompts that were called and where the bug is and you know so it's a basic sort

833
01:30:55,119 --> 01:31:03,039
of part of an ai startup stack to have a llm traces so let's assume you have traces how would

834
01:31:03,039 --> 01:31:09,039
you know is your system work you know we you know i'm going to summarize some of the things

835
01:31:09,039 --> 01:31:16,239
i heard earlier you gave us an example of an end-to-end metric you look at the user

836
01:31:16,239 --> 01:31:22,079
satisfaction at the end you can also do a component-based approach where you actually

837
01:31:22,079 --> 01:31:28,880
will look at the tool the database updates and you will manually do an error analysis and see oh

838
01:31:28,880 --> 01:31:34,000
the tool actually always forgets to update the email it just fails at writing you know and i'm

839
01:31:34,000 --> 01:31:40,880
going to fix that this is deterministic pretty much or you know when it tries to send the email

840
01:31:40,880 --> 01:31:46,079
and ping the system that is supposed to send the email it doesn't send it in the right

841
01:31:46,079 --> 01:31:53,119
format and so it bugs at that point again you could fix that draft of the email dllm doesn't

842
01:31:53,119 --> 01:31:58,239
do a great job it's not very polite at drafting the email you know so you could look at component

843
01:31:58,239 --> 01:32:02,960
by component and it's actually easier to debug than to look at it end to end you probably do

844
01:32:02,960 --> 01:32:10,560
a mix of both another way to look at it is what is objective versus what is subjective so

845
01:32:10,560 --> 01:32:19,039
for example an objective example would be a dllm extracted the wrong order id you know the user

846
01:32:19,039 --> 01:32:24,399
said my order id is x and the dllm when it actually pasted looked up in the database it

847
01:32:24,399 --> 01:32:29,920
used the wrong order id this is objectively wrong you can actually write a python code that

848
01:32:29,920 --> 01:32:34,880
checks that checks just the alignment between what the user mentioned and and what was actually

849
01:32:34,880 --> 01:32:40,319
pasted in the database or for the lookup you also have subjective stuff which we talked about

850
01:32:40,319 --> 01:32:45,359
where you probably want to do either human rating or llm as judges it's very relevant for

851
01:32:46,079 --> 01:32:54,640
subjective evals and finally you will find yourself having quantitative evals and more

852
01:32:54,640 --> 01:33:00,960
qualitative evals so quantitative would be percentage of successful address updates the latency you

853
01:33:00,960 --> 01:33:05,840
could actually track the latency component based and see which one is the slowest let's

854
01:33:05,840 --> 01:33:11,119
say sending the email is five seconds you know it's too long let's say you would notice component

855
01:33:11,119 --> 01:33:16,000
base or the full workflow and then you will decide where am i optimizing my latency and

856
01:33:16,000 --> 01:33:22,159
how am i going to do that and then finally qualitative you might actually do some error

857
01:33:22,159 --> 01:33:30,319
analysis and look at you know where are the hallucinations where are the tone mismatches

858
01:33:30,880 --> 01:33:35,600
you know are the user confused and by what they're confused you know that would be more

859
01:33:35,600 --> 01:33:41,439
qualitative and typically it would take more you know white glove approaches to do that

860
01:33:42,399 --> 01:33:46,079
okay so here's what it could look like i gave you some examples but

861
01:33:46,720 --> 01:33:53,920
you would build evals to determine objectively subjectively component based end-to-end based

862
01:33:53,920 --> 01:33:58,720
and then quantitatively and qualitatively where is your llm failing and where it's doing well

863
01:34:02,680 --> 01:34:07,800
does that give you a sense of the type of stuff you could do to fix improve that agenting workflow

864
01:34:09,920 --> 01:34:14,000
super well that was our case study on evals we're not going to delve deeper into it but

865
01:34:14,640 --> 01:34:19,119
hopefully it gave you a sense of the type of stuff you can do with llm judges with

866
01:34:19,119 --> 01:34:27,359
you know objective subjective component based end-to-end etc last section on multi-agent

867
01:34:27,359 --> 01:34:35,760
workflows so you might you might ask hey why do we need a multi-agent workflow when we are

868
01:34:35,760 --> 01:34:41,439
when the workflow already has multiple steps already calls the llm multiple times already

869
01:34:41,439 --> 01:34:46,399
gives them tools why do we need multiple agents and so many people are talking about

870
01:34:46,399 --> 01:34:50,000
multi-agent system online it's not even a new thing frankly i mean multi-agent system

871
01:34:50,000 --> 01:34:55,520
i've been around for a long time the the main advantage of a multi-agent system is going

872
01:34:55,520 --> 01:35:03,920
to be parallelism it's like is there something that i wish i would run in parallel sort of

873
01:35:03,920 --> 01:35:08,319
independently but maybe there are some sinks in the middle but that's where you want to

874
01:35:08,319 --> 01:35:14,239
put a multi-agent system it's when it's parallel the other advantage that some companies

875
01:35:14,880 --> 01:35:20,319
have with multi-agent system is an agent can be reused so let's say in a company you

876
01:35:20,319 --> 01:35:25,199
have an agent that's been built for design that agent can be used in the marketing team

877
01:35:25,199 --> 01:35:30,000
and it can be used in the product team you know and so now you're optimizing an agent

878
01:35:30,000 --> 01:35:34,560
which has multiple stakeholders that can communicate with it and benefit from its

879
01:35:34,560 --> 01:35:41,840
performance actually i'm going to ask you a question and take a few maybe a minute to

880
01:35:41,840 --> 01:35:48,640
think about it let's say you were building smart home automation for your apartment or your

881
01:35:48,640 --> 01:35:55,840
home what agents would you want to build yeah write it down and then i'm going to ask you

882
01:35:55,840 --> 01:36:02,159
in in a minute to share some of the agents that you will build also think about how you

883
01:36:02,159 --> 01:36:06,319
would put a hierarchy between these agents or how you would organize them or who should

884
01:36:06,319 --> 01:36:13,420
communicate with who okay okay take a minute for that be creative also because i'm going

885
01:36:13,420 --> 01:36:17,100
to ask all of your agents and maybe you have an agent that nobody has thought of

886
01:36:21,880 --> 01:36:28,680
okay let's get started who wants to give me a a set of agents that you would want for your home

887
01:36:28,680 --> 01:37:06,170
smart home yes okay so let me repeat there are four agents i think roughly one that tracks

888
01:37:06,170 --> 01:37:11,529
biometric like your where are you in the home where are you moving how you're moving things

889
01:37:11,529 --> 01:37:18,970
like that that sort of knows your location the second one determines the temperature

890
01:37:20,170 --> 01:37:26,729
of the rooms and has the ability to change it the third one tracks energy efficiency

891
01:37:26,729 --> 01:37:32,810
and might be feedback on energy and energy usage it might be i don't know maybe it has

892
01:37:32,810 --> 01:37:38,649
the control over the temperature as well i don't know actually or the gas or the water

893
01:37:39,369 --> 01:37:45,050
um might cut your water at some point the and then you have an orchestrator agent what is

894
01:37:45,050 --> 01:37:57,260
exactly the orchestrator doing okay passes instructions so is that the agent that

895
01:37:57,260 --> 01:38:02,619
communicates mainly with the user okay so if i have i'm coming back home and i'm saying

896
01:38:02,619 --> 01:38:08,779
i want the oven to be preheated i communicate with the orchestrator and then it would funnel

897
01:38:08,859 --> 01:38:14,939
to another agent okay sounds good yeah so that's an example of a i want to say a hierarchy called

898
01:38:16,140 --> 01:38:24,279
agency multi-agent system what else any other ideas what would you add to that yeah

899
01:38:55,289 --> 01:39:00,170
oh i like that that's a really good one so let me summarize you have a security agent

900
01:39:00,810 --> 01:39:07,130
that determines if you can enter or not and when you enter it understands who you are and then it

901
01:39:07,130 --> 01:39:12,329
gives you certain sets of permissions that might be different depending on if you're a parent or

902
01:39:12,329 --> 01:39:18,970
a kid or you know you might have access to certain cars and others or the kid cannot open

903
01:39:18,970 --> 01:39:23,529
the fridge or i don't know like something like that yeah or okay i like that that's a good

904
01:39:23,850 --> 01:39:28,170
yeah and it does feel like it's a complex enough workflow

905
01:39:28,170 --> 01:39:39,930
where you want a specific workflow tied to that i agree what what else yes

906
01:40:12,750 --> 01:40:16,189
well that's really good actually so you mentioned two of them

907
01:40:16,189 --> 01:40:22,270
one is maybe an agent that has access to external apis that can understand the weather out there

908
01:40:22,270 --> 01:40:30,350
the wind the sun and then has control over certain devices at home temperature blinds things

909
01:40:30,350 --> 01:40:34,750
like that and also understand your preferences for it that does feel like it's a good use

910
01:40:34,750 --> 01:40:39,069
case because you could give that to the orchestrator but it might use itself because

911
01:40:39,069 --> 01:40:43,149
it's doing too much so you probably and also these problems are tied together like

912
01:40:43,229 --> 01:40:48,989
temperature outdoor with the weather api might influence the temperature inside how you want it

913
01:40:48,989 --> 01:40:55,390
etc and then the second one which i also like is you might have an agent that looks at your

914
01:40:55,390 --> 01:41:00,189
fridge and what's inside and it might actually have access to the camera in the fridge for

915
01:41:00,189 --> 01:41:06,750
example and know your preferences and also has access to the e-commerce api to order

916
01:41:06,750 --> 01:41:12,510
amazon groceries ahead of time i agree and maybe the orchestrator will be the communication

917
01:41:12,510 --> 01:41:17,069
line with the user but it might communicate with that agents in order to get it done

918
01:41:18,109 --> 01:41:22,829
yeah i like those so those are all really good examples here is the list i had

919
01:41:24,510 --> 01:41:31,710
up there so climate control lighting security energy management entertainment notification

920
01:41:31,710 --> 01:41:36,109
agent alerts about the system updates energy saving and orchestrator so all of them you

921
01:41:36,109 --> 01:41:42,109
mentioned actually and then we didn't talk about the different interaction patterns

922
01:41:42,109 --> 01:41:49,229
but you do have different ways to organize a multi-agent system flat hierarchical it sounds

923
01:41:49,229 --> 01:41:55,710
like this would be hierarchical i agree and the reason is ui ux is i would rather have to

924
01:41:55,710 --> 01:42:00,909
only talk to the orchestrator rather than have to go to a specialized application to do

925
01:42:00,909 --> 01:42:05,069
something like it feels like the orchestrator could be responsible for that and so i agree

926
01:42:05,069 --> 01:42:10,750
i would probably go for a hierarchical setup here but maybe you might act also add some

927
01:42:10,750 --> 01:42:15,710
connections between other agents like in the flat system where it's all to all for example

928
01:42:16,350 --> 01:42:20,350
with climate control and energy if you want to connect those two you might actually allow them

929
01:42:20,350 --> 01:42:25,069
to speak with each other when you allow agents to speak with each other it is basically

930
01:42:25,069 --> 01:42:30,750
an mcb protocol by the way so you treat the agent like a tool exactly like a tool here is

931
01:42:30,750 --> 01:42:35,630
how you interact with this agent here is what it can tell you here is what it needs from you

932
01:42:35,710 --> 01:42:42,989
essentially okay super and then without going into the details there are advantages to multi-agent

933
01:42:42,989 --> 01:42:48,909
workflows versus you know single agents such as debugging it's easier to special

934
01:42:48,909 --> 01:42:54,590
debug a specialized agent and to debug an entire system parallelization as well it's easier

935
01:42:54,590 --> 01:43:00,909
to have things run in parallel and you can earn time you know there are some advantages

936
01:43:00,909 --> 01:43:06,189
to doing that and i leave you with the slide if you want to go deeper super so we've learned

937
01:43:06,189 --> 01:43:11,310
so many techniques to optimize llms from prompts to chains to fine tuning retrieval

938
01:43:12,109 --> 01:43:18,430
and to multi-agent system as well and then just to end on a couple of trends i want you to

939
01:43:18,430 --> 01:43:23,710
watch i think next week is thanksgiving is that it is thanksgiving break no the week after

940
01:43:23,710 --> 01:43:27,630
okay well ahead of the thanksgiving break so if you're traveling you can think about these

941
01:43:27,630 --> 01:43:35,630
things what's next is in ai i wanted to call out a couple of trends so ilias is cover one of

942
01:43:35,630 --> 01:43:44,510
the ogs of a you know llms and you know opening i co-founder raised that question about are we

943
01:43:44,510 --> 01:43:48,829
plateauing or not you know the question of are we going to see in the coming years

944
01:43:49,470 --> 01:43:56,270
llm sort of not improve as fast as we've seen in the past it's been the feeling in the

945
01:43:56,270 --> 01:44:03,789
community probably that you know the last version of gpt did not bring the level of performance that

946
01:44:03,789 --> 01:44:08,989
people were expecting although it did make it so much easier to use for consumers because

947
01:44:08,989 --> 01:44:12,510
you don't need to interact with different models it's all under the same hood so it

948
01:44:12,510 --> 01:44:18,510
seems that it's progressing but the plateau is unclear the way i would think about it is

949
01:44:19,069 --> 01:44:26,350
um the llm scaling laws tell us that if we continue to improve compute and energy

950
01:44:26,350 --> 01:44:30,350
then lm should continue to improve but at some point it's going to plateau so what's going to

951
01:44:30,350 --> 01:44:36,270
take us to the next step it's probably architecture search still a lot of llms even

952
01:44:36,270 --> 01:44:40,909
if we don't understand what's under the hood are probably transformer based today but we know

953
01:44:40,909 --> 01:44:45,470
that the human brain does not operate the same way there's just certain things that we do that

954
01:44:45,630 --> 01:44:51,149
much more efficient much faster we don't need as much data so theoretically we have so much

955
01:44:51,149 --> 01:44:55,789
to learn in terms of architecture search that we haven't figured out it's not a surprise that

956
01:44:55,789 --> 01:45:01,630
you see those labs hire so many engineers because it is possible that in the next few years

957
01:45:01,630 --> 01:45:05,229
you're going to have thousands of engineers trying to figure out the different engineering

958
01:45:05,229 --> 01:45:10,829
hacks and tactics and architectural searches that are going to lead to better models and one

959
01:45:10,829 --> 01:45:16,510
of them suddenly we find the next transformer and it will reduce by 10x the need for compute

960
01:45:16,510 --> 01:45:23,710
and the need for energy you know it's sort of if you've read isak azimov's foundation series

961
01:45:24,510 --> 01:45:30,270
individuals can have an amazing impact on the future because of their decisions you know whoever

962
01:45:30,270 --> 01:45:35,630
discovered transformers had a tremendous impact on the direction of ai i think we're going

963
01:45:35,630 --> 01:45:40,189
to see more of that in the coming years where some group of researcher that is iterating

964
01:45:40,189 --> 01:45:45,149
fast might discover certain things that would suddenly unlock that plateau and take us to the

965
01:45:45,149 --> 01:45:48,750
next step and it's going to continue to improve like that and so it doesn't surprise me that

966
01:45:48,750 --> 01:45:53,949
there's so many companies hiring engineers right now to figure out those hacks and those

967
01:45:53,949 --> 01:46:00,029
those techniques the other set of gains that we might see is from multimodality so the

968
01:46:00,029 --> 01:46:05,789
way to think about it is we've we've had llm's first text based and then we've added

969
01:46:06,670 --> 01:46:11,630
and today you know models are very good at images they're very good at text turns out that being

970
01:46:11,630 --> 01:46:16,989
good at images and being good at text makes the whole model better so the fact that you're good

971
01:46:16,989 --> 01:46:22,270
at understanding a cat image makes you better at text as well for a cat now you add another

972
01:46:22,270 --> 01:46:27,869
modality like audio or video the whole system gets better so you're better at writing about

973
01:46:27,869 --> 01:46:31,869
a cat if you know what a cat sounds like if you can look at a cat on an image as well

974
01:46:31,869 --> 01:46:36,350
does that make sense so we see gains that are translated from one modality to another

975
01:46:36,350 --> 01:46:40,350
and that might lead in the pinnacle of robotics where all these modalities come together

976
01:46:40,350 --> 01:46:45,149
and suddenly the robot is better at running away from a cat because it understands what a

977
01:46:45,149 --> 01:46:50,750
cat is how it sounds like what it looks like etc that makes sense the other one is the

978
01:46:50,750 --> 01:46:56,670
multiple methods working in harmony in the tuesday lectures we've seen supervised learning

979
01:46:56,670 --> 01:47:01,229
unsupervised learning self-supervised learning reinforcement learning prompt engineering rags

980
01:47:01,229 --> 01:47:09,149
etc if you look at how babies learn it is probably a mix of those different approaches

981
01:47:09,149 --> 01:47:16,109
like a baby might have some meta learning meaning you know it has some survival instinct that is

982
01:47:16,109 --> 01:47:21,949
in the encoded in the dna most likely and that's like the baby's pre-training if you

983
01:47:21,949 --> 01:47:29,470
will on top of that the mom or the dad is pointing at stuff and saying bad good bad good

984
01:47:29,470 --> 01:47:34,670
supervised learning on top of that the baby's falling on the ground and getting hurt and that's

985
01:47:34,670 --> 01:47:39,630
a reward signal for reinforcement learning on top of that the baby's observing other people doing

986
01:47:39,630 --> 01:47:44,350
stuff or other babies you know doing stuff unsupervised learning you see what i mean

987
01:47:44,350 --> 01:47:49,069
it's we're probably a mix of all these methods and and i think that's where the trend is

988
01:47:49,069 --> 01:47:54,989
going is where those methods that you've seen in cs 230 come together in order to build an

989
01:47:54,989 --> 01:48:01,390
ai system that learns fast is low latency is cheap energy efficient and makes the most out of

990
01:48:01,390 --> 01:48:07,789
all of these methods finally and this is especially true at stanford you have research

991
01:48:07,789 --> 01:48:13,789
going on that you would consider human centric and some research that is non-human centric

992
01:48:13,789 --> 01:48:18,989
by human centric i should say human approaches that are modeled after the brain and approaches

993
01:48:18,989 --> 01:48:24,350
that are not modeled after humans because it turns out that the human body is very limiting

994
01:48:24,350 --> 01:48:29,069
and so if you actually only do research on what the human brain looks like you're probably missing

995
01:48:29,069 --> 01:48:34,189
out on compute and energy and stuff like that that you can optimize even beyond neural connections

996
01:48:34,189 --> 01:48:38,670
in the brain but you still can learn a lot from the human brain and that's why there are

997
01:48:38,670 --> 01:48:43,949
professors that are running labs right now that try to understand how does back propagation

998
01:48:43,949 --> 01:48:48,350
work for humans and in fact it's probably that we don't have back propagation we don't

999
01:48:48,350 --> 01:48:52,909
use back propagation we only do forward propagation let's say so this type of stuff

1000
01:48:53,069 --> 01:48:57,869
interesting research that i would encourage you to read if you're curious about the direction of

1001
01:48:57,869 --> 01:49:04,350
of ai and then finally one thing that's going to be pretty clear i call it all the time but

1002
01:49:04,350 --> 01:49:08,590
it's the velocity at which things are moving you're noticing part of the reason

1003
01:49:08,590 --> 01:49:12,989
we're giving you a breath in cs2 30 is because these methods are changing so fast so i don't

1004
01:49:12,989 --> 01:49:18,510
want to bother going and teaching you the number 17 methods on rag that optimizes the

1005
01:49:18,590 --> 01:49:22,590
rag because in two years you're not going to need it you know so i would rather you think about

1006
01:49:23,149 --> 01:49:26,750
what is the breadth of things you want to understand and when you need it you are

1007
01:49:26,750 --> 01:49:30,989
sprinting and learning the exact thing you need faster because the half life of skill is

1008
01:49:30,989 --> 01:49:35,310
so low you know you want to come out of the class with a good breath and then have the

1009
01:49:35,310 --> 01:49:39,229
ability to go deep whenever you need after the class and so that's sort of how that

1010
01:49:39,229 --> 01:49:47,630
class is designed as well yeah that's it for today so thank you thank you for participating

1011
01:49:48,510 --> 01:49:49,310
you

