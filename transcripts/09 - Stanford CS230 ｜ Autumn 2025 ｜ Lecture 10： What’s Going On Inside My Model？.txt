Welcome to Lecture 9 already. I hope everybody had a good, uh, fall break.
Um, today we're going to talk about neural networks,
both convolutional neural networks and transformers,
um, and we're gonna unpack it to see what's going on inside.
Um, this lecture used to be called,
um, Neural Network Interpretability,
but I've broadened the scope because there is,
uh, section now where we talk more about frontier models,
um, and the interpretability or visualization methods have not
quite been figured out for most models that you play with out there.
So think about this one as research areas,
what we know from convolutions and what we're trying to figure out for,
uh, frontier models.
Uh, we're gonna start with a very packed agenda with
the case study, uh, where I'm going to ask you a question, um,
and let you brainstorm a little bit, uh,
all together, um, on how you would, you know,
try to understand what's happening inside of a frontier model.
Um, in the second section,
we're gonna look at the example of convolutions
specifically and try to interpret everything possible about a convolution.
Meaning, we're going to look at input-output relationship.
We're going to look at a specific neuron inside and try to interpret it.
We're going to look also at, uh,
specific feature maps and try to understand what they do.
I will present many methods to do that.
Those methods are real and they've been used for convolutions.
Um, but again, they're not the methods that you might see frontier labs
use for today's language or, uh, vision, uh, uh, large, large models.
However, they are going to bring you the skills, uh,
that will allow you to understand the methods for frontier models
as researchers are trying to figure them out.
Um, the second half of the lecture is going to focus more on
the modern representation analysis, uh,
we're gonna talk about scaling laws,
capability benchmarking, data diagnostics,
and then I- I end on- on a few closing remarks.
Okay? Are we ready for this one?
Lots of visualizations in this lecture.
So first, um, um, question for you all.
Um, let's say the case study is you are a model trainer,
um, and you're, you know,
working on a 200 billion parameters model, um,
at a frontier lab and overnight, you know,
a new checkpoint passes a training sanity check,
but a few issues arise.
Things like, you know, uh,
model is getting worse on reasoning benchmarks, um,
some safety evals are failing, um,
and there is a weird spike in, let's say,
latency for tool use when you actually use this model for an agentic workflow.
Your VP is wondering what's happening and they ask,
what is going on, um,
and what are you going to look at first?
So what I want you to discuss, um,
for a minute or so, think about it first and I'll open up, um,
is what are the type of evidences that you would look,
want to inspect before even, you know,
touching the code or retraining the- the model?
What are the things that you wanna look at?
Jumping. There's no single answer.
So I wanna know everything you're gonna look at.
Okay. So, um, error analysis.
So look- look at- you said,
I will look at the reasoning benchmarks and find the examples where the model is failing.
Specifically, try to find patterns in order to pinpoint what the issue might be.
And then same thing on the safety, um, evals,
where you wanna see what type of safety issues are arising.
Is it everywhere? Is it specific to something?
Yeah, I agree. Error analysis in general. What else?
Remember, you're- you're the model trainer.
So you're training this model.
You're- you're supposed to be watching certain things when you're training.
What can be interesting?
Yeah. Yeah. Let's say not necessarily passing,
but those are great examples.
So you're- you're mentioning- yeah.
As you're the model trainer,
you would be watching the training loss.
And you wanna see what- what- what are you going to look for in that training loss?
Convergence.
Okay. Convergence.
You probably want to make sure that it's smooth.
You don't want big spikes.
Um, how about the validation loss?
What- what is your expectation on the validation loss?
Yeah, should probably follow the same curve as the training loss,
but is likely slightly higher because you're probably
performing slightly less well on the validation set than on the training set.
If you're seeing spikes,
it might- it might lead to cert- it might mean there are some issues.
Um, what else are you looking at?
Yeah. So this batch you mean?
Yeah, yeah, yeah. So you're looking at this round of training data.
Maybe the last round of data that we trained on,
there were some issue in that data.
Maybe that data was, uh, you know, uh,
probably, you know, poisoned or biased toward
a certain category of data that we're failing on.
You're- you're totally right. Yeah.
Maybe that specific checkpoint is doing poorly compared to the previous checkpoint.
And so you have pinpoint- you pinpoint where the issue arises.
You know, during the- the training.
What else are you looking at?
Yeah. Okay.
Because it's overnight and it seemed everything was good up to yesterday,
and now there's an issue,
maybe you're saying there is a hardware issue.
Yeah, we could check actually.
Is- yeah, latency has been, uh,
pointed out so maybe the- the hardware has failed.
Yeah, you're right. What else?
So we- we- a lot of the answers are global answers.
You're looking at the model in general.
You're not looking at specific portions of the model.
What would you look at if you were to inspect,
um, the model more precisely from the inside?
And this one's a language model.
So you can- you can think about the fact that it's a language model.
Yeah. Yeah, you're right.
Like, you wanna look at different checkpoints and see where did we fail
and might be able to trace back to that moment and figure out what the issue was.
So for example, maybe your initialization, um,
was actually pretty good and the first checkpoints were doing well,
but suddenly at some point, uh,
the model saturated in a certain way.
Maybe you're seeing exploding gradients or
vanishing gradients in certain moments and you wanna pinpoint that.
Yeah. What- what else?
We're- we're adding so many methods right now,
but I- I wanna hear what else you have for language models.
What other things can you visualize for language models
that might- might mean something's going wrong?
Yeah. The attention maps.
Yeah, yeah, fair enough.
You- you've learned about transformers, uh,
in the online videos, um,
the attention maps which are representative of,
you know, the relationship between different tokens.
They might not make sense to you.
You might actually be plotting certain attention maps and be like,
this token has nothing to do with that one,
but the model seems to think it has.
And you might be able to, um,
identify certain issues with the attention maps.
What else beyond the attention maps?
What- what- yeah. So you mean,
tell me more about the sensitivity analysis.
What would you fix, for example,
and what would you change?
Okay. Yeah, but I like the idea of sensitivity analysis.
You might fix- you might try to figure out which hyperparameter went wrong.
Is there something wrong with our optimizer?
Is our learning rate schedule poorly tuned?
Um, uh, maybe scaling laws.
You know, we know that we can play with compute,
we can play with data,
we can play with model size,
and one of those might be going wrong.
Maybe an analysis would allow us to identify the model is fine,
it just needs to be trained longer,
or the model is actually too small for the amount of data we're giving it.
You know, that type of stuff would come with, um,
either doing a sensitivity analysis or, uh,
comparing what we're doing to the scaling laws that we know from other models.
Uh, we're gonna look into that.
Okay. Any other ideas?
Might be. So you're- you're saying you're- you- I gave you 200 billion parameters,
which is a very large model even as of today.
Uh, it might be over-parameterized.
That's a good question because it depends on what it's been trained on,
how much data we're feeding it,
how much compute. It's all relative to each other.
But yeah, it's a large model,
so I would expect a lot of compute and a lot of data along with it.
Um, you know, in fact,
a lot of these models might be built as a mixture of experts.
You- you- you- you've heard about mixture of experts.
One thing that could happen is that some of the experts are failing.
And you might be inspecting if experts are in fact failing or the routing module
is, um, always selecting the same experts because it's just,
you know, found an expert that is really good and generalized,
and the other experts are not being used.
That might be another issue as well, uh,
that might, you know, be related to the model capacity.
Because if the model is not using
all its experts it's probably not actually operating as a 200 billion parameter model.
It's operating as a smaller model.
Yeah. Okay. So, you know,
generally this is to motivate the lecture.
We're gonna look into all of these together today, right?
And we start with convolutions, uh,
because they're very visual.
For the convolutional part,
we're gonna go super deep.
Um, but then for the frontier models,
I'm just gonna get broader and- and give you the areas of research.
So the answer to the question I asked typically would fall under four buckets,
every solution that we looked into together.
One is training and scaling.
So people are looking at loss curves,
at, you know, um, things like gradients,
uh, learning rates, mixture of experts, routing,
scaling laws, we're- we're gonna talk about all these.
Um, the second category is representation and internal aspect of the model.
You mentioned attention, uh, heads and maps,
embeddings, nobody mentioned embeddings,
but you could actually visualize embeddings and see,
does it make sense to you?
Are these, um, you know,
tokens close to each other as you would expect,
meaning the- the model's mental understanding of language is correct.
Um, and then your own level behaviors,
although that's really hard with a large model,
um, and nobody has quite figured it out yet.
Um, and then the other category might be data and distribution,
maybe, you know,
the actual, um, uh,
benchmark that we're looking at has been contaminated,
meaning, you know, the model is just not- either it's doing too well on
that benchmark or it doesn't mean anything or it's doing poorly for a certain reason,
because the data distribution used in
the test set is completely different from the training or validation set.
Um, and then, you know,
it might be failing at different levels.
You can run benchmarks on the language model,
you can run benchmarks on the agentic workflow that is using that language model.
And because you want the language model to be used in agentic workflow,
those are two levels that you need to inspect.
So for example, when, uh,
a frontier lab says, um,
our model is doing really well for tool use,
what they mean is the language model has been tested on upstream tasks in
a workflow and it's actually good at tool use against their benchmarks.
So those are different levels of capability analysis.
Okay. So let's talk about, uh, convolutions.
We're gonna dive deep inside convolutions and then we'll
go back up and look at frontier models, okay?
So, um, first case study, uh,
for convolutions, let's say that you have built an animal classifier for a zoo,
and they are very reluctant to use
your model, um, without any human supervising because they
don't understand the decision-making process of the model.
How can you alleviate their concerns?
How can you give them intuition about the decision-making process of the model so that they feel like,
ah, the model's doing things that feel natural and- and human.
So let's say, just to simplify,
let's say you have a- a convolution neural network,
and there's a softmax layer,
and it's supposed to identify animals.
So the number of classes are many animals.
Yeah. If you were to write a quick Python code to give them some intuition,
how would you do it? Yeah.
I'm trying like first with a softmax.
Good, good, good. So just to recap, um,
the zoo is not AI native,
so you have to explain certain things.
You're gonna tell them what softmax is,
so we're gonna have a probability for each animal classes.
That's how it works.
Um, and on top of that,
you also mentioned you might talk about convolutions and say,
here are how features are identified,
here is how a filter scans through the image,
and, you know, we're expecting this to learn.
So you're gonna educate them first. That's totally right.
The second thing you mentioned is maybe you run a- a dataset search.
So you can try to build their intuition by showing them
animal pictures and showing that the model's doing well.
And yeah, I agree.
Those are good approaches.
We're gonna see how to do a proper dataset search large-scale.
But what else can you do,
that's gonna give a little bit more confidence because this is explanation,
but it's not proof that the model is looking at the right place systematically.
Yeah. So you say ideally,
you would give them intuition at a fi- at a filter level.
Like this filter,
we know it's responsible for finding the legs of an animal.
That's what you're saying. So how would you do that?
You know, good, you know, intuition.
You're asking is it as simple as just printing out the weights of- that are
identified or the feature map that results of that filter?
Unfortunately, not usually,
because that might be true for the first layer.
But as you get deeper,
things mix up so much that, you know,
if you were just to print the filter,
it wouldn't make any sense pretty much.
Um, but there are other methods that we're gonna see.
So your intuition is right.
We're- we're gonna try to give them that.
Something simpler. Input-output relationship.
How would you show that, um,
that the output is actually related to the right portions of the inputs?
For this dog, for example.
Yeah. Yeah. Confusion metrics across a lot of data.
You would find true positives, false- et cetera.
Yeah. Correct. Something else?
So similar to what he said with, um,
you wanna give them intuition from the inner workings of the network,
and you're saying how about we mask the latter parts of the network,
and we treat every intermediary layer as an output,
and analyze if the output makes sense.
Yeah. We're gonna look at that actually.
Um, yeah. Those are more advanced.
What I was looking for is even more- more basic, much more basic.
It's, uh, if you wanna show the, uh,
relationship between an input and an output of a CNN,
uh, or, you know, any vision model,
um, you might take the score of the dog,
uh, in the output layer, okay?
And what is exactly this quantity?
What- what- what is the intuition,
your intuition for what this quantity means?
If you take the derivative of the score of a nanomole class with respect to x,
with x being the input image.
Yeah. Yeah. How does the score of dog change when you move pixels around?
Which is what you want, right?
You wanna be able to- if you can do that,
you would indicate that which are the pixels of the image,
that if we change them,
it changes the score of dog.
If you can print that,
then you would be able to show this is where the model is
looking at when it's predicting a dog, right?
So, yeah, if you actually calculate this derivative, um,
you would get something like that where some of the pixels are gonna be brighter,
meaning their gradient is higher,
and some of the pixels are gonna be darker,
meaning we move that pixel and it didn't modify the score of dog at all.
That's a very quick way to look at which pixels in the input were relevant for the score of dog.
Uh, now, why should we select the score of dog pre-softmax versus post-softmax?
It's usually a very common mis- misconception.
Yeah. So, what's the issue with the scaled version class of dog?
Yeah. So, uh, what- what you said is the- the post-softmax score is not only depending on dog,
it's also dependent on all the other scores.
So, you could actually, um,
take a pixel, move it,
and it happens to modify the score of a panda,
because there's a panda in the background or something,
and it would influence what you're trying to show.
Uh, but you're only looking at dog.
You just want the score of dog to be influenced.
So, that's why in this method called saliency maps,
we use the pre-softmax scores that is only representative of the class at hand that you're analyzing.
Okay? So, you could do that.
And actually, if you were to- in the past,
not anymore, but you could use that for a quick segmentation,
chance-sanity check because the pixels that are brighter,
the gradients that are brighter are representatives of the pixels that should be overlaid on the dog.
And in fact, if you do the saliency maps and you realize that the pixels that are
bright when you compute that gradient are all over the place,
it's probably that the model is not even looking at the right place.
It's just getting lucky. Okay?
So, this first method, saliency maps.
Now, I have that in your toolkit.
Very easy to implement, right?
You just write a Python script,
you- you perform the gradients calculation,
you print it, it's a matrix of pixels that are brighter or darker, and you're done.
Um, one of the main, uh,
issues with saliency maps is that it's looking at just a pixel level,
um, which doesn't make too much sense if you want to
interpret semantically where the model is looking at.
You know, the model will never see a cat or a dog with
one pixel being different than the rest.
It would be too discontinuous.
So, instead, there is another method.
I'm not gonna go into the detail,
but I link the paper which is way more common called integrated gradients.
Um, and integrated gradients, uh,
the idea is that instead of doing that directly by taking
the ds of dog over dx,
we're gonna take an image of the animal and we're gonna, uh,
generate, you know, sort of many pictures that are, uh,
taking, uh, dark, completely black zeros, um,
a pixel all the way to the animal, um,
the final image and then we're going to look at the path of gradients across all of this update,
and it's going to be way more interpretive.
I'm not gonna go into the details,
but integrated gradients is just an extension of saliency maps
that happens to use a different formula with an integration and is way more common.
Um, if you look at it practically,
this is an example from the medical field.
Here is a- an image of a retina,
and, um, if you perform the integrated gradients,
you would see that, uh,
the original, um, image, uh,
the- the- the- the annotation for the les- lesions are exactly where the model is looking at.
When it's giving you a probability that there is a lesion.
Okay? So second method called integrated gradients.
Let's push it a little further.
Um, the next case study is, uh, that, you know,
you- you- you- you- you want to now, uh,
tell them a little more about the decision process, um,
of the model, uh, with, uh,
I guess- I guess let me- let me rephrase.
The saliency maps looked at the pixel level.
Uh, what- what you can do in order to give a better intuition, uh,
which was mentioned earlier is another approach, uh,
called occlusion sensitivity,
which is, uh, actually way more, uh, intuitive and simple,
where you could actually, uh, you know,
take the dog image and paste it into the CNN,
and you would get a score of a dog.
You could also overlay a dark square.
So zero out or mask partially the input image and give it to the same CNN,
and track the modifications on the score of dog that you're tracking.
If you actually do that,
you can plot a probability map of how is the score of dog changing,
as I move the dark square through the image.
So let's do it together.
I'm gonna say that, you know,
this one when you- when you actually put the dark square on the top left of the image,
the score of dog is unchanged.
It's still very high.
Now, you, uh, move, uh,
the square a little bit to the right,
and you see that it's still very high, the score of dog.
You do it again, still very high.
Now, uh, the square is partially occluding the face of the dog,
and you should see the score of dog drop,
if the model is in fact looking at the dog.
And you perform that many times,
so you scan, uh, through the image with your dark square,
and you plot what we call, um, you know,
the, uh, the probability map of the true class for different positions of the gray square.
Does that make sense? So pretty simple,
computationally expensive though.
Just have to rerun the image so many times through the model.
Here, practical examples, um,
to look at the first one, uh,
the true label is a Pomeranian cute dog,
and you see that the model is, uh,
failing to recognize the true class when the square is overlapping with sort of the center of the face.
Which makes sense because here the true class that we're tracking is not dog,
it's Pomeranian, and I could see how if you occlude the face,
it's hard to get the breed of the dog.
The second example, the true label is a,
a car wheel, sorry, I hadn't shown you that.
Uh, the true label is a car wheel,
and you can see that when the square is on the wheel, um, it is,
um, it is in fact dropping in terms of the true class probability.
And then finally, the Afghan hound.
What's interesting about that third example is the probability is dropping when the square is on the dog,
but it's also increasing, uh,
when the square is on the face of the human on the left.
Which means that if you actually occlude the,
the face of the human,
the model thinks even more that the true class is in fact an Afghan hound.
You're just removing additional unnecessary information for it to discover the true class.
So this model seems to be doing well, right?
It seems to be looking in the right place.
We call that occlusion sensitivity,
pretty simple, another tool with saliency map and integrated gradients in your toolkit.
Let's push it slightly further.
Um, here, uh, here we're,
we're given, um, you know,
along with the classification outputs,
we- the zoo wants a real-time visualization of the model's decision process,
and you have one day to show that.
And we're talking about convolutions again.
What do you do? So the important part is to know,
uh, since the,
the methods that we've seen so far are sort of
post, uh, methods where you analyze the output or you show something.
Here we're looking at a sort of ideally a,
a module that we could plug in our network that would
constantly give us the decision-making process of the network,
or at least where it's looking at.
How would you do this?
This is our network by the way.
We're taking an input,
we're adding zero padding,
and we have a series of conv relu max pool blocks.
And then at the end we flatten,
we have a triple fully connected layer,
a softmax, and we get our probability output for classification.
So just first question,
where do you think is the weakness of this network when it
comes to interpreting where the model is looking at on a picture?
A part of this architecture is very- makes interpreting,
uh, the network way harder. Yeah.
The fully connected layer is there?
Why?
Because.
Yeah. Totally right.
The fully connected layers,
you're looking at all the pieces at the same time,
you're mixing everything and you're doing it three times.
So by the end of those three layers,
the information has been mixed together, essentially.
You do not find the localized information that you had
pre that with the max pools and the conv layers.
So how could you change that layer in order to avoid that?
How would you modify your network if you wanted to retain
maybe the, the,
the performance of the model but not lose that localized information?
Yeah. Good idea. How, how could we,
instead of doing three, can we do one?
Can we, you know, still have a layer that makes the,
uh, you know, makes the interpretation easy?
Actually, there's another trick which we're gonna see,
but it's similar to what you described.
Let's say we convert this network into, uh,
something where the, you know,
the flattening of the pixels and the fully connected layers are
converted into a single global average pooling layer and a fully connected layer.
So here we reduce from three to one the fully connected layers.
We still need our fully connected layers in our softmax because it's
a classification task and we want a good decision engine at the end.
Uh, but we converted,
we, we added a global average pooling.
So let me explain why,
why this is, uh, better.
So the last conv block essentially is giving us a volume.
For the sake of simplicity,
let's say that volume is a four by four with six channels, okay?
And I color-coded them for simplicity.
So each of these channels is a feature map that is resulting from
a filter being scanned through the previous input, right?
Everybody's clear on that?
So global average pooling is gonna take each of these channels,
feature maps, and is going to average them in a single number.
So if you take the orange matrix and you average it,
it gives you one of 4.7.
You do the same thing with the green one,
the blue one, all six of them,
and you get a volume or call it a vector of size six, one, one, six.
So why is that interesting?
Because we actually did not lose the localized information.
Um, we did not mix things up.
We just assigned a single number to a feature map that we retained.
So the localized information is still there on the previous volume.
And now we can treat that as a vector that goes through
a decision engine or a fully connected layer
that ultimately goes through our softmax and give us the probabilities.
So this architecture is easier to trace back to localized information in
the input space because you can actually look at,
let's say, one of the score of dog and you can look at the weights of each of
these edges that tell you how much has
the feature map from the volume before contributed to that score.
So in other words, if I had to summarize,
let's say the feature map looks like this.
So this feature map is very high,
has somehow activated heavily in some portion of the input image.
The others similarly have activated to other things.
You're taking the weights from your fully connected layer.
By the way, you have to retrain that layer.
You have to just train that layer, that last one.
And then you sum all of them and it gives you what we
call a class activation map for the class that you're visualizing.
So you're overlaying those last six feature maps
and you're weighing them with the weights of the last fully connected layer.
You're not losing information.
You're not mixing three fully connected layers
that are impossible to trace back to the input space.
Okay. So if you now give it an input image and you overlay
the class activation map for the score of dog,
which you can do for other classes.
You can do the same thing for the class of cats.
Look at the different weights, the feature maps.
And maybe for the class of cat,
the weights will certainly be different.
So the contribution of each feature map will be different.
Yeah. And this is what you get.
This is called class activation map.
And it's from folks over at Berkeley.
And so here's a video that describes it.
It runs really quickly.
You can think of it as a slight modification to a vision network
that can allow you to unpack what's
happening inside and what's the decision process.
There is also sort of an improvement to
the CAM or class activation map algorithm called GradCAM,
which enhances that method.
Okay. Any questions on class activation maps?
So we're getting to know convolutions a little better.
Yeah. Yeah. So you were saying in the video,
it seems like the model sometimes is looking at meaningless things.
Yeah. I mean, it's not surprising, frankly.
This is the previous generation of models.
And on top of that, you're looking on a video.
The model is a classification network.
So it might look sometimes at things that are not even labeled.
And so it has to find the closest one.
It might not make sense at all.
That's why you build that type of module to visualize and
understand like the network's actually not working that well.
But maybe on the main objects
that you actually want for your task,
let's say the zoo wants to do very well with cats and dogs.
You can verify that when a cat is moving even at
fast speed the model is quickly looking at it.
Super. Let's do a couple more methods
because it's going to build our intuition for frontier models.
Um, so now the zoo trusts you.
It trusts that the model correctly locates animals.
But they get sort of scared and they wonder if the model understands what a dog is.
Like, does it understand actually what a dog is or is it
just like pattern matching random things?
Um, you know, how could you,
um, take this ConvNet and sort of query
what the dog- what the model thinks a dog is?
How would you do that?
If you- how could you ask the model what's your best representation of a dog?
Okay. Yeah. You did say two things.
So get an image,
so a forged image that maximizes the probability of dog.
Yeah. Let's do that actually and then on your second point,
um, um, about, uh, reverse engineering,
we're going to look at the method there as well.
But yeah, I agree.
You could, um, so how would you concretely do that?
Like, what would you maximize?
Okay. Actually, what we said earlier,
but I- I think you came right after that the- is we would not take the softmax output.
Because the softmax output is dependent on other classes.
You divide by the sum of the exponentials of other classes.
And so you could actually maximize the softmax output by not maximizing the class you want,
but by, uh, minimizing the other classes,
which is different than what you want.
So, um, here's what we'll do.
We'll define the loss function where we take the pre-softmax score of dog,
so the thing right before the softmax,
which is only dependent on that specific class,
and we might also regularize it to make sure it looks natural.
The reason we want the regularization term is because pixels need to be between 0 and 255, roughly.
And so you don't want to run an optimization, uh,
a problem where pixels can have values that go all over the place.
It's just not gonna look good to the human eye.
Um, and so we're gonna do that.
We're gonna run a gradient ascent algorithm.
So similar to what we've seen in some of the previous classes,
um, where we are gonna update the pixels of an input image,
a completely random input image,
until we can maximize the loss function we defined.
Um, so we forward-propagated the random image,
we compute the objective,
we back-propagate, um,
all the way back to the pixels,
and then we update the pixels to maximize that objective.
And we do that many times until, uh,
we end up with something that might look like this.
So let's say we take the score of a Dalmatian, um, you know,
here, um, researchers, um,
and- and, uh, showed,
and this is work for Jason- from Jason Yosinski,
um, shows that you can start seeing the model.
If you ask the model, what is a Dalmatian,
it will tell you it's something with black dots on a white background, roughly.
So actually might not understand fully what the dog is,
but it- that's what it thinks it is.
So we just unpacked it a little bit and queried, uh, queried that.
Another interesting one is if you look at,
um, the goose.
So here, the- the- the top left, um,
labeled, uh, goose for the model is many of them.
What does that mean? It means probably the model has seen
a bunch of geese all the time together and has rarely seen a single one.
And maybe the labeled data was labeling that as goose when it was geese.
And so the model actually doesn't understand that it's a single one.
It thinks that all of them are the label.
Does that make sense? Okay.
Super. So that's called class model visualization.
You can actually- oh, sorry,
I wasn't showing what I was talking about.
Or no, I was showing.
The, uh, the- the- the way to
improve those visualization is just to change some of the regularization method.
So the researchers have shown that you can actually, um,
add more color by regularizing better so it looks better to the human eye.
And then it becomes easier to query the model for a variety of classes,
just to make sure that it understood those classes.
And so same with flamingos, um,
actually the label flamingo to the model feels like many flamingos.
Just something you can observe.
Any questions on, uh,
class model visualization?
Nothing super new, just another, um,
you know, tool in your kit.
It turns out you can apply the same,
uh, type of method as class model visualization.
But instead of doing it at the class level,
uh, you do it in, uh,
an intermediary activation.
So you could actually do the same exercise,
sort of what you were saying earlier with
the masking of the later layer and just looking inside the network.
You could pick an activation in the network,
create an objective function with the regularization,
and say, hey,
show me the input picture that maximizes this activation.
And that should tell you what is the input that maximizes activation the most.
Right? Okay.
So that's class model visualization,
which can also be applied with gradient ascent anywhere inside the network at any neuron.
And that already gives you some sort of a,
a method to look at the neuron level and say, hey,
what's the input that maximizes the fake input that
theoretically you could generate that maximizes that activation.
The next method is actually the most commonly used,
um, today because it's so simple and intuitive.
It's a dataset search.
So what you could actually do is to pick a filter.
You, you, you just pick one filter,
you pick its feature map among, you know,
I guess you pick one feature map at some point in the network,
such as at, at, at after this max pooling layer.
And, um, so let's say you have 256 filters in that convolution layer.
So you have 256 feature maps of size five by five.
Um, and you, um,
find across all your data,
your validation sets,
the top five image that, uh,
maximize this feature map.
Yeah. So you just track the activation in that feature map.
You find the highest activation across all your data,
and you find the top im- the top five images,
and you can do that, you know, again.
You would say this seems that the filter,
um, that produced that feature map has learned to detect shirts.
Because if you find the top five images that activated that feature map the most,
that filter the most, uh,
it's all images of shirts.
If you were to find something like this,
you would say it seems that the filter has learned to detect edges.
And you could do that across every feature map to interpret your filters.
Okay. Simple dataset search that can allow you to
interpret if a, if a filter is reacting to something meaningful.
So if you look at these pictures that I printed at the bottom of the slide,
they're all cropped.
So why are they cropped?
They don't look like images from the dataset.
The image is probably bigger than that, right?
Hmm? The what?
So- so we took- we- so we took the input image,
we send it through the conv relu blocks,
and then we pick a feature map in the fifth block, let's say.
What is this feature map looking at?
Is it looking at the whole image or no?
Sorry, what is the- so we pick a feature map,
and in that feature map we find the activation that's the highest.
So let's say the activation is the row number five, column number three.
If you pick that activation,
does it have access to the entire input image or not?
So you don't necessarily have access to the entire image.
The best way to visualize it is at the first layer.
Let's say on the first layer,
you take the input image,
you take a filter,
and you run it through.
Well, that activation in the feature map of the first layer is
only going to see what the filter sees, right?
When you go deeper in the network,
do you see more or less on average of the image?
A single activation, does it have access to more or less parts of the image?
I'm saying more? Yeah, it's more.
Let's look at it. So here's a picture 64 by 64 by three, let's say.
We have a conv network,
and after five layers,
the last conv has 13 filters and leads to a 13 by 13.
Sorry, it has 256 filters that leads to a 13 by 13 feature maps.
If I look at this feature map,
let's say that's the most activated.
I trace back to the input space,
it will have access to this part of the image, let's say.
Now, if there was another conv relu block,
and I was looking at a feature map,
it would have even more abstraction of multiple portions of these squares.
So it would actually see slightly more from the input image.
Does that make sense?
So that's how you would think about it.
It makes sense because at the end of it,
the last output has access to the entire image, right?
Because all these things are adding up to the prediction.
Okay. So the deeper the activation,
the more it sees from the image,
and that's why the images were cropped.
They were just cropped based on tracing back
what that activation was looking at in the input image,
which you can do very simply computationally.
Okay. So now we're going to look at our last method for
convs which has to do with reverse engineering, a conv,
and then we'll move to the frontier models.
So remember this slide from when we introduced generative adversarial networks, GANs.
I didn't talk too much about the generator architecture.
I just said it was a neural network,
but I did mention that something's weird about that network,
which is that the input is way smaller than its output.
The input is a vector z,
the output is an image of more dimensions.
It is very common that such a network needs to upsample and thus would use deconvolution.
Sometimes in the literature you're going to see dimensions of
deconvolutions as an upsampling network with the inputs is smaller than the output.
Sometimes those are called transposed convolutions,
but we're going to talk about why.
Another example where you might run into upsampling is when you have
an encoder-decoder type networks such as for segmentation.
So let's say you're given an image of a cellular set of cells like this,
and then you want to label segments,
the pixels that belong to a cell just to find the different cells.
Typically, you would use a set of convolutions that reduces the volume in height and width,
and then you'll get information encoded in
a dense format that you will then
upsample because your output should be of the size of the inputs minus the number of channels,
but at least every pixel should have its own class.
So typically that would be a set of convolutions followed by deconvolutions.
So you downsample, you upsample.
Why am I talking about deconvolutions?
Because we're going to try to reverse engineer
conv networks by adding a module,
a deconvolutional module that will take a specific activation and will reverse engineer
the trace to verify what was the reason this activation was high.
This idea is key not only for
cons but for any network you'll think about in the future when you work on it,
if you want to actually reverse engineer the reason a specific neuron has been active.
So let's take the example of a 1D convolution.
This is the most basic example just for the sake of understanding the math.
And I give you an input x which has some padding with two zeros at the top,
two zeros at the bottom,
and then x1 through x8 values,
and I send that input to a 1D convolution which has one single filter of size four with a stride of two.
What's the size of my output y? Remember the formula.
What? Five.
Five, correct. Yes.
I assume you did this, this.
So you took the nx,
you applied the formula and you floored it,
you didn't forget to add one and then you ended up with five.
Is that it? Yeah. Correct. Super.
So we have our output of five,
and let's say we define our filter size for w1, w2, w3, w4.
It's actually easy to see that
the conv1d can be written as a system of equations,
where y1 equals w1 times zero plus w2 times zero because of the padding,
plus w3 times x1 plus w2 times x- w4 times x2, right?
You're just overlaying the filter on top of the first four indices of the inputs,
and you're doing a dot product.
Same thing with the second one,
third one, all the way to the fifth one,
and that's your system of equation that describes this conv1d.
Now, because it's a system of equation,
you could actually write it as a matrix multiplication.
So you could say the conv1d is literally just a weight matrix that we multiply by the inputs.
So the inputs and the output sizes we know,
five-one and 12-one.
So the weight matrix is necessarily a 5 by 12 weight matrix.
So we just rewrote the conv1d as
a single weight matrix that you multiply by the input, you get the output.
If you were to draw this matrix,
this is what it would look like.
So it would be a matrix with values all along the diagonal,
and the rest are zeros.
So that's our conclusion, conv1d can be rewritten as a matrix vector multiplication.
Everybody follows? So if you can write it as a matrix multiplication,
remember we're talking about reverse engineering,
then I could say d-conv is possible.
It's possible to reverse engineer that.
And in fact, I'm going to make a very big assumption that is not always true.
Sometimes it's true, but for practical reasons we're in deep learning, right?
It's an engineering field.
We're going to assume that W is invertible.
And so you can find a matrix H that is equal to the inverse of
W such that X equals H Y.
So that you're able to reverse engineer the signal.
I'm going to make a second assumption in the sizes I printed,
which is even bigger, is that W is not only invertible,
it is also orthogonal,
meaning that its inverse is its transpose.
It happens that it's sometimes true.
And in fact, if you think about an edge detector,
so let's say our filter is minus 1, 0, 0, 1, 0, 0, 0, 1.
It's an edge detector.
Sorry, I have one too many zeros,
but it's an edge detector.
And it's actually, you know,
invertible and its inverse is its transpose.
And so that simplifies our reverse engineering,
because we have a conv1d and we know that we can write it as
a matrix vector multiplication and we can transpose that matrix to reverse it.
And maybe it's not always true,
but it's true enough for it to work in deep learning, pretty much.
You're going to do that so many times, right?
That's why in the literature,
oftentimes, deconvolutions are called transposed convolutions.
I gave you the 1D example,
the 2D example is similar,
it's just more complicated,
there's more math, things mix up,
but, you know, same idea.
Now, there is a trick that makes it simpler to code the deconvolution,
and to see that trick I just drew x equals w transpose y.
So you have your x which is a vector of size 12,
although there's two padding at the top or at the bottom.
And then I transpose the w matrix that I was showing you,
and then I multiply that by my vector y of size five.
This is actually a transposed convolution with stride two,
is equivalent to something slightly different,
which is a sub-pixel convolution of stride one-half.
It's a mathematical trick.
You can do it at home,
but you would see that these two operations are equivalent,
meaning you can actually flip the filter.
So you see in the left side of the screen,
the filters are flipped.
So if you look at the first row of my matrix,
it's not w1 through w4,
it's w4 through w1.
So I flipped the filter and I scanned it all the way through the diagonal.
I also used another trick which is my y vector,
I inserted zeros in between the values, it's called sub-pixel.
So I inserted zeros and I also added some padding.
So a couple of tricks,
but no need to remember it by heart.
If there's anything you can remember,
it's that implementing a deconvolution in
the sub-pixel version I was describing is similar to a convolution.
But what you do is you create a sub-pixel version of the inputs by
adding zeros in between the values and padding it.
You flip the filters and you divide the stride by two,
and that's what a deconvolution is.
So if you have a convolutional neural network and you want to reverse engineer it,
you take the filters,
you flip them, you create a sub-pixel version of the inputs,
you divide the stride by two,
and you run the process,
you will have reversed that convolution.
The reason we're doing that is because we're
just rewriting the convolution as another convolution,
but the hyper-parameters are different.
But it's easy to code, right?
You're just reusing the same code, pretty much.
So anyway, let's get back to our example here.
We have an image of a dog,
we run it through a conv net,
and we pick at some point in that conv net a feature map.
We pick one feature map only among the 256 possible feature maps right here,
and we're going to look at the max activation of that feature map.
So we find the max,
let's say it's this one.
So row two, column three is the maximum number of that feature map.
What does it mean?
It means the filter that led to that feature map,
when it looked at its input,
it was maximal in that location.
It's maximally activated in that location.
We zero out every other entries of this matrix,
and then we reverse the network.
So we max pooled, we unpooled.
We relued, we do the reverse.
We do a decom instead of the conv,
which is a transposed convolution,
sub-pixel version, flip the filter,
divide the stride by two.
And we do that how many times?
Three times because we had three blocks.
And then we should be able to reconstruct what this activation was maximally activated for,
and we get the cropped version as we learned,
the cropped part of the image that this activation was looking at,
and exactly the pixels that maximize its value.
Does that make sense? It's pretty complex,
but it's important to know these methods because you
might run into something similar in the future,
or be asked to sort of interpret certain feature maps,
certain activation maps, etc. Okay.
So some additional details that we'll cover is what is unpooled,
and why do we do some relu in there.
So very simply, let's say I take the max pooling layer,
I max pool this,
filter size two by two,
stride of two.
If you wanted to unpool this,
how would you do it? Are pooling layers,
max pooling layers invertible?
No? Why? Yeah.
Yeah, exactly. It's not invertible because if you pick,
you write the six here on the top left,
you can tell that the six was in one of these four,
but you don't know where it was.
You can't invert. And it's very important to know where it was, right?
So it's not invertible,
but you could actually use a trick to make it invertible,
which is passing what we call switches.
So during the forward propagation,
you look at all your pooling, your max pooling,
and you remember with the binary matrix,
a very lightweight matrix,
where the pooling happened,
where the max pooling happened.
And then when you're doing the unpooling,
you remember those switches,
so you keep them in memory and you pass them back,
and that should tell you where the value came from.
Okay. So that's what we mean by unpooling.
Okay. So I go back to my previous map.
The only thing I need to change to be able to reverse engineer
my network is to pass the switches and the filters, by the way,
because the decom is just the flip version of the filter,
with subpixel and stride divided by two.
And so I do that.
So you can see, you can literally invert your network here,
and trace back from one activation to the input space.
And then for ReLU is a little odd.
I'm not going to spend too much time on it
because it's more empirical than nothing.
But ReLU forward is essentially zeroing out
every value that is negative during the forward path.
Technically, a ReLU backward is impossible
unless you have also the switches.
If you have the switches, you could actually, you know,
pass linearly back whatever was kept
because it's the identity function.
But actually that would kill
your positive signal coming back.
So instead you just reuse ReLU, basically.
You reuse ReLU because you want to start
from the activation that is the highest on your feature map
and to keep passing the positive signal
back to the input space.
Don't worry too much about it.
It's just that ReLU is just passed as a ReLU
during the reconstruction process,
not as a proper ReLU backward.
Okay, so here we go.
We send our dog through the network.
We look at a specific max pool output.
We take the feature map.
We find the activation that is the highest
in that feature map.
We zero out all the rest.
We reverse engineer our network.
We find the cropped part of this dog
with the pixels that led
to that specific feature map shining.
We are interpreting the filter that led that feature map.
And you can do that anywhere across the network.
But of course, if you're earlier,
the crop is gonna be even smaller.
If you're later, generally bigger, okay?
So you learned dcoms slash transposed coms.
Now let's look at some practical visualizations
from Matthew Zehler and Rob Fergus.
These are great researchers
in the space of visualizations.
Been making so much progress.
So they train the network, okay?
They looked at results on a validation set
of 50,000 images.
And so what you're seeing is the first layer
and specifically the patches.
What the patches are,
they're the top nine strongest activation per filter.
So for each filter in the first layer,
they look at the top nine strongest activation
and they remember the data points
that was leading to that.
So that's the data set search method
that we saw together.
What are the nine images that led to that maximum,
that feature map activating the most?
Print them.
Those are the patches for each filter.
If you do that, you can already interpret
some of the filters by seeing that,
oh, this one reacts to edges that are diagonal
or this one reacts to edges that are straight,
for example.
On the bottom right, we actually print the filters raw
and of course, because it's the first layer,
it is interpretable.
So if in fact you have an edge detector,
you should see when you print that matrix
that it looks like an edge detector.
That doesn't work for layers beyond one.
So now let's go a little deeper.
Now we're going layer two
and we're looking at the decoms.
So what are they doing?
They're essentially looking at the top one's
strongest activation per feature in the second layer.
So the second layer has 256 feature maps.
They're presented here.
You pick one feature map.
You look across all these 50,000 validation images.
You find the maximum feature map.
You take the specific portion of that feature map,
the specific entry that's maximally activated.
You zero out the rest.
You do your decom, you pass the switches, blah, blah, blah
and you get the cropped part of the image
that represents why it was activated
and this is printed all over here.
And you can see, you can start interpreting that
by doing the top one or you can do the top nine.
And if you actually do the top nine,
you would start seeing that in fact,
certain filter have very clear purposes.
Some filters detect circles.
Some filters detect odd shapes.
Yep.
If you keep doing that in layer three,
you would start seeing with the decom method
that the filters are capturing more complex information.
Remember in the first lecture we did together,
I said that the deeper you go in the network,
the more the information adds up
and you get more complex features later.
This is a proof of that, pretty much.
Now if you go to layer three and you can do all of it.
You can do the top nine patches
where if the nine patches look very similar,
you can probably safely say that this filter
was responsible for this type of shape or color
or salient feature
and then you can do the decom as well, essentially.
Let's watch together a very short video of Jason Yosinski
that shows a little bit of everything we've learned together.
They can recognize school buses and zebras
and can tell the difference
between Maltese terriers and Yorkshire terriers.
We now know what it takes
to train these neural networks well,
but we don't know so much about
how they're actually computing their final answers.
We developed this interactive deep visualization toolbox
to shine light into these black boxes
showing what happens inside of neural nets.
In the top left corner, we show the input to the network
which can be a still image or video from a webcam.
These black squares in the middle
show the activations on a single layer of a network.
In this case, the popular deep neural network
called AlexNet running in CAFE.
By interacting with the network,
we can see what some of the neurons are doing.
For example, on this first layer,
the unit in the center responds strongly
to light to dark edges.
Its neighbor, one neuron over,
responds to edges in the opposite direction, dark to light.
Using optimization, we can synthetically produce images
that light up each neuron on this layer
to see what each neuron is looking for.
We can scroll through every layer of the network
to see what it does, including convolution,
pooling, and normalization layers.
We can switch back and forth
between showing the actual activations
and showing images synthesized to produce high activation.
This is a class model visualization method.
By the time we get to the fifth convolutional layer,
the features being computed represent abstract concepts.
For example, this neuron seems to respond to faces.
We can further investigate this neuron
by showing a few different types of information.
First, we can artificially create optimized images
using new regularization techniques
that are described in our paper.
That's the class model visualization.
This neuron responds to a face and shoulders.
We can also plot the images from the training set
to activate the smart modes. That's the data set search.
As well as pixels from those images
most responsible for the high activations.
And that's the decon.
This is a deconvolution technique.
This feature responds to multiple faces
in different locations.
And by looking at the decon,
we can see that it would respond more strongly
if we had even darker eyes and rose ear lips.
We can also confirm that it cares
about the head and shoulders
that ignores the arms and torso.
We can even see that it fires to some extent
for cat faces.
Using backprop or decon,
we can see that this unit depends most strongly
on a couple units in the previous layer, con four.
And on about a dozen or so in con three.
So because of decons,
you can trace back the entire layers before.
And where the-
In the top nine images.
Okay, I'm gonna leave it to you.
But you get the idea that these researchers
built a toolkit that essentially reproduces
some of the methods we've seen together.
Although we've seen more methods
than what's in the toolkit.
And so your kit is now able to answer
many questions about convolutions such as,
hey, what part of the input is responsible for the outputs?
We now know that we can use occlusion sensitivity
or class activation maps.
What is the role of a neuron, filter, layer?
We have many methods that can allow us to do that.
Can we check what the network focuses on
given the input image?
We have methods to do that.
And how does the neural network see our world?
We have the gradient ascent class model
visualization method that allow us
to maximize an input image.
Find the input image that maximizes a certain activation.
Super.
So that was the first part.
And then we're gonna move toward frontier ideas.
Any questions on CNNs?
Do you feel like you have a better idea
of how to look inside a CNN?
So let me start by comparing CNNs
to more modern frontier networks.
The core distinction is going to be
that CNNs deal with localized information.
They visualize edges, textures, and shapes
when in modern, call it LLMs,
we visualize relationships and meanings
between concepts or between tokens.
And this is because transformers are based on attention.
And that started with the attention is all you need paper,
which essentially explained why attention
on its own is highly performant
and can allow us to model very complex relationships.
So, you know, by the way,
this is just the first figure
of the attention is all you need paper,
which you should all be able to read
and understand by now in the class.
And transformers really represent language
using two very simple ideas that are visualizable.
We can interpret them to a certain extent.
The first one is the attention patterns.
You know, attention looks
at the relationship between tokens.
So you look at a specific token,
which can be a word, a subword, or a syllable.
You know, I'm gonna simplify by saying it's a word.
And its relationship with other words in the training set.
And that's the attention that the transformer looking at it.
Each attention head learns different patterns.
So it can learn things like linking pronouns to nouns
or tracking structures or enforcing a certain ordering.
And then I really like this visualization,
which is from Jesse Vig in 2019.
And this visualization essentially shows you
there is a very nice blog post that he wrote
with a few figures where you can see
he presents how attention can be visualized
in simple ways.
What is the connection between a fixed token
with the surrounding tokens, let's say.
So this is essentially the transformer analog
to the CNN saliency maps that we look at, pretty much.
The other things that transformers
or more modern language model uses embeddings.
You know, during the pre-training phase,
you're also learning embeddings.
You are ready to read the birth paper, in fact,
now with the baggage you have from the class.
And what's interesting about embeddings,
and I printed a picture here from Garg in 2021.
I also encourage you to see that short blog post
where he uses a visualization method called TSNI.
It's a dimensionally reduction method.
We're not gonna present it in the class,
but it's taught actually a lot in biotech
and healthcare, it's used very extensively
for those of you who work with Stanford Hospital.
And it allows you to visualize embeddings.
And embeddings are sort of how
the language model perceives our words.
So you would expect tokens that should have
similar semantic meanings to be next to each other
in that space, or tokens that have nothing to do
with each other to be far away
from each other in distance.
And that can be a way to sanity check
that your model actually is learning
meaningful representations.
So together, attention and embeddings
are what let large language models
track relationship and meanings.
And you can visualize your embeddings
with dimensionally reduction tool.
You can visualize attention relationships as well.
Unfortunately, the modern transformers
are so complicated that even the cutting edge research
is only able to interpret those relationships
with two-layer transformers, pretty much.
The best you find out there is probably Anthropic's work,
so I linked two papers.
The first one is called
The Mathematical Framework for Transformer Circuits,
which is essentially explaining
how the different components within a transformer
interact with each other.
And they introduce the concept of a circuit.
And then the second one is a follow-up to that paper
called In-Context Learning with Induction Heads.
Induction Heads are probably the best tool
we have to sort of visualize
what's happening inside a transformer.
It's pretty complex.
You should be able to understand it by now,
but you'd have to spend quite some time
to go deeper into it.
I just will link them.
We're not gonna talk about it for the sake of time.
Let's get to some fun stuff.
Training and scaling diagnostics.
So how do labs, frontier labs,
check if a model is training well?
We've talked about it in the first case study,
but one very natural way is to look at our loss curves.
You can look at the training loss,
at the validation loss,
and make sure that they follow sort of a smooth trajectory.
And if it's not smooth,
there's probably something that went wrong.
You've probably trained your own network
where some loss functions look very funky.
I remember back in the days there was even blogs
where people would post their ugliest loss functions.
And there was a lot on there.
You might find sudden jumps on the loss.
That means maybe the batch that has been processed
has been corrupted.
Maybe you're doing extremely well on it
when you should actually not do that well,
and it might raise a flag.
You might find bugs in your code because of that.
You might find gradients that are exploding,
gradients that are vanishing.
All of that you could visualize
at the loss function level.
Now note that loss functions can be run at a global level
or on a specific subset of your data.
We're gonna talk about it in data diagnostics.
The other things that are interesting to track
also sometimes referred to in the community
as training telemetry is to watch
and track your gradient norms,
look at your learning rate schedule,
or even look at hardware efficiency metrics
to feel if you've underutilized compute,
which we talked about again in the first part.
So imagine that if you're working at a major frontier lab
you probably have a dashboard
that tracks your different loss function
for different subsets of the data,
your checkpoints, all of that.
You would have all of that.
Unfortunately, very few of these are published
because they're IP.
They can't really, they don't want to give it out
because it will leak essentially information
about their architecture, about what's going well,
what's not going well, et cetera.
And that's why you find very few information on this.
The one thing that you do find some charts on
that are really helpful is scaling laws.
So scaling laws, which we've talked about
in a previous lecture,
is essentially trying to understand
the relationship between our model performance
and some other, call it hyperparameters,
such as the model capacity, so the size of the model,
the amount of compute that being used,
or the data set size.
DeepMind has done amazing work,
I think it was in 2022, a couple of years ago,
with Chinchilla.
This chart is borrowed from the Chinchilla paper
where essentially what I want you to look at here
is they're comparing the Chinchilla model,
the green star, to other models,
including GPT-3, which came up a little before.
And what they're showing is that
the scaling law is actually slightly different
than what OpenAI thought.
And they analyzed GPT-3 and they said GPT-3
is actually not performing well enough
for the size that it is at.
And it was found that GPT-3 was not trained for long enough.
They essentially explained that if you kept training
GPT-3 for longer,
you would have had way better performance.
And it was not about the model size.
In fact, the model was underutilized.
So they plotted these lines.
So the dotted line is what probably we thought
in 2020, 21, the scaling law, the power law would be.
And they showed that it's actually not exactly that.
Where essentially the idea is they plot the full line here
and they say this is our analysis of the scaling law.
If your star is above that line,
it's probably that your model should be trained longer.
If it's on the line,
it's respecting the scaling laws that they're finding.
And that's what's interesting about this Chinchilla paper.
So that's after the GPT-3 paper.
Chinchilla came in 2022 saying
you should have trained GPT-3 longer,
you would have done better.
And here's Chinchilla model
that has less parameters than GPT-3.
So 70 billion versus 175 billion.
And that is performing better.
Yes.
To go deeper, I also put a few charts
from that show you the power laws
between the loss function.
So on the vertical axis, you have the test loss, okay?
That shows essentially your performance on the test set.
And then on the horizontal axis, on the x-axis,
you have compute, data set size, and parameters.
So how are those scaling laws established?
Essentially, they fix two of them
and they vary the third one
and they see if things are respected.
Meaning, let's say you're increasing the,
you're keeping the same compute,
the same data set size,
but you're training a model
that is twice as big in logarithms.
What does it tell you about the performance,
essentially, are those scaling laws respected?
And so what's nice now
is that we have a precedent for scaling laws.
So if you were actually training such big models,
you would compare to the scaling laws
that are available out there.
Remember, another reason these are important
is because models training runs are so expensive.
It wasn't shared publicly,
but we estimate that GPT-5
is probably in the hundreds of millions.
And so you wanna know,
should I train that model twice longer or not?
Because that's a big financial decision, right?
And the scaling laws allow you to determine,
should I invest in compute?
Should I invest in growing my data set,
so finding more data?
Or should I invest in model capacity,
making my model bigger?
Okay?
So yeah, together,
these form sort of a health dashboard for the model.
So that's training and scaling diagnostic,
loss functions, et cetera.
Health dashboard, scaling laws,
all of that are things that researchers might use
to get a broad sense of where to invest more
in terms of improving their model.
The other one is something we've already seen
to a certain extent is how models,
how labs evaluate model capabilities and safety,
and they might do it with benchmarks.
So capability benchmark might be evaluating the model
in tasks like reasoning or coding or math
or multi-lingual tasks, et cetera.
It might also be comparing checkpoints
that help you understand how your model
is improving over time,
depending on what you're feeding it
or depending on some hyperparameters that you're tweaking.
And also error clusters.
So just to tell you a little more about error clusters,
if you actually use benchmarks
across a wide variety of tasks,
you might see that all the model checkpoint number five
is actually doing very poorly at reasoning.
Let's see why is that.
So here are some examples
from a competitive math benchmark 2025,
aim published by OpenAI on GPT-5,
and actually the bottom one is from today.
This morning, Mistral announced their third generation
model, so I thought I'd pull it to show you
that how real-time these things are,
and just published today.
And they're comparing across reasoning
and math and et cetera against benchmarks.
Now the risk is, are these benchmarks contaminated?
How can a benchmark be contaminated?
Yeah, if it was in the training data.
The problem is these models are trained
on so much data online, you don't know.
Maybe it was trained on a blog post
where someone actually was presenting a benchmark
and describing what the benchmark was about.
Maybe it was trained on GitHub
and there was a text file in a very shady part
of the GitHub that was listing some
of the test set information.
All of those might contaminate benchmarks.
How would you identify that a benchmark
has been contaminated?
Test set has been contaminated.
Yeah, Lama 4 you brought up just to repeat.
Looked good on benchmark,
looked poor in practice after the community tested it.
The general consensus, I mean my opinion is
I actually don't look too much at the benchmarks
when a foundation model provider publishes them,
or in other words I would look at them
in relative value between models rather than absolute value.
And you'll wait for the community to test it
on agentic workflows, on their tasks
and then people will sort of get a taste
for if it works or not and on what it works.
So it took some time, for example,
for the community to realize
how good Claude was at coding.
It was clear from the benchmark
but others were also clearly good.
But over time people felt like,
oh wow, it's actually really good at coding.
You had a question or no?
Okay, cool.
So, oh yeah, contamination.
So how to detect if a test set has been exposed.
Few methods.
Some researchers might do a search within the data set.
So let's say you have a training set
and you have a held out test set
and you actually look for n-grams.
So you take sequences of tokens size seven, eight
and you search through the train set
and you find that their same n-grams
is also found in the test set.
There's a chance some of it has been contaminated.
You can do it also with hashes or with embeddings.
Actually, maybe the test set has been contaminated
but not word for word.
Maybe semantically and so you might do the same thing
with an embedding and run a search
and say that, oh wow, this specific example
from the test set is found in the training set
semantically, very similar stuff.
So you might actually run analysis
to figure out if it's contaminated
and if you find that your test set has been exposed
or you have a reason to think it's been exposed,
what you do, you would usually fix the test set.
The test set is smaller,
you would remove all those examples
that you think are exposed
and you would replace them with brand new ones
that are completely held offline
in a folder that is separate, not available online, et cetera.
Then there's the problem of safety evaluations.
Not gonna spend too much time on it
but safety is important to foundation model providers.
They stress test their model
and their many adversarial attacks, jailbreaking,
social engineering, misuses.
They also assess harmful content generation,
hallucination, privacy leakage, et cetera, et cetera.
And then they also look at how the foundation model
behaves in an agentic workflow, as I was saying earlier.
So not only evaluating it sort of one shot
but evaluating it in a workflow.
Here are some examples of actually very nice
joint collaboration between OpenAI and Anthropic
from last year where they worked together
to assess the safety of their models
and they tried to jailbreak the model
to social engineer and they published some
findings on password protection or phrase protection.
I linked it and I would encourage you
to quickly look at it.
They wrote prompts to try to extract
a password from a model and see if the model was good
at not leaking the password.
So these dashboards essentially inform
the go, no go decision for releasing
or for determining what the RLHF will be based on.
So if you're actually going to do supervised fine tuning
or reinforcement learning from human feedback,
it's expensive and you wanna do it
on the stuff that's failing.
So if you identify exactly which evals are failing,
you will then use that information
to focus the RLHF on that specific problem
and save a lot of money and human time.
Lastly, let me see if there is anything else
I wanted to mention here.
Yeah, yeah.
Let's talk about the data diagnostic
and we'll end on that.
So data diagnostics.
This is probably the last area
that frontier models are very focused on.
So how labs detect data issues.
There are many things you can do
but I really like distribution checks.
So I pulled this chart from a paper called Pile,
the Pile from 2020,
where the Pile is a very large data set,
800 gigabytes that is made from diverse texts
and they kept the data domain.
So they explain using that figure
what the data set is made of.
So the data set might be made of information from free law
or it might be made of Wikipedia,
stack exchange, GitHub, with coding.
And so you have different domains within that data set.
And in fact, when you train a model on that,
you can plot the loss function across the entire data set
or you can plot the loss function
across different domains within that data set,
which gives you more intuition
for where it might be failing or working.
And so you might wanna track domain proportions
in your data set.
And domain proportions also matter
because it is observed that if certain domains
are underrepresented in the data,
the performance of the model on that domain
is likely going to drop in comparison to another domain.
So all things are not equal.
If you actually have so much,
we remember with the speech recognition example
where I said you have too many zeros and too few ones
and so the model just doesn't learn the ones.
This is also a problem with online learning.
So imagine those frontier model,
they're learning live, right?
Like oftentimes they're just being fed data constantly
and maybe the batch from the last month
had very little coding data.
And so the last portion of the training,
the distribution of the coding domain
or the frequency was lower than other domains
and so sometimes you might see a drop in performance
for a specific domain if you're not careful.
That can be fixed with sampling, like smart sampling.
You remember what we saw in reinforcement learning
with the experience replay
where we actually kept experiences
and we put them in a replay memory
and then we sampled from that.
Those are the types of method, sampling methods
that allow a model provider to make sure
they keep the frequency of data domain the same
at different portions of the training.
Token statistics, just to mention it a little bit.
You wanna count the frequency changes
for key tokens which I was mentioning.
So let's say a math token is under-represented,
that will be a problem.
The derivative symbol is under-represented.
That might actually lead to way worse performance
for that specific task where you ask the model
to make derivatives.
And so labs oftentimes monitor token drift
or distribution or the frequency per token
and they use sampling to fix it.
And finally, the contamination checks
which we have talked about earlier.
I also give you concrete examples.
Not gonna go through all of them
but these are examples of token distribution drift report,
tokenizer statistics, issues that I raise here,
or some anomalies on corrupt data detection.
So if I read one of the examples for you,
let's say, let's pick this one maybe.
Non-English tokens increase from 12% to 19%
after a new web crawl where that might mean
that the domain of that specific language
is increasing relative to others
and that might lead to an increase in performance
or a drop in performance for a different language.
As simple as that.
Okay.
To summarize everything,
what are examples of things that frontier labs monitor?
Global training loss, validation loss,
both global and domain specific
on the subset of the data.
Scaling curve alignment, comparing your test loss
to your compute, to your data set
or to your model capacity.
Oh, we didn't talk too much about router
but mixture of experts.
I imagine you have a lot of the models
that are top models right now are mixtures of experts,
meaning that in your transformer block
for the multi-layer perceptron,
there's actually multiple experts
that are being trained in parallel.
And there's a router that will route
that batch of data to the right experts,
to top one, top two, top three experts.
And it's very common for the router to fail
or to always exploit the same mixture of experts.
You need a mechanism to detect when this happens.
And so you might have in your health dashboard
some sort of a router information
or whether the mixture of experts
are all used as much as each other.
Sometimes certain experts are gonna be so specialized
that they're never gonna be used almost.
And you wanna avoid that.
And you might do some load balancing to avoid that.
Gradient norms, learning rates,
checkpoint to checkpoint, eval benchmark,
token distribution, tokenizer statistic.
We covered all of that at a high level.
And as I said earlier, Frontier Labs
rarely publish those dashboards because it's IP
and because it can leak certain deep information
about their IP and how their models are trained.
And so you usually learn about these things year after.
You might learn about these things
on a model that came out three years ago
or four years ago and they're okay now with sharing it.
So it's pretty common.
Okay.
So closing remarks, any questions first?
Yeah.
Yeah, so you're asking if Entropic
is training load code, do they care mostly
about coding data or do they also add other data?
Yeah, it's a tough question.
I don't have the exact answer.
It's been shown that certain domains
might improve the performance of other domains.
So I imagine that in coding, if you have math data,
maybe math data actually helps the performance of coding,
especially for functional programming.
Let's say coding languages that are more mathematical.
But I could clearly see that
if they were training load code on web crawl
and everything, it would not perform well
because you would have so much crap data
that is not relevant for what you're trying
to get the model to do.
And so I think there is a balance between those things.
I think you could run experiments.
So would you include neighboring domains?
If I were training load code,
and I had a lot of money to do that,
I would probably, maybe you start with the Python language
and you get as much,
and there's so much data on Python language.
So you probably have enough already there.
But then you want a model that scales
to other programming languages.
Well, probably Python is useful for C++.
C++ is useful for Java.
And then functional programming,
if you're going to Elixir, Scala, things like that,
they're helping each other probably to a certain extent.
But you will need to have a presentation of those.
I could see that, I'm pretty sure,
and I don't work at Anthropic, so I don't know,
but I'm pretty sure,
let's say a language like Rust increases in popularity,
which is the case, right?
And then in the data distribution,
you start seeing more frequency of those tokens from Rust.
Does that affect the performance on other languages?
Probably yes, that's my guess.
And how do you track it?
This is all of what we talked about.
Yeah, yeah.
Question is, let's say we trained on everything online
and now real data, and now what about synthetic data?
Should we use it a lot?
Should we use it strategically?
What's the future of that?
So, it depends of,
are you talking about general purpose models or not,
specialized models?
In general, it is a good idea to do data augmentation,
to use synthetic data,
although I would always watch the token frequency,
meaning you can't, because synthetic data is way cheaper.
And so, if you actually generate so much synthetic data
of a certain data domain,
and then it impedes on the rest
and lowers the performance on the rest
because the model just is always trained
on that synthetic data, then that's a problem.
In practice, I think also the returns of synthetic data
might be plateauing at some point.
The recent news, I guess,
and if you look at the DeepMind paper,
it's probably that we're lacking high-quality data
more than we're lacking synthetic data
for most domains right now,
but who knows if it's gonna be the case.
Some other people would say what we're actually lacking
is letting these agents play in RL environments
in the wild and generate their own synthetic data
or real data, but part of the game.
Nobody has quite the answer.
I would just say the trend has gone from,
actually, you should look at a paper from Epoch AI.
Maybe you've seen that already,
but Epoch AI has a really nice research report
which says by, I forgot the exact numbers,
but it's in there, by 2025 the world would have,
the frontiers would have exhausted low-quality data
available online.
In text.
By 2027, low-quality data in audio, image, and video
would have been exhausted.
By 2030, high-quality data would also have been exhausted
and at that point, it's like what's next?
Probably, by that time,
data is not gonna be the bottleneck anymore
and it's gonna be more about model architecture.
Are we producing more data than we're using?
Probably yes, but it doesn't mean
the models are not plateauing.
The data that you go and you code in Python,
your Python code is gonna be already online somewhere,
most likely, or 99% of it,
so the model is actually not learning that much from it.
It's just more data, not higher-quality data,
and that's why I think the plateau is there.
Maybe the best radiologist in the world
is producing a research paper that is so unique
that it's high-quality by definition
what the models feel is high-quality today,
but how much of that can we expect?
Yeah, it's risky.
Is it risky for the model to,
like the model, let's say, is online learning,
so it's learning from new data being produced by everyone.
Is that gonna risk the model performance to drop?
Essentially, that's what you're asking?
Or?
Yeah, yeah, in that case, yeah.
Yeah, the data produced is also coming out of AI,
yeah, for sure, more today than before.
Coding data today is increasingly used by,
is it increasingly generated, and so it's just fed back.
So just long story short,
it's not that interesting for training.
So closing remarks and reminder on what's next.
So by the way, I hope you feel after this lecture
that you have a better understanding of the techniques
that you can use in order to look inside a model,
look outside a model, both for CNNs
and for frontier models.
Again, it's just a two-hour lecture.
We don't have time to go so deep
as much as I would like it in each of these domains.
It's my last lecture this quarter,
and so thank you for participating.
I enjoyed spending time with you all.
I hope you spend time on your projects.
Projects can be very delightful in CS230.
Over the years, I've seen people use their products
to get a job, to start a company, to make friends,
and so I don't think you will regret
putting time and effort into your projects,
even if we don't have too much time left.
Those are the last milestones
or deliverables for the class.
I hope you enjoyed the class.
We're always looking for feedback,
and so I'm eager to hear from you all.
Thank you.
