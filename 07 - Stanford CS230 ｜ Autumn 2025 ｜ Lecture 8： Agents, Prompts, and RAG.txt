Hi, everyone. Welcome to another lecture for CS230 Deep Learning. Today, we're going to
talk about enhancing large language model applications. And I call this lecture Beyond
LLM. It has a lot of newer content. And the idea behind this lecture is we started
to learn about neurons, and then we learned about layers, and then we learned about deep
neural networks. And then we learned a little bit about how to structure projects in C3.
And now we're going one level beyond into what would it look like if you were building
agent AI systems at work, in a startup, in a company. And it's probably one of the
more practical lectures. Again, the goal is not to build a product end to end in the
next hour or so, but rather to tell you all the techniques that AI engineers have
cracked, figured out or exploring, so that after the class, you have sort of the breadth
of view of different prompting techniques, different agent workflows, multi agency stands,
evals. And then when you want to dive deeper, you have the baggage to dive deeper
and learn faster about it. Okay, let's try to make it as interactive as possible as
usual. When we look at the agenda, the agenda is going to start with the core idea
behind challenges and opportunities for augmenting LLMs. So we start from a base model,
how do we maximize the performance of that base model, then we dive deep into the
first line of optimization, which is prompting methods, and we see a variety of them. Then
we go slightly deeper if we were to get our hands under the hood and do some fine
tuning, what would it look like? I'm not a fan of fine tuning. And I talk a lot about
that. But I'll explain why I try to avoid fine tuning as much as possible. And then
we'll do a section four on retrieval, augmented generation or RAG, which you've
probably heard of in the news. Maybe some of you have played with RAGs, we're going to sort of
unpack what a RAG is and how it works and then the different methods within RAGs. And then we
talk about agent AI workflows. I define it, Andrew Eng is one of the colleagues, first ones
who have called this trend a agent AI workflows. And so we look at the definition that Andrew
gives to agent AI workflows. And then we start seeing examples. The section six is very practical.
It's a case study where we will think about an agent AI workflow and we'll ask you to measure
if the agent actually works. And we brainstorm how we can measure if an agent AI workflow
is working the way you want it to work. There's plenty of methods called evals that
solve that problem. And then we look briefly at multi-agent workflow and then we can have a
sort of open-ended discussion where I'll share some thoughts on what's next in AI.
And I'm looking forward to hearing from you all as well on that one.
Okay, so let's get started with the problem of augmenting LLM. So open-ended question for you.
You are all familiar with pre-trained models like GPT 3.5 turbo or GPT 4.0.
What's the limitation of using just a base model? What are the typical issues that
might arise as you're using a vanilla pre-trained model? Yes. Lacks some domain
knowledge. You're perfectly right. You know, you, we had a group of students a few years ago,
was not LLM related, but you know, they were building an autonomous farming device or vehicle
that had a camera underneath taking pictures of crops to determine if the crop is sick or not,
if it should be thrown away, like if it should be, if it should be used or not.
And that data set is not a data set you find out there. And the base model or a pre-trained
computer vision model would lack that knowledge, of course. What else? Yes. Okay. Maybe the,
you're saying, so just to repeat for people online, you're saying the model might have
been trained on high quality data, but the data in the wild is actually not that high quality.
And in fact, yes, the distribution of the real world might differ as we've seen with GANs
from the training sets. And that might create an issue with pre-trained models. Although
pre-trained LLMs are getting better at, you know, handling all sorts of data inputs. Yes.
Like what? Lacks current information. The LLM is not up to date. And in fact,
you're right. Imagine you have to retrain from scratch your LLM every couple of months.
One story that I found funny, it's from probably three years ago, or maybe more five
years ago, where during his first presidency, President Trump one day tweeted Cove Fefe. You
remember that tweet or no? Just Cove Fefe. And it was probably a typo or it was in his pocket.
I don't know. But that word did not exist. The LLMs, in fact, that Twitter was running
at the time, could not recognize that word. And so the recommender system sort of went
wild. Because suddenly everybody was making fun of that tweet using the word Cove Fefe.
And the LLM was so confused on, you know, what does that mean? Where should we show it? To whom
should we show it? And it's an example of nowadays, especially on social media, there's
so many new trends. And it's very hard to retrain an LLM to match the new trend and
understand the new words out there. I mean, you know, you oftentimes hear Gen Z words like
or mid or whatever, I don't know all of them. But you probably want to find a way that
can allow the LLM to understand those trends without retraining the LLM from scratch. What
else? It's trained to have a breadth of knowledge. Yeah, it might be trained on a
breadth of knowledge, but it might fail or not perform adequately on a narrow task that
is very well defined. Think about enterprise applications that, yeah, enterprise application,
you need high precision, high fidelity, low latency, and maybe the model is not great
at that specific thing. It might do fine, but just not good enough. And you might want to
augment it in a certain way. Yeah. So maybe it has a lot of broad domain knowledge that
might not be needed for your application. And so you're using a massive heavy model
when you actually are only using 2% of the model capability, you're perfectly right.
You might not need all of it. So you might find ways to prune, quantize the model,
modify it. All of these are good points. I'm going to add a few more as well.
LLMs are very difficult to control. Your last point is actually an example of that. You want
to control the LLM to use a part of its knowledge, but it's not. It's in fact getting
confused. We've seen that in history. In 2016, Microsoft created a notorious Twitter bot
that learned from users and it quickly became a racist jerk. Microsoft ended up removing the
bot 16 hours after launching it. The community was really fast at determining that this was a
racist bot. And you can empathize with Microsoft in the sense that it is actually
hard to control an LLM. They might have done a better job to qualify before launching,
but it is really hard to control an LLM. Even more recently, this is a tweet from Sam Altman
last November, where there was this debate between Elon Musk and Sam Altman on whose
LLM is the left-wing propaganda machine or the right-wing propaganda machine. And they
were hating on each other's LLMs, but that tells you at the end of the day that even those
two teams, Grok and OpenAI, which are probably the best-funded team with a lot of talent,
are not doing a great job at controlling their LLMs.
And from time to time, if you hang out on X, you might see screenshots of users
interacting with LLMs and the LLMs saying something really controversial or racist or
something that would not be considered great by social standards, I guess. And that tells
you that the model is really hard to control. The second aspect of it is something that you've
mentioned earlier. LLMs may underperform in your task, and that might include specific
knowledge gaps such as medical diagnosis. If you're doing medical diagnosis, you would rather
have an LLM that is specialized for that and is great at it. And in fact, something that we
haven't mentioned as a group has sources. So the answer is sourced specifically. You have a hard
time believing something unless you have the actual source of the research that backs it up.
Inconsistencies in style and format. So imagine you're building a legal AI
agentic workflow. Legal has a very specific way to write and read where every word counts.
If you're negotiating a large contract, every word on that contract might mean something else
when it comes to the court. And so it's very important that you use an LLM that is very good
at it. The precision matters. And then task-specific understandings such as doing
a classification on a niche field. Here I pulled an example where let's say a biotech
product is trying to use an LLM to categorize user reviews into positive, neutral, or
negative. You know, maybe for that company something that would be considered a negative
review typically is actually considered a neutral review because the NPS of that industry
tends to be way lower than other industries, let's say. That's a task-specific understanding
and the LLM needs to be aligned to what the company believes is the categorization that it
wants. We will see an example of how to solve that problem in a second.
And then limited context handling. A lot of AI applications, especially in the enterprise,
have required data that has a lot of context. Just to give you a simple example,
knowledge management is an important space that enterprises buy a lot of knowledge management
tools. When you go on your drive and you have all your documents, ideally you could have
an LLM running on top of that drive. You can ask any question and it will read immediately
thousands of documents and answer what was our Q4 performance in sales? It was X dollars.
It finds it super quickly. In practice, because LLMs do not have a large enough context,
you cannot use a standalone vanilla pre-trained LLM to solve that problem.
You will have to augment it. Does that make sense? The other aspect around context windows
is they are in fact limited. If you look at the context windows of the models from
the last five years, even the best models today will range in context window or number
of tokens it can take as input, somewhere in the hundreds of thousands of tokens max.
Just to give you a sense, 200,000 tokens is roughly two books. That's how much you can upload
and it can read pretty much. You can imagine that when you're dealing with video understanding
or heavier data files, that is of course an issue. You might have to chunk it. You
might have to embed it. You might have to find other ways to get the LLM to handle
larger contexts. The attention mechanism is also powerful but problematic because it does not
do a great job at attending in very large contexts. There is actually an interesting
problem called needle in a haystack. It's an AI problem or call it a benchmark where
in order to test if your LLM is good at putting attention on a very specific fact
within a large corpus, researchers might randomly insert in a book one sentence that
outlines a certain fact, such as Arun and Max are having coffee at Blue Bottle in the middle
of the Bible, let's say, or some very long text. Then you ask the LLM what were Arun and
Max having at Blue Bottle and you see if it remembers that it was coffee. It's actually
a complex problem not because the question is complex but because you're asking the model to
find a fact within a very large corpus and that's complicated. Again, this is a limiting
factor for LLMs. We'll talk about RAG in a second but I want to preview. There is
debates around whether RAG is the right long-term approach for AI systems. As a high level idea,
RAG is a mechanism, if you will, that embeds documents that an LLM can retrieve
and then add as context to its initial prompt and answer a question. It has lots of
applications. Knowledge management is an example. Imagine you have your drive again but every
document is sort of compressed in representation and the LLM has access to that lower dimensional
representation. The debate that this tweet from Yao Fu outlines is in theory, if we have
infinite compute, then RAG is useless because you can just read a massive corpus immediately
and answer your question. Even in that case, latency might be an issue. Imagine the time it
takes for an AI to read all your drive every single time you ask a question. It doesn't make
sense. RAG has other advantages beyond even the accuracy. On top of that, the sourcing
matters as well. RAG allows you to source. We'll talk about all that later. There's always
this debate in the community whether a certain method is actually future proof because in practice
as compute power doubles every year, let's say, some of the methods we're learning right now
might not be relevant three years from now. We don't know essentially. The analogy that
he makes on context windows and why RAG approaches might be relevant even a long time
from now is search. When you search on a search engine, you still find sources of information.
In fact, in the background, there is very detailed traversal algorithms that rank and find
the specific links that might be the best to present you versus if you had to read,
imagine you had to read the entire web every single time you're doing a search query
without being able to narrow to certain portion of the space that might again not be reasonable.
When we're thinking of improving LLMs, the easiest way we think of it is two dimensions.
One dimension is we are going to improve the foundation model itself. For example,
we move from GPT 3.5 turbo to GPT 4 to GPT 4.0 to GPT 5. Each of that is supposed to improve the
base model. GPT 5 is another debate because it's sort of packaging other models within itself,
but if you're thinking about 3.5, 4, and 4.0, that's really what it is. The pre-trained
model improves, and so you should see your performance improve on your tasks.
The other dimension is we can actually engineer, leverage the LLM in a way that makes it better.
You can prompt simply GPT 4.0. You can change some prompts and improve the prompt, and it
will improve the performance. It's shown. You can even put a rag around it. You can
put an agentic workflow around it. You can even put a multi-agent system around it,
and that is another dimension for you to improve performance. That's how I want you to think about
it. Which LLM I'm using, and then how can I maximize the performance of that LLM?
This lecture is about the vertical axis. Those are the methods that we will see together.
Sounds good for the introduction. Let's move to prompt engineering.
I'm going to start with an interesting study just to motivate why prompt engineering matters.
There is a study from HPS, UPenn, as well as Harvard Business School, and others also in
Wharton that took a subset of BCG consultants, individual contributors, and split them into three
groups. One group had no access to AI. One group had access to, I think it was GPT 4.0,
and then one group had access to the LLM, but also a training on how to prompt better.
Then they observed the performance of these consultants across a wide variety of tasks.
There are a few things that they noticed that I thought was interesting.
One is something they call the JAG frontier, meaning that certain tasks that consultants are
doing fall beyond the JAG frontier, meaning AI is not good enough. It's not improving
human performance. In fact, it's actually making it worse. Some tasks are within the frontier,
meaning that AI is actually significantly improving the performance, the speed, the quality
of the consultants. Many tasks fell within and many tasks fell without, and they shared their
insights, but the TLDR is there is a frontier within which AI is absolutely helping and one
where they call out this behavior of falling asleep at the wheel, where people relied on AI
on a task that was beyond the frontier. In fact, it ended up going worse because the human
was not reviewing the outputs carefully enough. They did note that the group that was trained was
the best, better than the group that was not trained on prompt engineering,
which also motivates why this lecture matters so that you're within that group afterwards.
One other insight were the centaurs and the cyborgs. They noticed that consultants had the
tendency to work with AI in one of two ways, and you might yourself be part of one of these
groups. The centaurs are mythical creatures that are half-human, half-horses, half-something,
and those were individuals that would divide and delegate. They might give a pretty big
task to the AI, so imagine you're working on a PowerPoint, which consultants are known to do.
You might actually write a very long prompt on how you want it to do your PowerPoint,
and then let it work for some time, and then come back and it's done.
When others would act as cyborgs, cyborgs are fully blended, bionic human robots,
and robots augmented with robotic parts, and those individuals would not delegate fully a
task. They would actually work super quickly with the model and back and forth. I find that
a lot of students are actually more working like cyborgs than centaurs, but why maybe in the
enterprise when you're trying to automate the workflow, you're thinking more like a centaur.
That's just something good to keep in mind. Also, a lot of companies will tell you,
we're hiring prompt engineers, etc. It's a curer. I don't buy that. I think it's just a skill
that everybody should have. You're not going to make a curer out of prompt engineering,
but you're probably going to use it as a very powerful skill in your curer.
Let's talk about basic prompt design principles. I'm giving you a very simple prompt here.
Summarize this document, and then the document is uploaded alongside it,
and the model has not much context around what should be the summary, how long should
be the summary, what should it talk about, etc. You can actually improve these prompts
by doing something like summarize this 10-page scientific paper on renewable energy in five
bullet points focusing on key findings and implications for policymakers. That's already
better. You're sharing the audience, and it's going to tailor it to the audience. You're
saying that you want five bullet points, and you want to focus only on key findings.
That's a better prompt, you would argue. How could you even make these prompts better?
What are other techniques that you've heard of or tried yourself that could make this
one-shot prompt better? Write examples. Here is an example of a great summary.
Yeah, you're right, that's a good idea.
Very popular technique. Act like a renewable energy expert giving a conference at Davos,
let's say. Yeah, that's great. You are the best in the world at this.
Explain. Yeah, actually, these things work. It's funny, but it does work to say act
like XYZ. It's a very popular prompt template. We see a few examples. What else could you do?
Yes. Critique your own project, so you're using reflection. You might actually do one output
and then ask it to critique it and then give it back. Yeah, we see that. That's a great
one. That's the one that probably works best within those typically, but we see some
examples. What else? Yeah, okay. Break the task down in two steps. Do you know how that
is called? Chain of thoughts. This is actually a popular method that's been shown in research
that it improves. You could actually give a clear instruction and also encourage the
model to think step-by-step. Approach the task step-by-step and do not steep any step.
Then you give it some steps such as step one, identify the three most important findings.
Step two, explain how key each finding impacts renewable energy policy. Step three,
write the five bullet summary with each point addressing a finding, etc. Chain of thoughts.
I linked the paper from 2023 that popularized chain of thoughts. Chain of thoughts is very
popular right now, especially in AI startups that are trying to control their LLMs.
To go back to your examples about act like XYZ, what I like to do,
Andrew also talks about that, is to look at other people's prompts. In fact, online you have a lot
of prompt repositories for free on GitHub. In fact, I linked the awesome prompt template repo
on GitHub where you have so many examples of great prompt that engineers have built.
They said it works great for us and they published it online. A lot of them starts
with act as, act as a Linux terminal, act as an English translator, act like a position
interviewer, etc. The advantage of a prompt template is that you can actually put it in your
code and scale it for many user requests. Let me give you an example from Workera.
Workera evaluates skills. Some of you have taken the assessments already
and tries to personalize it to the user. In fact, if you actually read in an HR system
in an enterprise in the HR system, you might have Jane is a product manager level three
and she is in the US and her preferred language is English. Actually, that metadata
can be inserted in a prompt template that we personalize for Jane. Similarly, for Joe,
whose preferred language is Spanish, it will tailor it to Joe and that's called a prompt template.
The question is, do the foundation models use a prompt template or do you have to
integrate it yourself? The foundation models probably use a system prompt that you don't see.
When actually you type on chat GPT, it is possible, it's not public, that OpenAI
behind the scene act like a very helpful assistant for this user. By the way,
here is your memories about the user that we kept in our database. You can actually check
your memories and then your prompt goes under and then the generation starts. Probably they're
using something like that, but it doesn't mean you can't add one yourself. In fact,
if you think about a prompt template for the work here example I was showing,
maybe it starts when you call OpenAI by act like a helpful assistant and then underneath,
it's like act like a great AI mentor that helps people in their career and OpenAI's
prompt template also has follow the instruction from the creator or something like that.
It's possible. Questions about prompt templates? Again, I would encourage you to go and read
examples of prompts. Some of them are quite thoughtful. Let's talk about zero-shot versus
few-shot prompting. It came up earlier. Here's an example again going back to the
categorization of product reviews. Let's say that we're working on a task where the prompt
is classified the tone of the sentence as positive, negative, or neutral and then you
paste the review, which is the product is fine, but I was expecting more. If I were to survey
the room, I would bet that some of you would say it's negative. Some of you would say it's
neutral because you actually have a first part that is relatively positive. It's fine.
And then the second part I was expecting more, which is relatively negative. So where do you
land? This can be a subjective question and maybe in one industry this would be considered
amazing and in another one it would be considered really bad because people are used to really
flourishing reviews. The way you can actually align the model to your task is by converting
that zero-shot prompt. Zero-shot refers to the fact that it's not beginning given any example
into a few-shot prompt where the model is given in the prompt a set of examples to
align it to what you want it to do. The example here is again you paste the same prompt
as before with the user review and then you add here are examples of tone classifications.
This exceeded my expectation completely. Positive. It's okay, but I wish it had more features.
Negative. The service was adequate, neither good nor bad. Mutual. Now classify the tone of
this sentence after you've heard about these things and the model then says
negative and the reason it says negative of course is likely because of the second example
which was it's okay but I wish it had more features which we told the model that was
negative because the model saw that it's aligned now with your expectations.
Few-shot prompts are very popular and in fact for AI startups that are slightly more sophisticated
you might see them keep a prompt up to date whenever a user says something and they might
have a human label it and then add it as a few shots in their relevant prompts in their code
base. You can think of that as almost building a data set but instead of actually building a
separate data set like we've seen with supervised fine-tuning and then fine-tuning
the model on it you're just putting it directly in the prompt and turns out it's probably
faster to do that if you want to experiment quickly because you don't touch the model
parameters you just update your prompts and you know if it's text examples you can actually
you know concatenate so many examples in a single prompt. At some point it will be too long
and you will not have the necessary context window but it's a pretty strong approach
that is quick to align an LLM. Okay, yes. So the question was is there any research
on how long the prompt can be before the model essentially uses itself or doesn't follow
instructions anymore? There is. The problem is that research is outdated every few months
because models get better and so I don't know where the state of the art is you can probably
find it online on benchmarks on like we see that I use an example on the Workera product
you have a voice conversation for some of you that have tried it where you you're asked
explain what is the prompt and then you explain and then there's a scoring algorithm in the
line. We know that after eight turns the model loses itself after eight turns because
you always paste the previous user response it just starts going wild and so the techniques
we use in the background is we actually create chapters of the conversation maybe one chapter
is the first eight prompt and then you actually start over from another prompt you can summarize
the first part of the conversation insert the summary and then keep going you know those are
engineering hacks that engineers might have figured out in the background yeah because
the eight turns makes a prompt quite long actually. Let's move on to chaining. Chaining
is the most popular technique out of everything we've seen so far in prompt engineering.
It's not chain of thought so chain of thought we've seen is think step by step step one step two
step three do not skip any step this is different this is chaining complex prompt
to improve performance and this is what it looks like. You take a single step prompt such
as read this customer review and write a professional response that acknowledges their
concern explains the issue offers a resolution and then you paste the customer review which is
I ordered a laptop it arrived three days late the packaging was damaged very disappointing
I needed that urgently for work and then the output is an email that is immediately given to
you by the LLM after it reads the prompt. So this might work but it might be hard to
control you know because think about it there's multiple steps that you have listed and
everything is embedded in the same prompt and if you wanted to debug step by step and know which
step is weaker you couldn't you would have everything mixed together. So one advantage of
chaining is you know you would you would separate the prompts so that you can debug
them separately and it will also lead to an easier manner to improve your workflow.
Let's say a first prompt is extract the key issues identify the key concerns mentioned in
this customer review paste a customer review. Second prompt using these issues so you paste back
the issues draft an outline for a professional response that acknowledges concerns explains
possible reasons and offer a resolution. So this is not you know prompt number three
write the full response so using the outline write the professional response
and then you get your final output. So in theory you can't tell me oh the second approach is
better than the first one at first but what you can notice is that we can actually test those
three prompts separately from each other and determine if we will get the most gains out of
engineering the first prompt optimizing it or the second one or the third one. We now have
three prompts that are independent from each other and you know maybe if the outline was
better the performance of the email the email how much it will the open rate will be or the
user satisfaction on the response will actually get higher you know and so chaining improves
performance but most importantly helps you control your workflow and debug it more seamlessly.
Yes so let me try to rephrase you say let's say we look at the first prompt which has
all three tasks built in that prompt but what exactly you mean you mean like if we evaluate
the output and we measure some user insight satisfaction etc why don't we just modify that
prompt and essentially see how it improves user satisfaction. Yeah it's not going to give you
that process. I see why do we need the three steps yeah I mean think about it the intermediate
output is what you want to see like if I'm debugging the first approach the way I would
do it is I would capture user insights like here's the email how good was the response
thumbs up thumbs down was your issue resolved thumbs up thumbs down those would tell me how
good is my prompt and I can engineer that prompt optimize it and I would probably drive some gains
but I will not be able easily to trace back to what the problem was while in the second
approach not only I can use the end-to-end metrics to improve my process I can also
use the intermediate steps for example if I look at prompt two and I look at the outline
and I see the outline is actually meh is not great then I think I can get a lot of gains
out of the outline or the outline is actually really good but the last prompt doesn't do a good
job at translating it into an email so the outline is exactly what I want the LLM to do
but the the translation in a customer facing email is not good in fact it doesn't follow
our vocabulary internally internally then I know the third prompt is where I would get
the most gains so that that's what it allows me to do have intermediate steps to review
yeah we'll talk about it are there any latency concerns yes in certain applications
you don't want to use a chain or you don't want to use a long chain because it adds latency
we'll talk about that later good point so practically this is what chaining complex
prompts look like you have your first prompt with your first task it outputs the output is
pasted in the second prompt with the second task being defined the output is then pasted
into the third prompt with the third task being defined and so on that's what it looks like in
practice super we'll talk more later about testing your prompts but there are methods
now to do it and we will see later in this lecture with our case study how we can test
prompts but here is an example of how you might do it you might have a
summarization workflow you know prompts that is the baseline it's a single prompt you might
have a refined summarization which is a modified prompt of this or workflow with a chain you
know and then you have your test case which is the input that you want to summarize let's say
and then you have the generated outputs and you can have humans go and rate these outputs
and you would notice that the baseline is better or worse than the refined prompt of course
this manual approach takes time but it's a good way to start and usually the advice is
get hands-on at the beginning because you would quickly notice some issues and it will give you
better intuition on what tweaks can lead to better performance however if you wanted to scale that
system across many products many parts of your code base you might want to find a way to do
that automatically without asking humans to review and grade summaries right one approach
is to use you know platforms like at Porchera our team uses a platform called TrumpFoo that
allows you to actually automate part of this testing in a nutshell what it does is it can
allow you to run the same prompts with five different LLMs immediately put everything in
a table that makes it super easy for a human to grade let's say or alternatively it might
allow you to define define LLM judges LLM judges can come in different flavors
for example I can have an LLM judge that does a pairwise comparison so what the LLM is asked to
do is here are two summaries just tell me which one is better than the other one
that's what the LLM does and that can be used as a proxy for how good the summarization
baseline versus the refined version is another way to do an LLM judge is if you do it for a
single answer grading so here's a summary rated from one to five you know and then you can go
even deeper and do a reference guided pairwise comparison or you add also a rubric you say
a five is when a summary is below a hundred characters I'm just making up below a hundred
characters mentions at least three key points that are distinct and starts with a first
sentence that displays the overview and then goes into detail that's a great summary number
five out of five zero is the LLM failed to summarize and actually was very verbose let's
say and so you put a rubric behind it and you have an LLM as just finding the rubric of course
you can now pair different techniques you can do a few shots for the rubric you can actually
give examples of them five out of fives four out of fours three out of threes because now
you know multiple techniques okay does that make sense okay so that was the second section
on prompt engineering or the first line of optimization now let's say you've exhausted
all your chances for prompt engineering and you're thinking about actually touching the
model modifying its weights or fine tuning it in other words I was telling you I'm not a fan
of fine tuning there's a few reasons why one it requires substantial labeled data typically
to fine tune although now there are approaches that are getting better at fine tuning that look
more like few shot prompting actually than fine tuning it's sort of merging although one
modifies the weight the other doesn't modify the weights fine tune models may also overfeed
to specific data we're going to see a funny example actually losing their general purpose
utility so you might fine tune a model and actually when someone asks a pretty generic
question it doesn't do well anymore you know it might do well on your task so it might be
relevant or not and then it's it's time and cost intensive that's my main problem and you
know I work here we don't we don't we steer away from fine tuning as much as possible
because by the time you're done fine tuning your model the next model is out and it's actually
beating your fine-tuned version of the previous model so I would steer away from fine tuning
as much as you can the advantage of the prompt engineering methods we've seen is you can put
the next best pre-trained model directly in your code it will update everything immediately
fine tuning doesn't work like that there are advantages though where it still makes sense
if the task requires repeated high precision outputs such as legal scientific explanation
and if the general purpose llm struggles with domain specific language so let's look at a
quick example together which is an example from ross lazarovitz I think it was a couple of
years ago september 23 where ross tried to do slack fine tuning so he looked at a lot of slack
messages within his company and he was like I'm going to fine tune a model that speaks like us
or operates like us because this is how we work right this is the data that represents how
people work at the company and so if he actually went ahead and fine tuned the model
gave it a prompt like hey write a you know he was delegating to the model write a 500 word
blog post on prompt engineering and the model responded I shall work on that in the morning
and and then he tries to push the model a little further and say it's morning now and
the model said I'm writing right now it's 6 30 a.m here write it now okay please
okay I shall write it now I actually don't know what you would like me to say about prompt
engineering I can only describe the process the only thing that comes to mind for a headline
is how do we build prompt you know it's kind of a funny example for fine tuning because
it's true that it went wrong like he was supposed to think like I want the model to
speak like us at work and it ended up acting like people and not actually following
instructions so one example why I would steer away from fine tuning super
let's talk about rags rags is important it's important to know out there and at least having
the basics it's a very common interview question by the way if you go interview
for a job they might ask you to explain in a nutshell to a five-year-old what is a rag
and hopefully after that you'll be able to do it so we've seen some of the challenges
with standalone LLMs those challenges include the context window being small the fact that
it's hard to remember details within a large context window knowledge gaps you know cutoff
dates you mentioned earlier the model might be trained up to a date and then it cannot
follow the trends or be up to date hallucinations there are some fields think about medical
diagnosis where hallucination are very costly you can't afford a hallucination you know even
in education imagine deploying a model for the us youth education and it hallucinates and it
teaches millions of people something completely wrong it's a problem and then lack of sources
a lot of fields love sources research fields love sources education love sources legal
loves sources as well and so the pre-trained LLM doesn't do a good job to source and in fact
if you if you have tried to find sources on a plain LLM it actually hallucinates a lot it
makes up research papers it just lists like completely fake stuff so how do we solve that
with a rag rag integrates with external knowledge sources databases documents
apis it ensures that answers are more accurate up to date and grounded because you can actually
update your document your drive is always up to date i mean ideally you're always pushing
new documents to it and when you query what is our queue for performance in sales hopefully
there is the last board deck in the drive and it can read the last board deck you know
and more developer control we'll see why rags allow for targeted customization without actually
requiring the retraining of the model in fact you don't touch the model with rags
it's really a technique that is put on top of the model so to see an example of a rag
this is a question answering application where we're in the medical field and a user is asking
a query what are the side effects of drug x is an important question you can't hallucinate you
need to source you need to be up to date maybe there is a new update to that drug that is now
in the database and you need to read that so you have to like a rag is a great example of
what you would want to use here the way it works is you have your knowledge base of a bunch of
documents what you do is you use an embedding to embed those documents into lower dimensional
representations so for example if the document is a pdf a long pdf you might you know read
the pdf understand it and then embed it we've seen plenty of embedding approaches together
triplet loss etc you remember so imagine one of them here for lms is embedding those documents
into lower representation if the representation is too small you will lose information if it's
too big you will add latency right so trade-off you will store typically those representation
into a database called a vector database there's a lot of vector database providers
out there you know i think i've listed a couple that are very common no i haven't listed but i
can i can share afterwards the vector database is essentially storing those vector in a very
efficient manner allowing the fast retrieval with a certain distance metric so what you
do is you also embed usually with the same algorithm the user prompts and you run a retrieval
process which is essentially saying based on the embedding from the user query and the vector
database find the relevant documents based on the distance between those embeddings
once you found the relevant documents you pull them and then you add them to the user query
with a system prompt or a prompt template on top so the prompt template can be answer user
query based on list of documents if answer not in the document say i don't know that's your
prompt templates where the user query is pasted the documents are pasted and then your output
should be what you want because it's now grounded in the documents you can also add
to these prompt templates tell me the exact page chapter line of the document that was
relevant and in fact link it as well just to be more precise any question on rags is a simple
vanilla rag yeah yes question is do the document embeddings still retain information of the
location of the information within that document especially in big documents
great question we get to it in the in a second because you're right that the vanilla
rag might not do a good job with very large documents so let's say you know when you open
a medication box and you have this gigantic white paper with all the information and it's
very long maybe a vanilla rag would not cut it so what people have figured out is a bunch
of techniques to improve rags and in fact chunking is a great technique that is very
popular so you might actually store in the vector database the embedding of the full
documents and on top of that you will also store a chapter level vector you know and when
you retrieve you retrieve the document you retrieve the chapter and that allows you to be
more precise with the sourcing it's one example another technique that's popular is hide
hypothetical document embeddings where a group of researchers published a paper showing that
when you get your user query one of the main problem is the user query actually does not look
like your documents for example the user query might be what are the side effects of drug x
when actually in the document in the vector database the vectors will present very long
documents so how do you guarantee that the vector embedding is going to be close to the
document embedding what they do is they use the user query to generate a fake hallucinated
document they embed that document and then they compare to the vector in the vector database
that makes sense so for example the user says what is the side effect of drug x there's a
prompt that this is given to another prompt that says based on this user query generates
a five page report answering the user query it generates potentially a completely fake answer
you embed that and it will be closer to the document that you're looking for likely
it's one example of a of a rag approach again the purpose of this lecture is not to go through
all this tree and explain you every single method that has been discovered for rags but
i just wanted to show you how much research has been done between 2020 and 2025 in rags
and how many branches of research you you now have that you can learn from the survey paper
is linked in the slides by the way and i'll share them after the lecture super so we've
made some progress hopefully now you feel like if you were to start an LLM application
you know how to do better prompts you know how to do chains you know how to do fine tuning
you also know how to do retrieval and you have the baggage of techniques that you can go
and read and find the code base pull the code vibe code it but you have the breath now
the next set of topics we're going to see is around the question of how could
we extend the capabilities of LLMs from performing single tasks enhanced with external
knowledge to handling multi-step autonomous workflows and this is where we get into proper
agent ii so let's talk about agent ii workflows towards autonomous and specialized systems
then we'll talk about evals then we'll see multi-agent systems and we'll end with the
with a little thoughts on what's next in ai so and rewring actually
um coined the term agent ii workflows and his reason was that a lot of companies use
uh say agents agents agents everywhere agents everywhere if you go and work at these companies
you will notice that they mean very different things by agent some people actually have a
prompt and they call it an agent you know other people they have a very complex multi-agent
system they call it an agent and so calling everything an agent doesn't do it justice
so andrew says let's call it agent workflows because in practice it's a bunch of prompts
with tools with additional resources api calls that ultimately are put in a workflow
and you can call that workflow agent ii so it's all about the multi-step process
um to complete the task also calling it agent ii workflow allows us to not mix it up with what
i called agent the line the last lecture with reinforcement learning because in rl agent has
a very specific definition interacts with an environment passes from one state to the other
has a reward and an observation you remember that chart right so um here's an example of
how we move from a one-step bomb to a multi-step agent workflow let's say a user queries a
product what is your refund policy on a chat bot and the response using a rag says refunds
are available within 30 days of purchase and maybe the rag can even look linked to the
policy documents that's what we learned so far instead an agent workflow can function like
this the user says can i get a refund for my order and the response via the agent workflow
is the agent retrieves the refund policy using a rag the agent then follows up with
the users and says can you provide your order number then the agent queries an api to check
the order details and finally it comes back to the user and confirms your order qualifies for
a refund the amount will be processed in three to five business days this is much more thoughtful
than the first version which is sort of vanilla right so that's what we're going to talk about in
the next a couple of slides is how do we get from the first one to the second one
there are plenty of specialized agent workflows online you know you've heard and if you hang
out in sf you probably see a bunch of billboards you know ai software engineer ai skills
mentor you've interacted within the class to work here ai sdr ai lawyers ai you know
specialized cloud engineer you know it would be a stretch to say that everything works but
there's work being done towards that you know i'm not personally a fan of putting a
face behind those things i think it's gimmicky and i think in a few years from now actually
very few products will have a human face behind it but might be a marketing tactic
from some startups it's more scary than it is engaging frankly um okay i want to talk about
the pirating shift uh that's especially useful let's say you're a software engineer or you're
planning to be a software engineer because software engineering as a discipline is sort
of shifting or at least the best engineers i've worked with are able to move from a
deterministic mindset to a fuzzy mindset and balance between the two whenever they need to
get something done so here's the paradigm shift between traditional software and agent tki
software the first one is the way you handle data traditional software deals with structured
data you have json's you have databases they're pasted in a very structured manner in a data
engineering pipeline and then they're used to be displayed on a certain interface the user
might fill a form that is then retrieved and pasted in the database all of that
historically has been structured data now more and more companies are handling freeform text
images and all of that requires dynamic interpretation to transform an input into
an output the software itself used to be deterministic now you have a lot of software
that is fuzzy and fuzzy software creates so many issues i mean imagine if you let your
user ask anything on your website the chances that it breaks is tremendous the chances that
you're attacked is tremendous the chances it's really really complicated it's more complicated
than people make it seem on twitter fuzzy engineering is truly hard you know you might
get hate as a company because one user did something that you authorize them to do that
ended up breaking the database and ended up you know we've seen that with many companies
in the last couple of years so it takes a very specialized engineering mindset to do
fuzzy engineering but also know when you need to be deterministic the other thing i call is
with agent AI software you sort of want to think about your software as like your manager
so you're familiar with the monolith or or you know microservices approaches in software
you know where you structure your software in different you know boxes that can talk to each
other and it allows teams to debug one section at a time you know now the equivalent with
agent AI is you think as a manager so you think okay if i was to delegate my products
to be done by a group of humans what would be those roles would i have a graphic designer
that then you know puts together a chart and then sends it to a marketing manager that
converts it into a nice blog post that then gives it to the performance marketing expert
that then publishes the work the blog post and then optimizes an ab test then to a data
scientist that analyzes the data and then puts hypotheses and validates them or invalidates them
that's how you typically think if you're building an agent AI software
when actually the equivalent of that in traditional software might be completely
different it might be we have a data engineer box right here that handles all our data engineering
and then here we have the UI UX stuff everything UI UX related goes here and you know
companies might structure it in very different ways and here is the business logic that we
want to care about and there's five engineers working on the business logic let's say okay
testing and debugging is also very different and we'll talk about it in the next section
uh the other thing that uh i feel matters is with AI in engineering the cost of experimentation
is going down drastically and so people i feel should be more comfortable throwing away codes
you know it's it's like in traditional software engineering you probably don't throw away code
at on you you build a code and it's solid and it's bulletproof and then you you update
it over time when we've seen AI companies be more comfortable throwing away codes
which has advantages in terms of the speed at which you move but also disadvantages in
terms of the quality of your software that can break more okay so anyway just wanted to do
an apartheid on the the the paradigm shift from deterministic to fuzzy engineering um
oh and actually i can give you an example from uh from work here that we learned uh probably
over the last 12 months is like if you if you've used work here you might have seen that the
interface has um asks you sometimes multiple choice questions and sometimes it asks you
multiple select and sometimes it asks you drag and drop ordering matching whatever right
those are examples of deterministic item types meaning you answer the question on a
multiple choice there's one correct answer it's fully deterministic on the other hand you sometimes
have voice questions where you go to a role play or you have voice plus coding questions
where your code is being read by the interface or whatever those are fuzzy meaning the scoring
algorithm might actually make mistakes and those mistakes might be costly and so companies
have to figure out a human in the loop system which you might have seen the appeal
feature at the end so at the end of the assessment you have an appeal feature where
it allows you to say i want to appeal the agent because i want to challenge what the
agent said on my answer because i thought that was better than what the agent thought
and then you bring your human in the loop that then can fix the agent can tell the agent
actually you were too harsh on the answer of this person and you know that's an example of
a fuzzy engineered system that then adds a human in the loop to make it more aligned and
so if you're building a company i would encourage you to think about what can i get done with
determinism and let's get that done and then the fuzzy stuff i want to do fuzzy because it
allows more interaction it allows more back and forth but i need to put guardrails around it
and how am i going to design those guardrails pretty much all right here's another example
from enterprise workflows which are likely to change due to agent TKi this is a paper from
McKinsey i believe from last year where they looked at a financial institution and they said
that you know we observed that they often spend one to four weeks to create a credit risk
memo and here's the process a relationship manager gathers data from 15 and more than
15 sources on the borrower loan type other factors then the relationship manager and the
credit analyst collaboratively analyze that data from these sources then the credit analyst
typically spends you know 20 more 20 hours or more writing a memo and then goes back to the
relationship manager they give feedback and then they go through this loop again and again
and it takes a long time to get a credit memo out and then then run a research study where
they change the process they said gene AI agents could actually cut time by 20 to 60
percent on credit risk memos and the process has changed to the relationship manager directly
work with the gene AI agent system provides relevant materials that needs to produce the
memo the agent subdivises the project into tasks that are assigned to specialist agents
gathers and analyzes the data from multiple sources drafts a memo then the relationship
manager and the credit analyst sit down together review the memo give feedback to the agent
and within you know 20 to 60 percent less time are done and so this is an example where
you're actually not changing the human stakeholders you're just changing the process
and adding gene AI to reduce the time it takes to get a credit memo out it turns out that's
imagine you're an enterprise and you have you know 100,000 employees and there's a lot of
enterprises with 100,000 employees out there you are currently under crisis in terms of redesigning
your workflows you you are you know it turns out that if you actually pull the job descriptions
from the HR system and you interpret them you also pull the business process workflows that
you have encoded in your drive you actually can find gains in multiple places and in the next
few years you're probably going to see workflows being more optimized to add gene AI even if that
happens the hardest part is changing people what we know this is this is great in theory
but now let's try to fit that second workflow for 10,000 credits risk analysts
and relationship managers my guess is it will take years it will take 10 20 years to get to
this being actually done at scale within an organization because change is so hard you know
so hard to rewire business workflows job descriptions incentivize people to do different
and be different and train them and so so you know this is what the world is going towards
but it's going to take a long time I think okay then I want to talk about how the agent
actually works and what are the core components of an agent imagine a travel booking AI agent
that's an easy example you've all thought about I still haven't been able to get
an agent to book a trip for me or or I was scared because it was going to book a
very expensive or long trip but in theory you can you can have a travel booking agent that
has prompts so the prompts we've seen we know the methods to optimize those prompts
that travel agent also has a content management context management system which is essentially
the memory of what it knows about the user that context management system might include
a core memory or working memory and an archival memory okay what the difference is
within memory is not every memory needs to be fast to access like think about it you
you're more than on a product and the first question is hi what's your name and I say
my name is Keon that's probably going to sit in the working memory because the agent every
time he's going to talk to me he's going to want to use my name right but then maybe the
second question is what's your birthday and I give it my birthday does it need my birthday
every day probably not so it's probably going to park it on the long-term memory or the
archival memory and those memories are slower to access they're farther down the stack
and you know that structure allows agent to determine what's the working memory and what's
a long-term memory you know and that makes it easier for the agent to retrieve super fast
because think about it when you interact with gpt you feel that it's very personal at times
right you feel like it understands you imagine every time you call it it has to read the
memories right and that can be costly it's like a very it's a very burdensome cost because it
happens every time you talk to it so you want to be highly optimized with the working memory
you know if it takes three seconds to look in the memory every time you're going to talk to
your LLM it's going to take three seconds which you don't want so anyway and then you have the
tools the tools can include APIs like a flight search API hotel booking API car rental API
weather API and then the payment processing API and typically you would want to tell your agent
how that API works it turns out that agents or LLMs I should say are very good at reading API
documentation so you're giving the API documentation and it reads the json and it reads what does it
get requests look like and this is the format that I need to push and then it pushes it in
that format let's say and then it retrieves something does that make sense those different
components you know entropic also talks about resources resources is data that is sitting
somewhere that you might let your agent read for example if you're building your startups
you have a CRM a CRM has data in it and you want to use lookups in that data you will probably
give a lookup tool and you will give access to the resource and it will do lookups whenever
you want super fast this type of architecture can be built with different degrees of autonomy
from the least autonomous to the most autonomous and I'll give you a few examples
uh less autonomous would be you've hardcoded the steps so let's say I tell the travel agent
first identify the intent then look up in the database the history of this customer with us
and their preferences then go to the flight API blah blah blah then go to the that I would
hardcode the steps okay that's the least autonomous the semi-autonomous is I might
hardcode the tools but I'm not going to hardcode the steps so I'm going to tell the agent
you're act like a travel agent and um and uh you your task is to help the person book
a travel and these are the tools that you have accessible to yourself and so I'm not
hardcoding the steps I'm just hardcoding the tools that you have access to yourself
for yourself the more autonomous is the agent decides the steps and can create the tools so
that's where you might give actually access to a code editor to the agent and the agent might
actually be able to ping any API in the web perform some web search it might even be able
to create some code to display data to the user it might even be able to perform some
calculations like oh I'm going to calculate the fastest route to get from San Francisco to New
York and which one might be the most appropriate for what the user is looking for and then I want
to calculate the distance between the airport and that hotel versus that hotel and I'm going
to write code to do that so it's actually fully autonomous from that perspective okay
so yeah remember those keywords memory prompts tools etc now I presented the flight
API but it does not have to be an API you probably have heard the term mcp
or model context protocol that was coined by entropic I pasted the seminal article
on mcp at the bottom of this slide but let me explain in a nutshell why those things would
differ in the API case you would actually teach your LLM to ping an API so you would say this
is how you ping this API and this is the data that it will send you back and you would have
to do that in a one-off manner so you would have to build or sort of give the API documentation
of your flight API your booking hotel API your car rental API and then you would give
tools for your model to communicate with those APIs it doesn't scale very well you know versus
mcp mcp it's really about you know putting a system in the middle sort of that would make
it simpler for your LLM to communicate with that end point so for instance you might
you know have an mcp server and mcp client where you're trying to communicate with that
travel database or the flight API or mcp and your agent might actually just communicate with
it and say hey what do you need in order to give me more flight information and that that agent
will respond by I would like you to tell me where is the origin flight where is the destination
and what you're looking for at a high level this is my requirement okay let me get back to
you with my requirement oh you forgot to tell me your budget whatever oh let me give you my
budget etc and it's it's it's agent to agent communication which allows more scalability you
don't need to hardcode everything companies have displayed their mcps out there and you can
your agent can communicate with them and figure out how to get the data it needs does that make
sense yeah oh sorry yes I think it is ultimately the question is isn't it shifting the issue
because anyway if an API has to be updated the mcp has to be updated is what you say right
yes that's correct but at least it allows the agent to sort of go back and forth and figure out
what the requirements are but at the end of the day ideally if you're a startup you have
some documentation and automatically you have an agent or an LLM workflow that reads that
documentation and updates the code accordingly you know but I agree it's not it's not something
that is fully autonomous yeah which should security specifically
yeah so are there security issues with mcps so think about it this way mcps depending on
the data that you get access to might have different requirements lower stake or higher
stake I'm not an expert that you know the full range but it wouldn't surprise me that
you know when you expose an mcp to an I think you would a lot of mcps have
authentication so you know you might actually need a code to actually talk to it just like
you would with an API or a key yeah but that's a good question I'm you know I'm not an expert
at the security of these systems but you know we can look into any other questions on
what we've seen with the agentic workflows APIs tools mcps memory all of that is under
progress so even memory is not a solved problem by any mean it's pretty hard actually
yes exactly exactly yeah is mcp about efficiency or accessing more data it's about efficiency
it's like you know let's say you have a coding agent and you know it has an mcp clients and
there's multiple mcp servers that are exposed out there that agent can communicate very
efficiently with them and find what it needs and it's a more efficient process than actually
in displaying APIs and the APIs on that side and how to ping them and what the protocol
but you know it's not about the data that is being exposed because ultimately you control
the data is being exposed you probably you know depending on how the mcp is built my guess is you
probably expose yourself to other risks because your your mcp server can can see any inputs
pretty much from another llm and so it has to be robust yeah super so let's look at an
example of a step-by-step workflow for the travel agent so let's say the user
says i want to plan a plan a trip to paris from december 15 to 20th with flights hotels near the
fl tower and then an itinerary of must visit places that's the task to the travel agent
step two the agent plans the steps so it says i'm going to find flights use the flight
search api to get option for december 15 search hotels generate recommendation for places to visit
validate preferences budget etc book the trip with the payment processing api step three
that's just the planning by the way step three execute the plan use your tools combine the
results and then proactive user interaction and booking it might make a first proposal to the
user and ask the user to validate or invalidate and then may repeat that planning and execution
process and then finally it might actually update the memory it might say oh i just
learned through this interaction that the user only likes direct flights next time i'll only
give direct flights or i notice users are fine with three star hotels or four star hotels and
in fact they're they don't want to go above budget or something like that
so that hopefully makes sense by now and you know how you might do that my question for you
is how would you know if this works and if you had such a system running in production
how would you improve it yeah so that's an example so let users rate their experience at
the end that would be an end-to-end test right you're looking at the user experience
through the steps and say how good was it from one to five let's say yeah it's a good way and
then if you learn that a user says one what how do you improve the the workflow okay so you
would go down three and say okay you said one what what was your issue and then the user
says the prices were too high let's say and then you would go back and fix that specific
tool or prompt or yeah okay any other ideas yeah good so that's a good insight separate the
llm related stuff from the non-llm related stuff the deterministic stuff the deterministic
stuff you might be able to fix it you know more objectively essentially yeah it was what else
so give me an example of an objective issue that you can notice and how you would fix it
versus a subjective issue yeah okay so let's say you say there's the same flight but one
is cheaper than the other let's say it's objectively worst and so you can capture
that almost automatically yeah so you could actually build evals that are objective
that are tracked across your users and you might actually run an analysis after and see that
for the objective stuff we notice that our llm ai agent again workflow is bad with pricing
it just doesn't read price as well because it always gives a more expensive option
yeah you're perfectly right how about the subjective stuff yeah like do you choose a
direct or indirect flight if the indirect yeah good one do you do you choose a direct flight
or an indirect flight if the indirect is cheaper but the direct is more comfortable
yeah that's a good one actually so how would you capture that information let's say this
is used by thousands of users could you feed something in yeah i mean you could you could
uh could you feed something in about the user preferences well you could you could build a
data set that has some of that information so you build 10 prompts where the user is asking
specifically for direct is saying that i prefer direct flights because i care about my time
i say and then you look at the output and you actually give a good the example of a good
output and you probably are able to capture the performance of your
agentic workflow on this specific eval whether does it prioritize does it understand price
conscious is it price conscious essentially and comfort conscious yeah what about the tone let's
say let's say the llm right now is not very friendly how would you notice that and how would
you fix it yeah okay have a test user run the prompt and see if there's something wrong with
that tell me about the last step how would you notice that something is wrong so
yeah i agree with your approach have llm judges that evaluate the response against a certain
rubric of what politeness looks like so here in this case you could actually start
with error analysis so you start you have a thousand users and you know you can pull up
20 user interaction and read through it and you might notice at first sight the llm seems to be
very rude you know it's just super super short in its answers and it's not very helpful
you notice that with your error analysis manually then you go to the next stage you
actually put evals behind it you say i'm going to create a set of a set of lm judges
that are going to look at the user interaction and are going to rate
how polite it is and i'm going to give it a rubric then what i'm going to do is i'm going
to flip my llm instead of using gpt4 i'm going to use grok and instead of using rock i'm going
to use llama and then i'm going to run those three llm side by side give it to my llm judges
and then get my subjective score at the end to say oh x model was more polite on average
perfectly right that's an example of an eval that is very specific and allows you to choose between
elements you could actually do the same eval not across llms but fix the llm change the prompts
you actually instead of saying act like a travel agent you say act like a helpful travel
agent and then you see the influence of that word on your eval with the llm as judges does
that make sense okay super so let's let's move forward and do a case study with evals and
then we're we're almost done for today let's say your product managers manager asks you to build
an ai agent for customer support okay where do you start and here is an example of the user
prompt i need to change my shipping address for order blah blah i move to a new address
so what would you start if i'm giving you that project you know yes all right so do
some research see benchmarks and how different models perform at customer support and then pick
a model that's what you mean yeah you it's true you could do that what what else could you do
yeah okay yeah i like that try to decompose the different tasks that it will need
and try to guess which ones will be more of a struggle which ones should be fuzzy which
should be deterministic yeah you're right yeah similar to what you said that's what i would
recommend as well you say i would sit down with a customer support agent for a day or two
and i would decompose the tasks they're going through i will ask them where do they struggle
how much time it takes yes that's usually where you want to start with task decomposition
so let's say we've done that work and we have this list i'm simplifying but the customer
support agent human typically would extract info then look up in the database to retrieve
the customer record then check the policy you know are we allowed to update the address or
is it a fixed data point and then draft a response email and send the email okay so
we've decomposed that task once you've decomposed that task how do you design
your agent workflow yes exactly so to repeat i'm going to you're going to look at the
decomposition of tasks get an instinct of what's fuzzy what's deterministic and then
determine which line is going to be an llm one shot which one will require maybe a rag
which one will require a tool which one will require memory which one so you will start
designing that map completely right that's also what i would recommend you you might actually
draft it and say okay i take the user prompt and the first step of my task deposit
decomposition was extract information that seems to be a vanilla llm you you can
guess that the vanilla lm would probably be good enough at extracting the user wants to
change their address and this is the order number and this is the new address you probably
don't need too much technology there other than the llm the next step it feels like you need a
tool because you're actually going to have to look up in the database and also update the address
so that might be a tool and you might have to build a custom tool for the llm to say
let me connect you to that database or let me give you access to that resource with an mcp
you know after that you probably need an lm again to draft the email but you would
probably paste confirmation you paste the confirmation that your address has been
updated from x to y and then the llm will draft an answer and of course just to not forget
you might need a tool to send the email you know you might actually need to you know post
something to for the email to to go out and then you'll get the output does that make sense
so exactly what you described okay now moving to the next step once we have the composer tasks
then we have designed an agentic workflow around it it took us five minutes in practice it will
take you more if you're building your startup on that you want to make sure your task decomposition
is accurate your thing is accurate here and then you can have a lot of work done on every
tool and optimize it and latency and cost but let's say and now we want to know how
if it works you know and i'm going to assume that you have llm traces llm traces are very
important actually if you're interviewing with an ai startup i would recommend you in the interview
process to ask them do you have llm traces because if they don't have lm traces it is
pretty hard to debug an lm system you know because you don't have visibility on the chain
of complex prompts that were called and where the bug is and you know so it's a basic sort
of part of an ai startup stack to have a llm traces so let's assume you have traces how would
you know is your system work you know we you know i'm going to summarize some of the things
i heard earlier you gave us an example of an end-to-end metric you look at the user
satisfaction at the end you can also do a component-based approach where you actually
will look at the tool the database updates and you will manually do an error analysis and see oh
the tool actually always forgets to update the email it just fails at writing you know and i'm
going to fix that this is deterministic pretty much or you know when it tries to send the email
and ping the system that is supposed to send the email it doesn't send it in the right
format and so it bugs at that point again you could fix that draft of the email dllm doesn't
do a great job it's not very polite at drafting the email you know so you could look at component
by component and it's actually easier to debug than to look at it end to end you probably do
a mix of both another way to look at it is what is objective versus what is subjective so
for example an objective example would be a dllm extracted the wrong order id you know the user
said my order id is x and the dllm when it actually pasted looked up in the database it
used the wrong order id this is objectively wrong you can actually write a python code that
checks that checks just the alignment between what the user mentioned and and what was actually
pasted in the database or for the lookup you also have subjective stuff which we talked about
where you probably want to do either human rating or llm as judges it's very relevant for
subjective evals and finally you will find yourself having quantitative evals and more
qualitative evals so quantitative would be percentage of successful address updates the latency you
could actually track the latency component based and see which one is the slowest let's
say sending the email is five seconds you know it's too long let's say you would notice component
base or the full workflow and then you will decide where am i optimizing my latency and
how am i going to do that and then finally qualitative you might actually do some error
analysis and look at you know where are the hallucinations where are the tone mismatches
you know are the user confused and by what they're confused you know that would be more
qualitative and typically it would take more you know white glove approaches to do that
okay so here's what it could look like i gave you some examples but
you would build evals to determine objectively subjectively component based end-to-end based
and then quantitatively and qualitatively where is your llm failing and where it's doing well
does that give you a sense of the type of stuff you could do to fix improve that agenting workflow
super well that was our case study on evals we're not going to delve deeper into it but
hopefully it gave you a sense of the type of stuff you can do with llm judges with
you know objective subjective component based end-to-end etc last section on multi-agent
workflows so you might you might ask hey why do we need a multi-agent workflow when we are
when the workflow already has multiple steps already calls the llm multiple times already
gives them tools why do we need multiple agents and so many people are talking about
multi-agent system online it's not even a new thing frankly i mean multi-agent system
i've been around for a long time the the main advantage of a multi-agent system is going
to be parallelism it's like is there something that i wish i would run in parallel sort of
independently but maybe there are some sinks in the middle but that's where you want to
put a multi-agent system it's when it's parallel the other advantage that some companies
have with multi-agent system is an agent can be reused so let's say in a company you
have an agent that's been built for design that agent can be used in the marketing team
and it can be used in the product team you know and so now you're optimizing an agent
which has multiple stakeholders that can communicate with it and benefit from its
performance actually i'm going to ask you a question and take a few maybe a minute to
think about it let's say you were building smart home automation for your apartment or your
home what agents would you want to build yeah write it down and then i'm going to ask you
in in a minute to share some of the agents that you will build also think about how you
would put a hierarchy between these agents or how you would organize them or who should
communicate with who okay okay take a minute for that be creative also because i'm going
to ask all of your agents and maybe you have an agent that nobody has thought of
okay let's get started who wants to give me a a set of agents that you would want for your home
smart home yes okay so let me repeat there are four agents i think roughly one that tracks
biometric like your where are you in the home where are you moving how you're moving things
like that that sort of knows your location the second one determines the temperature
of the rooms and has the ability to change it the third one tracks energy efficiency
and might be feedback on energy and energy usage it might be i don't know maybe it has
the control over the temperature as well i don't know actually or the gas or the water
um might cut your water at some point the and then you have an orchestrator agent what is
exactly the orchestrator doing okay passes instructions so is that the agent that
communicates mainly with the user okay so if i have i'm coming back home and i'm saying
i want the oven to be preheated i communicate with the orchestrator and then it would funnel
to another agent okay sounds good yeah so that's an example of a i want to say a hierarchy called
agency multi-agent system what else any other ideas what would you add to that yeah
oh i like that that's a really good one so let me summarize you have a security agent
that determines if you can enter or not and when you enter it understands who you are and then it
gives you certain sets of permissions that might be different depending on if you're a parent or
a kid or you know you might have access to certain cars and others or the kid cannot open
the fridge or i don't know like something like that yeah or okay i like that that's a good
yeah and it does feel like it's a complex enough workflow
where you want a specific workflow tied to that i agree what what else yes
well that's really good actually so you mentioned two of them
one is maybe an agent that has access to external apis that can understand the weather out there
the wind the sun and then has control over certain devices at home temperature blinds things
like that and also understand your preferences for it that does feel like it's a good use
case because you could give that to the orchestrator but it might use itself because
it's doing too much so you probably and also these problems are tied together like
temperature outdoor with the weather api might influence the temperature inside how you want it
etc and then the second one which i also like is you might have an agent that looks at your
fridge and what's inside and it might actually have access to the camera in the fridge for
example and know your preferences and also has access to the e-commerce api to order
amazon groceries ahead of time i agree and maybe the orchestrator will be the communication
line with the user but it might communicate with that agents in order to get it done
yeah i like those so those are all really good examples here is the list i had
up there so climate control lighting security energy management entertainment notification
agent alerts about the system updates energy saving and orchestrator so all of them you
mentioned actually and then we didn't talk about the different interaction patterns
but you do have different ways to organize a multi-agent system flat hierarchical it sounds
like this would be hierarchical i agree and the reason is ui ux is i would rather have to
only talk to the orchestrator rather than have to go to a specialized application to do
something like it feels like the orchestrator could be responsible for that and so i agree
i would probably go for a hierarchical setup here but maybe you might act also add some
connections between other agents like in the flat system where it's all to all for example
with climate control and energy if you want to connect those two you might actually allow them
to speak with each other when you allow agents to speak with each other it is basically
an mcb protocol by the way so you treat the agent like a tool exactly like a tool here is
how you interact with this agent here is what it can tell you here is what it needs from you
essentially okay super and then without going into the details there are advantages to multi-agent
workflows versus you know single agents such as debugging it's easier to special
debug a specialized agent and to debug an entire system parallelization as well it's easier
to have things run in parallel and you can earn time you know there are some advantages
to doing that and i leave you with the slide if you want to go deeper super so we've learned
so many techniques to optimize llms from prompts to chains to fine tuning retrieval
and to multi-agent system as well and then just to end on a couple of trends i want you to
watch i think next week is thanksgiving is that it is thanksgiving break no the week after
okay well ahead of the thanksgiving break so if you're traveling you can think about these
things what's next is in ai i wanted to call out a couple of trends so ilias is cover one of
the ogs of a you know llms and you know opening i co-founder raised that question about are we
plateauing or not you know the question of are we going to see in the coming years
llm sort of not improve as fast as we've seen in the past it's been the feeling in the
community probably that you know the last version of gpt did not bring the level of performance that
people were expecting although it did make it so much easier to use for consumers because
you don't need to interact with different models it's all under the same hood so it
seems that it's progressing but the plateau is unclear the way i would think about it is
um the llm scaling laws tell us that if we continue to improve compute and energy
then lm should continue to improve but at some point it's going to plateau so what's going to
take us to the next step it's probably architecture search still a lot of llms even
if we don't understand what's under the hood are probably transformer based today but we know
that the human brain does not operate the same way there's just certain things that we do that
much more efficient much faster we don't need as much data so theoretically we have so much
to learn in terms of architecture search that we haven't figured out it's not a surprise that
you see those labs hire so many engineers because it is possible that in the next few years
you're going to have thousands of engineers trying to figure out the different engineering
hacks and tactics and architectural searches that are going to lead to better models and one
of them suddenly we find the next transformer and it will reduce by 10x the need for compute
and the need for energy you know it's sort of if you've read isak azimov's foundation series
individuals can have an amazing impact on the future because of their decisions you know whoever
discovered transformers had a tremendous impact on the direction of ai i think we're going
to see more of that in the coming years where some group of researcher that is iterating
fast might discover certain things that would suddenly unlock that plateau and take us to the
next step and it's going to continue to improve like that and so it doesn't surprise me that
there's so many companies hiring engineers right now to figure out those hacks and those
those techniques the other set of gains that we might see is from multimodality so the
way to think about it is we've we've had llm's first text based and then we've added
and today you know models are very good at images they're very good at text turns out that being
good at images and being good at text makes the whole model better so the fact that you're good
at understanding a cat image makes you better at text as well for a cat now you add another
modality like audio or video the whole system gets better so you're better at writing about
a cat if you know what a cat sounds like if you can look at a cat on an image as well
does that make sense so we see gains that are translated from one modality to another
and that might lead in the pinnacle of robotics where all these modalities come together
and suddenly the robot is better at running away from a cat because it understands what a
cat is how it sounds like what it looks like etc that makes sense the other one is the
multiple methods working in harmony in the tuesday lectures we've seen supervised learning
unsupervised learning self-supervised learning reinforcement learning prompt engineering rags
etc if you look at how babies learn it is probably a mix of those different approaches
like a baby might have some meta learning meaning you know it has some survival instinct that is
in the encoded in the dna most likely and that's like the baby's pre-training if you
will on top of that the mom or the dad is pointing at stuff and saying bad good bad good
supervised learning on top of that the baby's falling on the ground and getting hurt and that's
a reward signal for reinforcement learning on top of that the baby's observing other people doing
stuff or other babies you know doing stuff unsupervised learning you see what i mean
it's we're probably a mix of all these methods and and i think that's where the trend is
going is where those methods that you've seen in cs 230 come together in order to build an
ai system that learns fast is low latency is cheap energy efficient and makes the most out of
all of these methods finally and this is especially true at stanford you have research
going on that you would consider human centric and some research that is non-human centric
by human centric i should say human approaches that are modeled after the brain and approaches
that are not modeled after humans because it turns out that the human body is very limiting
and so if you actually only do research on what the human brain looks like you're probably missing
out on compute and energy and stuff like that that you can optimize even beyond neural connections
in the brain but you still can learn a lot from the human brain and that's why there are
professors that are running labs right now that try to understand how does back propagation
work for humans and in fact it's probably that we don't have back propagation we don't
use back propagation we only do forward propagation let's say so this type of stuff
interesting research that i would encourage you to read if you're curious about the direction of
of ai and then finally one thing that's going to be pretty clear i call it all the time but
it's the velocity at which things are moving you're noticing part of the reason
we're giving you a breath in cs2 30 is because these methods are changing so fast so i don't
want to bother going and teaching you the number 17 methods on rag that optimizes the
rag because in two years you're not going to need it you know so i would rather you think about
what is the breadth of things you want to understand and when you need it you are
sprinting and learning the exact thing you need faster because the half life of skill is
so low you know you want to come out of the class with a good breath and then have the
ability to go deep whenever you need after the class and so that's sort of how that
class is designed as well yeah that's it for today so thank you thank you for participating
you
